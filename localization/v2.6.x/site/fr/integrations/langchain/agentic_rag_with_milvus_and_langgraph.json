{"codeList":["$ pip install --upgrade langchain langchain-core langchain-community langchain-text-splitters langgraph langchain-milvus milvus-lite langchain-openai bs4\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load blog posts about AI topics\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=1000, chunk_overlap=200\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Let's see how many document chunks we have\nprint(f\"Total document chunks: {len(doc_splits)}\")\n","from langchain_milvus import Milvus\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.tools.retriever import create_retriever_tool\n\n# Initialize embeddings\nembeddings = OpenAIEmbeddings()\n\n# Create Milvus vector store\nvectorstore = Milvus.from_documents(\n    documents=doc_splits,\n    embedding=embeddings,\n    connection_args={\n        \"uri\": \"./milvus_agentic_rag.db\",\n    },\n    drop_old=True,\n)\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# Create retriever tool\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about AI agents, prompt engineering, and adversarial attacks on LLMs from Lilian Weng's blog posts.\",\n)\n\n# Test the retriever tool\nprint(retriever_tool.invoke({\"query\": \"What is Tree of Thought strategy?\"})[:1000])\n","from langgraph.graph import MessagesState\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the language model\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","def generate_query_or_respond(state: MessagesState):\n    \"\"\"\n    Decide whether to retrieve information or respond directly.\n\n    Args:\n        state: Current graph state with messages\n\n    Returns:\n        Updated state with the model's response\n    \"\"\"\n    response = llm.bind_tools([retriever_tool]).invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n\n# Test with a simple greeting\ntest_state = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}\nresult = generate_query_or_respond(test_state)\nprint(\"Response to greeting:\", result[\"messages\"][-1].content)\n\n# Test with a question that needs retrieval\ntest_state = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is Chain of Thought prompting and how does it work?\",\n        }\n    ]\n}\nresult = generate_query_or_respond(test_state)\nif hasattr(result[\"messages\"][-1], \"tool_calls\") and result[\"messages\"][-1].tool_calls:\n    print(\"Model decided to use retrieval tool\")\n    print(\"Tool call:\", result[\"messages\"][-1].tool_calls[0])\n","from pydantic import BaseModel, Field\nfrom typing import Literal\n\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n    )\n\n\ndef grade_documents(state: MessagesState) -> Literal[\"generate\", \"rewrite\"]:\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.\n\n    Args:\n        state: Current graph state with messages\n\n    Returns:\n        Decision to generate answer or rewrite question\n    \"\"\"\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n\n    # Get the question and retrieved documents\n    question = state[\"messages\"][0].content\n    docs = state[\"messages\"][-1].content\n\n    # Create structured LLM grader\n    structured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n    # Grade prompt\n    grade_prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n    \n    Retrieved document:\n    {docs}\n    \n    User question:\n    {question}\n    \n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n    Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\"\n\n    score = structured_llm_grader.invoke(\n        [{\"role\": \"user\", \"content\": grade_prompt}]\n    ).binary_score\n\n    if score == \"yes\":\n        print(\"---DECISION: DOCS RELEVANT---\")\n        return \"generate\"\n    else:\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\n        return \"rewrite\"\n","def rewrite_question(state: MessagesState):\n    \"\"\"\n    Transform the query to produce a better question.\n\n    Args:\n        state: Current graph state with messages\n\n    Returns:\n        Updated state with rewritten question\n    \"\"\"\n    print(\"---TRANSFORM QUERY---\")\n\n    question = state[\"messages\"][0].content\n\n    rewrite_prompt = f\"\"\"You are an expert at query expansion and transformation.\n    \n    Look at the input question and try to reason about the underlying semantic intent / meaning.\n    \n    Here is the initial question:\n    {question}\n    \n    Formulate an improved question that will retrieve better documents from a vector database:\"\"\"\n\n    response = llm.invoke([{\"role\": \"user\", \"content\": rewrite_prompt}])\n\n    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n","def generate(state: MessagesState):\n    \"\"\"\n    Generate answer based on retrieved documents.\n\n    Args:\n        state: Current graph state with messages\n\n    Returns:\n        Updated state with generated answer\n    \"\"\"\n    print(\"---GENERATE ANSWER---\")\n\n    question = state[\"messages\"][0].content\n    docs = state[\"messages\"][-1].content\n\n    # RAG generation prompt\n    rag_prompt = f\"\"\"You are an assistant for question-answering tasks.\n    \n    Use the following pieces of retrieved context to answer the question.\n    \n    If you don't know the answer, just say that you don't know.\n    \n    Use three sentences maximum and keep the answer concise.\n    \n    Question: {question}\n    \n    Context: {docs}\n    \n    Answer:\"\"\"\n\n    response = llm.invoke([{\"role\": \"user\", \"content\": rag_prompt}])\n\n    return {\"messages\": [response]}\n","from langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\n# Create the graph\nworkflow = StateGraph(MessagesState)\n\n# Add nodes\nworkflow.add_node(\"generate_query_or_respond\", generate_query_or_respond)\nworkflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\nworkflow.add_node(\"rewrite\", rewrite_question)\nworkflow.add_node(\"generate\", generate)\n\n# Add edges\nworkflow.add_edge(START, \"generate_query_or_respond\")\n\n# Conditional edge: decide whether to retrieve or end\nworkflow.add_conditional_edges(\n    \"generate_query_or_respond\",\n    tools_condition,\n    {\n        \"tools\": \"retrieve\",  # If tool call, go to retrieve\n        END: END,  # If no tool call, end (direct response)\n    },\n)\n\n# Conditional edge: grade documents\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    grade_documents,\n    {\n        \"generate\": \"generate\",  # If relevant, generate answer\n        \"rewrite\": \"rewrite\",  # If not relevant, rewrite question\n    },\n)\n\n# After rewriting, try to generate query again\nworkflow.add_edge(\"rewrite\", \"generate_query_or_respond\")\n\n# After generating answer, end\nworkflow.add_edge(\"generate\", END)\n\n# Compile the graph\ngraph = workflow.compile()\n","from IPython.display import Image, display\n\n# Visualize the graph\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n","inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello! How are you?\"}]}\n\nprint(\"=\" * 50)\nprint(\"Test 1: Simple greeting\")\nprint(\"=\" * 50)\n\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Node '{key}':\")\n        if \"messages\" in value:\n            value[\"messages\"][-1].pretty_print()\n    print(\"\\n\")\n","inputs = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What are the main components and building blocks of an AI agent system?\",\n        }\n    ]\n}\n\nprint(\"=\" * 50)\nprint(\"Test 2: Question requiring retrieval\")\nprint(\"=\" * 50)\n\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Node '{key}':\")\n        if \"messages\" in value:\n            print(value[\"messages\"][-1])\n    print(\"-\" * 50)\n","inputs = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"How do we defend against potential risks in AI systems?\",\n        }\n    ]\n}\n\nprint(\"=\" * 50)\nprint(\"Test 3: Question that might need rewriting\")\nprint(\"=\" * 50)\n\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Node '{key}':\")\n        if \"messages\" in value:\n            print(value[\"messages\"][-1])\n    print(\"-\" * 50)\n"],"headingContent":"Agentic RAG with Milvus and LangGraph","anchorList":[{"label":"RAG agentique avec Milvus et LangGraph","href":"Agentic-RAG-with-Milvus-and-LangGraph","type":1,"isActive":false},{"label":"Conditions préalables","href":"Prerequisites","type":2,"isActive":false},{"label":"Préparer les données","href":"Prepare-the-data","type":2,"isActive":false},{"label":"Créer un outil de récupération avec Milvus","href":"Create-a-retriever-tool-with-Milvus","type":2,"isActive":false},{"label":"Construire le graphe RAG agentique","href":"Build-the-agentic-RAG-graph","type":2,"isActive":false},{"label":"Définir l'état du graphe","href":"Define-the-graph-state","type":3,"isActive":false},{"label":"Nœud 1 : Générer une requête ou répondre","href":"Node-1-Generate-query-or-respond","type":3,"isActive":false},{"label":"Nœud 2 : Classer les documents","href":"Node-2-Grade-documents","type":3,"isActive":false},{"label":"Nœud 3 : Réécrire la question","href":"Node-3-Rewrite-question","type":3,"isActive":false},{"label":"Nœud 4 : Générer la réponse","href":"Node-4-Generate-answer","type":3,"isActive":false},{"label":"Assembler le graphe","href":"Assemble-the-graph","type":3,"isActive":false},{"label":"Exécuter le système RAG agentique","href":"Run-the-agentic-RAG-system","type":2,"isActive":false},{"label":"Test 1 : Simple message d'accueil (aucune recherche nécessaire)","href":"Test-1-Simple-greeting-no-retrieval-needed","type":3,"isActive":false},{"label":"Test 2 : Question nécessitant une recherche","href":"Test-2-Question-requiring-retrieval","type":3,"isActive":false},{"label":"Test 3 : Question susceptible de déclencher une réécriture","href":"Test-3-Question-that-might-trigger-rewrite","type":3,"isActive":false},{"label":"Résumé","href":"Summary","type":2,"isActive":false}]}