{"codeList":["$ pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-text-splitters langchain-milvus langchain-openai bs4 #langchain-voyageai\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","URI = \"http://localhost:19530\"\n# TOKEN = ...\n","from langchain_core.documents import Document\n\ndocs = [\n    Document(page_content=\"I like this apple\", metadata={\"category\": \"fruit\"}),\n    Document(page_content=\"I like swimming\", metadata={\"category\": \"sport\"}),\n    Document(page_content=\"I like dogs\", metadata={\"category\": \"pets\"}),\n]\n","from langchain_milvus import Milvus, BM25BuiltInFunction\nfrom langchain_openai import OpenAIEmbeddings\n\n\nvectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=OpenAIEmbeddings(),\n    builtin_function=BM25BuiltInFunction(),\n    # `dense` is for OpenAI embeddings, `sparse` is the output field of BM25 function\n    vector_field=[\"dense\", \"sparse\"],\n    connection_args={\n        \"uri\": URI,\n    },\n    drop_old=False,\n)\n","# from langchain_voyageai import VoyageAIEmbeddings\n\nembedding1 = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\nembedding2 = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n# embedding2 = VoyageAIEmbeddings(model=\"voyage-3\")  # You can also use embedding from other embedding model providers, e.g VoyageAIEmbeddings\n\n\nvectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=[embedding1, embedding2],\n    builtin_function=BM25BuiltInFunction(\n        input_field_names=\"text\", output_field_names=\"sparse\"\n    ),\n    text_field=\"text\",  # `text` is the input field name of BM25BuiltInFunction\n    # `sparse` is the output field name of BM25BuiltInFunction, and `dense1` and `dense2` are the output field names of embedding1 and embedding2\n    vector_field=[\"dense1\", \"dense2\", \"sparse\"],\n    connection_args={\n        \"uri\": URI,\n    },\n    drop_old=False,\n)\n\nvectorstore.vector_fields\n","vectorstore.similarity_search(\n    \"Do I like apples?\", k=1\n)  # , ranker_type=\"weighted\", ranker_params={\"weights\":[0.3, 0.3, 0.4]})\n","vectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=None,\n    builtin_function=BM25BuiltInFunction(\n        output_field_names=\"sparse\",\n    ),\n    vector_field=\"sparse\",\n    connection_args={\n        \"uri\": URI,\n    },\n    drop_old=False,\n)\n\nvectorstore.vector_fields\n","analyzer_params_custom = {\n    \"tokenizer\": \"standard\",\n    \"filter\": [\n        \"lowercase\",  # Built-in filter\n        {\"type\": \"length\", \"max\": 40},  # Custom filter\n        {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom filter\n    ],\n}\n\n\nvectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=OpenAIEmbeddings(),\n    builtin_function=BM25BuiltInFunction(\n        output_field_names=\"sparse\",\n        enable_match=True,\n        analyzer_params=analyzer_params_custom,\n    ),\n    vector_field=[\"dense\", \"sparse\"],\n    connection_args={\n        \"uri\": URI,\n    },\n    drop_old=False,\n)\n","vectorstore.col.schema\n","import bs4\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Create a WebBaseLoader instance to load documents from web sources\nloader = WebBaseLoader(\n    web_paths=(\n        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    ),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\n# Load documents from web sources using the loader\ndocuments = loader.load()\n# Initialize a RecursiveCharacterTextSplitter for splitting text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n\n# Split the documents into chunks using the text_splitter\ndocs = text_splitter.split_documents(documents)\n\n# Let's take a look at the first document\ndocs[1]\n","vectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=OpenAIEmbeddings(),\n    builtin_function=BM25BuiltInFunction(),\n    vector_field=[\"dense\", \"sparse\"],\n    connection_args={\n        \"uri\": URI,\n    },\n    drop_old=False,\n)\n","from langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the OpenAI language model for response generation\nllm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n\n# Define the prompt template for generating AI responses\nPROMPT_TEMPLATE = \"\"\"\nHuman: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\nUse the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n<context>\n{context}\n</context>\n\n<question>\n{question}\n</question>\n\nThe response should be specific and use statistics or numbers when possible.\n\nAssistant:\"\"\"\n\n# Create a PromptTemplate instance with the defined template and input variables\nprompt = PromptTemplate(\n    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n)\n# Convert the vector store to a retriever\nretriever = vectorstore.as_retriever()\n\n\n# Define a function to format the retrieved documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","# Define the RAG (Retrieval-Augmented Generation) chain for AI response generation\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# rag_chain.get_graph().print_ascii()\n","query = \"What is PAL and PoT?\"\nres = rag_chain.invoke(query)\nres\n"],"headingContent":"Using Full-Text Search with LangChain and Milvus","anchorList":[{"label":"Utilisation de la recherche plein texte avec LangChain et Milvus","href":"Using-Full-Text-Search-with-LangChain-and-Milvus","type":1,"isActive":false},{"label":"Conditions pr√©alables","href":"Prerequisites","type":2,"isActive":false},{"label":"Initialisation avec la fonction BM25","href":"Initialization-with-BM25-Function","type":2,"isActive":false},{"label":"Personnaliser l'analyseur","href":"Customize-analyzer","type":2,"isActive":false},{"label":"Utilisation de la recherche hybride et du reclassement dans RAG","href":"Using-Hybrid-Search-and-Reranking-in-RAG","type":2,"isActive":false}]}