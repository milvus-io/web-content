---
id: milvus_rag_with_vllm.md
summary: >-
  Ce blog vous montrera comment construire et ex√©cuter un RAG avec Milvus, vLLM
  et Llama 3.1. Plus pr√©cis√©ment, je vous montrerai comment int√©grer et stocker
  des informations textuelles sous forme d'embeddings vectoriels dans Milvus et
  utiliser ce stockage vectoriel comme base de connaissances pour r√©cup√©rer
  efficacement des morceaux de texte pertinents pour les questions des
  utilisateurs.
title: 'Construire RAG avec Milvus, vLLM, et Llama 3.1'
---
<h1 id="Building-RAG-with-Milvus-vLLM-and-Llama-31" class="common-anchor-header">Construire RAG avec Milvus, vLLM, et Llama 3.1<button data-href="#Building-RAG-with-Milvus-vLLM-and-Llama-31" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h1><p>L'Universit√© de Californie - Berkeley a fait don de <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a>, une biblioth√®que rapide et facile √† utiliser pour l'inf√©rence et le service LLM, √† la <a href="https://lfaidata.foundation/">Fondation LF AI &amp; Data</a> en tant que projet en phase d'incubation en juillet 2024. En tant que projet membre, nous souhaitons la bienvenue √† vLLM qui rejoint la famille LF AI &amp; Data ! üéâ</p>
<p>Les grands mod√®les de langage<a href="https://zilliz.com/glossary/large-language-models-(llms)">(LLM</a>) et les <a href="https://zilliz.com/learn/what-is-vector-database">bases de donn√©es vectorielles</a> sont g√©n√©ralement associ√©s pour construire Retrieval Augmented Generation<a href="https://zilliz.com/learn/Retrieval-Augmented-Generation">(RAG)</a>, une architecture d'application d'IA populaire pour r√©pondre aux <a href="https://zilliz.com/glossary/ai-hallucination">hallucinations de l'IA</a>. Ce blog vous montrera comment construire et ex√©cuter un RAG avec Milvus, vLLM, et Llama 3.1. Plus pr√©cis√©ment, je vous montrerai comment int√©grer et stocker des informations textuelles sous forme d'<a href="https://zilliz.com/glossary/vector-embeddings">embeddings vectoriels</a> dans Milvus et utiliser ce stockage vectoriel comme base de connaissances pour r√©cup√©rer efficacement des morceaux de texte pertinents pour les questions de l'utilisateur. Enfin, nous utiliserons vLLM pour servir le mod√®le Llama 3.1-8B de Meta afin de g√©n√©rer des r√©ponses enrichies par le texte r√©cup√©r√©. Plongeons dans l'aventure !</p>
<h2 id="Introduction-to-Milvus-vLLM-and-Meta‚Äôs-Llama-31" class="common-anchor-header">Introduction √† Milvus, vLLM et Meta's Llama 3.1<button data-href="#Introduction-to-Milvus-vLLM-and-Meta‚Äôs-Llama-31" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Milvus-vector-database" class="common-anchor-header">Base de donn√©es vectorielles Milvus</h3><p><a href="https://zilliz.com/what-is-milvus"><strong>Milvus</strong></a> est une base de donn√©es vectorielles distribu√©e √† code source ouvert, <a href="https://zilliz.com/blog/what-is-a-real-vector-database">con√ßue sp√©cialement pour</a> le stockage, l'indexation et la recherche de vecteurs pour les charges de travail de l <a href="https://zilliz.com/learn/generative-ai">'IA g√©n√©rative</a> (GenAI). Sa capacit√© √† effectuer une <a href="https://zilliz.com/blog/a-review-of-hybrid-search-in-milvus">recherche hybride, un</a> <a href="https://zilliz.com/blog/what-is-new-with-metadata-filtering-in-milvus">filtrage des m√©tadonn√©es</a>, un reclassement et √† g√©rer efficacement des trillions de vecteurs fait de Milvus un choix de premier ordre pour les charges de travail d'IA et d'apprentissage automatique. <a href="https://github.com/milvus-io/">Milvus</a> peut √™tre ex√©cut√© localement, sur un cluster ou h√©berg√© dans le <a href="https://zilliz.com/cloud">Zilliz Cloud</a> enti√®rement g√©r√©.</p>
<h3 id="vLLM" class="common-anchor-header">vLLM</h3><p><a href="https://vllm.readthedocs.io/en/latest/index.html"><strong>vLLM</strong></a> est un projet open-source lanc√© au SkyLab de l'Universit√© de Berkeley et ax√© sur l'optimisation des performances de service LLM. Il utilise une gestion efficace de la m√©moire avec PagedAttention, une mise en lot continue et des noyaux CUDA optimis√©s. Par rapport aux m√©thodes traditionnelles, vLLM am√©liore les performances de service jusqu'√† 24 fois tout en r√©duisant de moiti√© l'utilisation de la m√©moire du GPU.</p>
<p>Selon l'article<a href="https://arxiv.org/abs/2309.06180">"Efficient Memory Management for Large Language Model Serving with PagedAttention</a>", le cache KV utilise environ 30 % de la m√©moire du GPU, ce qui peut entra√Æner des probl√®mes de m√©moire. Le cache KV est stock√© dans une m√©moire contigu√´, mais une modification de sa taille peut entra√Æner une fragmentation de la m√©moire, ce qui est inefficace pour les calculs.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="/docs/v2.6.x/assets/vllm_1.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Image 1. Gestion de la m√©moire cache KV dans les syst√®mes existants (2023 Paged Attention <a href="https://arxiv.org/pdf/2309.06180">paper</a>)</em></p>
<p>En utilisant la m√©moire virtuelle pour le cache KV, vLLM n'alloue la m√©moire physique du GPU qu'en cas de besoin, ce qui √©limine la fragmentation de la m√©moire et √©vite la pr√©-allocation. Lors des tests, vLLM a surpass√© <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">HuggingFace Transformers</a> (HF) et Text <a href="https://github.com/huggingface/text-generation-inference">Generation Inference</a> (TGI), atteignant un d√©bit jusqu'√† 24 fois plus √©lev√© que HF et 3,5 fois plus √©lev√© que TGI sur les GPU NVIDIA A10G et A100.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="/docs/v2.6.x/assets/vllm_2.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Image 2. D√©bit de service lorsque chaque requ√™te demande trois sorties parall√®les. vLLM atteint un d√©bit 8,5x-15x plus √©lev√© que HF et 3,3x-3,5x plus √©lev√© que TGI (2023 <a href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM blog</a>).</em></p>
<h3 id="Meta‚Äôs-Llama-31" class="common-anchor-header">Le lama de Meta 3.1</h3><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models"><strong>Meta's Llama 3.1</strong></a> a √©t√© annonc√© le 23 juillet 2024. Le mod√®le 405B offre des performances de pointe sur plusieurs benchmarks publics et dispose d'une fen√™tre contextuelle de 128 000 jetons d'entr√©e avec diverses utilisations commerciales autoris√©es. Parall√®lement au mod√®le √† 405 milliards de param√®tres, Meta a publi√© une version mise √† jour du Llama3 70B (70 milliards de param√®tres) et 8B (8 milliards de param√®tres). Les poids des mod√®les peuvent √™tre t√©l√©charg√©s <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWbMJv2vnLfjW3Rh6L96gqS5YW7MhRLh5j9tjNN8BHR5W3qgyTW6N1vHY6lZ3l8N8htfRfqP8DzW72mhHB6vwYd2W77hFt886l4_PV22X226RPmZbW67mSH08gVp9MW2jcZvf24w97BW207Jmf8gPH0yW20YPQv261xxjW8nc6VW3jj-nNW6XdRhg5HhZk_W1QS0yL9dJZb0W818zFK1w62kdW8y-_4m1gfjfNW2jswrd3xbv-yW5mrvdk3n-KqyW45sLMF21qDrwW5TR3vr2MYxZ9W2hWhq23q-nQdW4blHqh3JlZWfW937hlZ58-KJCW82Pgv9384MbYW7yp56M6pvzd6f77wnH004">sur le site web de Meta</a>.</p>
<p>L'une des principales conclusions est qu'un r√©glage fin des donn√©es g√©n√©r√©es peut am√©liorer les performances, mais que des exemples de mauvaise qualit√© peuvent les d√©grader. L'√©quipe Llama a beaucoup travaill√© pour identifier et supprimer ces mauvais exemples en utilisant le mod√®le lui-m√™me, des mod√®les auxiliaires et d'autres outils.</p>
<h2 id="Build-and-Perform-the-RAG-Retrieval-with-Milvus" class="common-anchor-header">Construire et ex√©cuter le RAG-Retrieval avec Milvus<button data-href="#Build-and-Perform-the-RAG-Retrieval-with-Milvus" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Prepare-your-dataset" class="common-anchor-header">Pr√©parez votre ensemble de donn√©es.</h3><p>Pour cette d√©monstration, j'ai utilis√© la <a href="https://milvus.io/docs/">documentation</a> officielle <a href="https://milvus.io/docs/">de Milvus</a>, que j'ai t√©l√©charg√©e et sauvegard√©e localement.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> DirectoryLoader
<span class="hljs-comment"># Load HTML files already saved in a local directory</span>
path = <span class="hljs-string">&quot;../../RAG/rtdocs_new/&quot;</span>
global_pattern = <span class="hljs-string">&#x27;*.html&#x27;</span>
loader = DirectoryLoader(path=path, glob=global_pattern)
docs = loader.load()


<span class="hljs-comment"># Print num documents and a preview.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loaded <span class="hljs-subst">{<span class="hljs-built_in">len</span>(docs)}</span> documents&quot;</span>)
<span class="hljs-built_in">print</span>(docs[<span class="hljs-number">0</span>].page_content)
pprint.pprint(docs[<span class="hljs-number">0</span>].metadata)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">loaded 22 documents
Why Milvus Docs Tutorials Tools Blog Community Stars0 Try Managed Milvus FREE Search Home v2.4.x About ...
{&#x27;source&#x27;: &#x27;https://milvus.io/docs/quickstart.md&#x27;}
<button class="copy-code-btn"></button></code></pre>
<h3 id="Download-an-embedding-model" class="common-anchor-header">T√©l√©chargez un mod√®le d'int√©gration.</h3><p>Ensuite, t√©l√©chargez un <a href="https://zilliz.com/ai-models">mod√®le d'int√©gration</a> gratuit et open-source √† partir de HuggingFace.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer


<span class="hljs-comment"># Initialize torch settings for device-agnostic code.</span>
N_GPU = torch.cuda.device_count()
DEVICE = torch.device(<span class="hljs-string">&#x27;cuda:N_GPU&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)


<span class="hljs-comment"># Download the model from huggingface model hub.</span>
model_name = <span class="hljs-string">&quot;BAAI/bge-large-en-v1.5&quot;</span>
encoder = SentenceTransformer(model_name, device=DEVICE)


<span class="hljs-comment"># Get the model parameters and save for later.</span>
EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()
MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()


<span class="hljs-comment"># Inspect model parameters.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;model_name: <span class="hljs-subst">{model_name}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;EMBEDDING_DIM: <span class="hljs-subst">{EMBEDDING_DIM}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MAX_SEQ_LENGTH: <span class="hljs-subst">{MAX_SEQ_LENGTH}</span>&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">model_name: BAAI/bge-large-en-v1.5
EMBEDDING_DIM: 1024
MAX_SEQ_LENGTH: 512
<button class="copy-code-btn"></button></code></pre>
<h3 id="Chunk-and-encode-your-custom-data-as-vectors" class="common-anchor-header">D√©composez et encodez vos donn√©es personnalis√©es sous forme de vecteurs.</h3><p>J'utiliserai une longueur fixe de 512 caract√®res avec un chevauchement de 10 %.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter


CHUNK_SIZE = <span class="hljs-number">512</span>
chunk_overlap = np.<span class="hljs-built_in">round</span>(CHUNK_SIZE * <span class="hljs-number">0.10</span>, <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;chunk_size: <span class="hljs-subst">{CHUNK_SIZE}</span>, chunk_overlap: <span class="hljs-subst">{chunk_overlap}</span>&quot;</span>)


<span class="hljs-comment"># Define the splitter.</span>
child_splitter = RecursiveCharacterTextSplitter(
   chunk_size=CHUNK_SIZE,
   chunk_overlap=chunk_overlap)


<span class="hljs-comment"># Chunk the docs.</span>
chunks = child_splitter.split_documents(docs)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">len</span>(docs)}</span> docs split into <span class="hljs-subst">{<span class="hljs-built_in">len</span>(chunks)}</span> child documents.&quot;</span>)


<span class="hljs-comment"># Encoder input is doc.page_content as strings.</span>
list_of_strings = [doc.page_content <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> chunks <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(doc, <span class="hljs-string">&#x27;page_content&#x27;</span>)]


<span class="hljs-comment"># Embedding inference using HuggingFace encoder.</span>
embeddings = torch.tensor(encoder.encode(list_of_strings))


<span class="hljs-comment"># Normalize the embeddings.</span>
embeddings = np.array(embeddings / np.linalg.norm(embeddings))


<span class="hljs-comment"># Milvus expects a list of `numpy.ndarray` of `numpy.float32` numbers.</span>
converted_values = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(np.float32, embeddings))


<span class="hljs-comment"># Create dict_list for Milvus insertion.</span>
dict_list = []
<span class="hljs-keyword">for</span> chunk, vector <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(chunks, converted_values):
   <span class="hljs-comment"># Assemble embedding vector, original text chunk, metadata.</span>
   chunk_dict = {
       <span class="hljs-string">&#x27;chunk&#x27;</span>: chunk.page_content,
       <span class="hljs-string">&#x27;source&#x27;</span>: chunk.metadata.get(<span class="hljs-string">&#x27;source&#x27;</span>, <span class="hljs-string">&quot;&quot;</span>),
       <span class="hljs-string">&#x27;vector&#x27;</span>: vector,
   }
   dict_list.append(chunk_dict)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">chunk_size: 512, chunk_overlap: 51.0
22 docs split into 355 child documents.
<button class="copy-code-btn"></button></code></pre>
<h3 id="Save-the-vectors-in-Milvus" class="common-anchor-header">Enregistrez les vecteurs dans Milvus.</h3><p>Int√©grez les vecteurs encod√©s dans la base de donn√©es vectorielle de Milvus.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Connect a client to the Milvus Lite server.</span>
<span class="hljs-keyword">from</span> pymilvus <span class="hljs-keyword">import</span> MilvusClient
mc = MilvusClient(<span class="hljs-string">&quot;milvus_demo.db&quot;</span>)


<span class="hljs-comment"># Create a collection with flexible schema and AUTOINDEX.</span>
COLLECTION_NAME = <span class="hljs-string">&quot;MilvusDocs&quot;</span>
mc.create_collection(COLLECTION_NAME,
       EMBEDDING_DIM,
       consistency_level=<span class="hljs-string">&quot;Eventually&quot;</span>,
       auto_id=<span class="hljs-literal">True</span>, 
       overwrite=<span class="hljs-literal">True</span>)


<span class="hljs-comment"># Insert data into the Milvus collection.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start inserting entities&quot;</span>)
start_time = time.time()
mc.insert(
   COLLECTION_NAME,
   data=dict_list,
   progress_bar=<span class="hljs-literal">True</span>)


end_time = time.time()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Milvus insert time for <span class="hljs-subst">{<span class="hljs-built_in">len</span>(dict_list)}</span> vectors: &quot;</span>, end=<span class="hljs-string">&quot;&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">round</span>(end_time - start_time, <span class="hljs-number">2</span>)}</span> seconds&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">Start inserting entities
Milvus insert time for 355 vectors: 0.2 seconds
<button class="copy-code-btn"></button></code></pre>
<h3 id="Perform-a-vector-search" class="common-anchor-header">Effectuer une recherche de vecteurs.</h3><p>Posez une question et recherchez les morceaux les plus proches de votre base de connaissances dans Milvus.</p>
<pre><code translate="no" class="language-python">SAMPLE_QUESTION = <span class="hljs-string">&quot;What do the parameters for HNSW mean?&quot;</span>


<span class="hljs-comment"># Embed the question using the same encoder.</span>
query_embeddings = torch.tensor(encoder.encode(SAMPLE_QUESTION))
<span class="hljs-comment"># Normalize embeddings to unit length.</span>
query_embeddings = F.normalize(query_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)
<span class="hljs-comment"># Convert the embeddings to list of list of np.float32.</span>
query_embeddings = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(np.float32, query_embeddings))


<span class="hljs-comment"># Define metadata fields you can filter on.</span>
OUTPUT_FIELDS = <span class="hljs-built_in">list</span>(dict_list[<span class="hljs-number">0</span>].keys())
OUTPUT_FIELDS.remove(<span class="hljs-string">&#x27;vector&#x27;</span>)


<span class="hljs-comment"># Define how many top-k results you want to retrieve.</span>
TOP_K = <span class="hljs-number">2</span>


<span class="hljs-comment"># Run semantic vector search using your query and the vector database.</span>
results = mc.search(
    COLLECTION_NAME,
    data=query_embeddings,
    output_fields=OUTPUT_FIELDS,
    limit=TOP_K,
    consistency_level=<span class="hljs-string">&quot;Eventually&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<p>Le r√©sultat de la recherche est illustr√© ci-dessous.</p>
<pre><code translate="no" class="language-text">Retrieved result #1
distance = 0.7001987099647522
(&#x27;Chunk text: layer, finds the node closest to the target in this layer, and&#x27;
...
&#x27;outgoing&#x27;)
source: https://milvus.io/docs/index.md

Retrieved result #2
distance = 0.6953287124633789
(&#x27;Chunk text: this value can improve recall rate at the cost of increased&#x27;
...
&#x27;to the target&#x27;)
source: https://milvus.io/docs/index.md
<button class="copy-code-btn"></button></code></pre>
<h2 id="Build-and-Perform-the-RAG-Generation-with-vLLM-and-Llama-31-8B" class="common-anchor-header">Construire et r√©aliser la g√©n√©ration RAG avec vLLM et Llama 3.1-8B<button data-href="#Build-and-Perform-the-RAG-Generation-with-vLLM-and-Llama-31-8B" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Install-vLLM-and-models-from-HuggingFace" class="common-anchor-header">Installer vLLM et les mod√®les de HuggingFace</h3><p>Par d√©faut, vLLM t√©l√©charge de grands mod√®les linguistiques √† partir de HuggingFace. En g√©n√©ral, chaque fois que vous voulez utiliser un nouveau mod√®le sur HuggingFace, vous devriez faire un pip install --upgrade ou -U. De plus, vous aurez besoin d'un GPU pour ex√©cuter l'inf√©rence des mod√®les Llama 3.1 de Meta avec vLLM.</p>
<p>Pour une liste compl√®te de tous les mod√®les support√©s par vLLM, voir cette <a href="https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models">page de documentation</a>.</p>
<pre><code translate="no" class="language-shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">(Recommended) Create a new conda environment.</span>
conda create -n myenv python=3.11 -y
conda activate myenv
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Install vLLM with CUDA 12.1.</span>
pip install -U vllm transformers torch


import vllm, torch
from vllm import LLM, SamplingParams
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Clear the GPU memory cache.</span>
torch.cuda.empty_cache()
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Check the GPU.</span>
!nvidia-smi
<button class="copy-code-btn"></button></code></pre>
<p>Pour en savoir plus sur l'installation de vLLM, voir sa page d'<a href="https://docs.vllm.ai/en/latest/getting_started/installation.html">installation</a>.</p>
<h3 id="Get-a-HuggingFace-token" class="common-anchor-header">Obtenir un jeton HuggingFace.</h3><p>Certains mod√®les sur HuggingFace, comme Meta Llama 3.1, requi√®rent que l'utilisateur accepte leur licence avant de pouvoir t√©l√©charger les poids. Par cons√©quent, vous devez cr√©er un compte HuggingFace, accepter la licence du mod√®le et g√©n√©rer un jeton.</p>
<p>En visitant cette <a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B">page Llama3.1</a> sur HuggingFace, vous recevrez un message vous demandant d'accepter les termes. Cliquez sur "<strong>Accepter la licence</strong>" pour accepter les termes de Meta avant de t√©l√©charger les poids du mod√®le. L'approbation prend g√©n√©ralement moins d'une journ√©e.</p>
<p><strong>Apr√®s avoir re√ßu l'approbation, vous devez g√©n√©rer un nouveau jeton HuggingFace. Vos anciens jetons ne fonctionneront pas avec les nouvelles autorisations.</strong></p>
<p>Avant d'installer vLLM, connectez-vous √† HuggingFace avec votre nouveau jeton. Ci-dessous, j'ai utilis√© Colab secrets pour stocker le jeton.</p>
<pre><code translate="no" class="language-shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Login to HuggingFace using your new token.</span>
from huggingface_hub import login
from google.colab import userdata
hf_token = userdata.get(&#x27;HF_TOKEN&#x27;)
login(token = hf_token, add_to_git_credential=True)
<button class="copy-code-btn"></button></code></pre>
<h3 id="Run-the-RAG-Generation" class="common-anchor-header">Ex√©cuter la g√©n√©ration RAG</h3><p>Dans la d√©mo, nous ex√©cutons le mod√®le <code translate="no">Llama-3.1-8B</code>, qui n√©cessite un GPU et une m√©moire importante pour tourner. L'exemple suivant a √©t√© ex√©cut√© sur Google Colab Pro (10 $/mois) avec un GPU A100. Pour en savoir plus sur l'ex√©cution de vLLM, vous pouvez consulter la <a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html">documentation Quickstart</a>.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># 1. Choose a model</span>
MODELTORUN = <span class="hljs-string">&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;</span>


<span class="hljs-comment"># 2. Clear the GPU memory cache, you&#x27;re going to need it all!</span>
torch.cuda.empty_cache()


<span class="hljs-comment"># 3. Instantiate a vLLM model instance.</span>
llm = LLM(model=MODELTORUN,
         enforce_eager=<span class="hljs-literal">True</span>,
         dtype=torch.bfloat16,
         gpu_memory_utilization=<span class="hljs-number">0.5</span>,
         max_model_len=<span class="hljs-number">1000</span>,
         seed=<span class="hljs-number">415</span>,
         max_num_batched_tokens=<span class="hljs-number">3000</span>)
<button class="copy-code-btn"></button></code></pre>
<p>R√©digez une invite en utilisant les contextes et les sources r√©cup√©r√©s dans Milvus.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Separate all the context together by space.</span>
contexts_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(contexts)
<span class="hljs-comment"># Lance Martin, LangChain, says put the best contexts at the end.</span>
contexts_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">reversed</span>(contexts))


<span class="hljs-comment"># Separate all the unique sources together by comma.</span>
source_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">dict</span>.fromkeys(sources))))


SYSTEM_PROMPT = <span class="hljs-string">f&quot;&quot;&quot;First, check if the provided Context is relevant to
the user&#x27;s question.  Second, only if the provided Context is strongly relevant, answer the question using the Context.  Otherwise, if the Context is not strongly relevant, answer the question without using the Context. 
Be clear, concise, relevant.  Answer clearly, in fewer than 2 sentences.
Grounding sources: <span class="hljs-subst">{source_combined}</span>
Context: <span class="hljs-subst">{contexts_combined}</span>
User&#x27;s question: <span class="hljs-subst">{SAMPLE_QUESTION}</span>
&quot;&quot;&quot;</span>


prompts = [SYSTEM_PROMPT]
<button class="copy-code-btn"></button></code></pre>
<p>Maintenant, g√©n√©rez une r√©ponse en utilisant les morceaux r√©cup√©r√©s et la question originale ins√©r√©e dans l'invite.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Sampling parameters</span>
sampling_params = SamplingParams(temperature=<span class="hljs-number">0.2</span>, top_p=<span class="hljs-number">0.95</span>)


<span class="hljs-comment"># Invoke the vLLM model.</span>
outputs = llm.generate(prompts, sampling_params)


<span class="hljs-comment"># Print the outputs.</span>
<span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> outputs:
   prompt = output.prompt
   generated_text = output.outputs[<span class="hljs-number">0</span>].text
   <span class="hljs-comment"># !r calls repr(), which prints a string inside quotes.</span>
   <span class="hljs-built_in">print</span>()
   <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{SAMPLE_QUESTION!r}</span>&quot;</span>)
   pprint.pprint(<span class="hljs-string">f&quot;Generated text: <span class="hljs-subst">{generated_text!r}</span>&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">Question: &#x27;What do the parameters for HNSW MEAN!?&#x27;
Generated text: &#x27;Answer: The parameters for HNSW (Hiera(rchical Navigable Small World Graph) are: &#x27;
&#x27;* M: The maximum degree of nodes on each layer oof the graph, which can improve &#x27;
&#x27;recall rate at the cost of increased search time. * efConstruction and ef: &#x27; 
&#x27;These parameters specify a search range when building or searching an index.&#x27;
<button class="copy-code-btn"></button></code></pre>
<p>La r√©ponse ci-dessus me semble parfaite !</p>
<p>Si cette d√©mo vous int√©resse, n'h√©sitez pas √† l'essayer vous-m√™me et √† nous faire part de vos impressions. Vous √™tes √©galement invit√©s √† rejoindre notre <a href="https://discord.com/invite/8uyFbECzPX">communaut√© Milvus sur Discord</a> pour discuter directement avec tous les d√©veloppeurs GenAI.</p>
<h2 id="References" class="common-anchor-header">R√©f√©rences<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><p><a href="https://docs.vllm.ai/en/latest/getting_started/installation.html">Documentation officielle de</a> vLLM et <a href="https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models">page du mod√®le</a>.</p></li>
<li><p><a href="https://arxiv.org/pdf/2309.06180">2023 vLLM paper on Paged Attention (en anglais)</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=80bIUggRJf4">2023 pr√©sentation de vLLM</a> au Ray Summit</p></li>
<li><p>Blog vLLM : <a href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM : Easy, Fast, and Cheap LLM Serving with PagedAttention</a> (en anglais)</p></li>
<li><p>Blog utile sur le fonctionnement du serveur vLLM : <a href="https://ploomber.io/blog/vllm-deploy/">D√©ploiement de vLLM : un guide √©tape par √©tape</a></p></li>
<li><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">Le troupeau de mod√®les Llama 3 | Recherche - AI at Meta</a></p></li>
</ul>
