{"codeList":["pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-text-splitters langchain-milvus langchain-openai bs4\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","import bs4\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Create a WebBaseLoader instance to load documents from web sources\nloader = WebBaseLoader(\n    web_paths=(\n        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    ),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\n# Load documents from web sources using the loader\ndocuments = loader.load()\n# Initialize a RecursiveCharacterTextSplitter for splitting text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n\n# Split the documents into chunks using the text_splitter\ndocs = text_splitter.split_documents(documents)\n\n# Let's take a look at the first document\ndocs[1]\n","from langchain_milvus import Milvus\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\nvectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    connection_args={\n        \"uri\": \"./milvus_demo.db\",\n    },\n    drop_old=False,  # Drop the old Milvus collection if it exists\n)\n","query = \"What is self-reflection of an AI Agent?\"\nvectorstore.similarity_search(query, k=1)\n","from langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the OpenAI language model for response generation\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# Define the prompt template for generating AI responses\nPROMPT_TEMPLATE = \"\"\"\nHuman: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\nUse the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n<context>\n{context}\n</context>\n\n<question>\n{question}\n</question>\n\nThe response should be specific and use statistics or numbers when possible.\n\nAssistant:\"\"\"\n\n# Create a PromptTemplate instance with the defined template and input variables\nprompt = PromptTemplate(\n    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n)\n# Convert the vector store to a retriever\nretriever = vectorstore.as_retriever()\n\n\n# Define a function to format the retrieved documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","# Define the RAG (Retrieval-Augmented Generation) chain for AI response generation\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# rag_chain.get_graph().print_ascii()\n\n# Invoke the RAG chain with a specific question and retrieve the response\nres = rag_chain.invoke(query)\nres\n","vectorstore.similarity_search(\n    \"What is CoT?\",\n    k=1,\n    expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n)\n\n# The same as:\n# vectorstore.as_retriever(search_kwargs=dict(\n#     k=1,\n#     expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n# )).invoke(\"What is CoT?\")\n","from langchain_core.runnables import ConfigurableField\n\n# Define a new retriever with a configurable field for search_kwargs\nretriever2 = vectorstore.as_retriever().configurable_fields(\n    search_kwargs=ConfigurableField(\n        id=\"retriever_search_kwargs\",\n    )\n)\n\n# Invoke the retriever with a specific search_kwargs which filter the documents by source\nretriever2.with_config(\n    configurable={\n        \"retriever_search_kwargs\": dict(\n            expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n            k=1,\n        )\n    }\n).invoke(query)\n","# Define a new RAG chain with this dynamically configurable retriever\nrag_chain2 = (\n    {\"context\": retriever2 | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n","# Invoke this RAG chain with a specific question and config\nrag_chain2.with_config(\n    configurable={\n        \"retriever_search_kwargs\": dict(\n            expr=\"source == 'https://lilianweng.github.io/posts/2023-06-23-agent/'\",\n        )\n    }\n).invoke(query)\n","rag_chain2.with_config(\n    configurable={\n        \"retriever_search_kwargs\": dict(\n            expr=\"source == 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\",\n        )\n    }\n).invoke(query)\n"],"headingContent":"Retrieval-Augmented Generation (RAG) with Milvus and LangChain","anchorList":[{"label":"Retrieval-erweiterte Generierung (RAG) mit Milvus und LangChain","href":"Retrieval-Augmented-Generation-RAG-with-Milvus-and-LangChain","type":1,"isActive":false},{"label":"Voraussetzungen","href":"Prerequisites","type":2,"isActive":false},{"label":"Bereiten Sie die Daten vor","href":"Prepare-the-data","type":2,"isActive":false},{"label":"RAG-Kette mit Milvus-Vektorspeicher aufbauen","href":"Build-RAG-chain-with-Milvus-Vector-Store","type":2,"isActive":false},{"label":"Filtern von Metadaten","href":"Metadata-filtering","type":2,"isActive":false}]}