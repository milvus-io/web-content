{"codeList":["$ pip install llama-index-vector-stores-milvus\n$ pip install llama-index-embeddings-openai\n$ pip install llama-index-llms-openai\n","import openai\n\nopenai.api_key = \"sk-\"\n","URI = \"http://localhost:19530\"\n# TOKEN = \"\"\n","$ mkdir -p 'data/paul_graham/'\n$ wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n","from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# Let's take a look at the first document\nprint(\"Example document:\\n\", documents[0])\n","# Create an index over the documnts\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\nfrom llama_index.core import StorageContext, VectorStoreIndex\n\n\nvector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    dim=1536,  # vector dimension depends on the embedding model\n    enable_sparse=True,  # enable the default full-text search using BM25\n    overwrite=True,  # drop the collection if it already exists\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n","import textwrap\n\nquery_engine = index.as_query_engine(\n    vector_store_query_mode=\"hybrid\", similarity_top_k=5\n)\nresponse = query_engine.query(\"What did the author learn at Viaweb?\")\nprint(textwrap.fill(str(response), 100))\n","from llama_index.vector_stores.milvus.utils import BM25BuiltInFunction\n\nbm25_function = BM25BuiltInFunction(\n    analyzer_params={\n        \"tokenizer\": \"standard\",\n        \"filter\": [\n            \"lowercase\",  # Built-in filter\n            {\"type\": \"length\", \"max\": 40},  # Custom cap size of a single token\n            {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom stopwords\n        ],\n    },\n    enable_match=True,\n)\n\nvector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    dim=1536,\n    enable_sparse=True,\n    sparse_embedding_function=bm25_function,  # BM25 with custom analyzer\n    overwrite=True,\n)\n","$ pip install -q FlagEmbedding\n","from llama_index.vector_stores.milvus.utils import BGEM3SparseEmbeddingFunction\n\nvector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    dim=1536,\n    enable_sparse=True,\n    sparse_embedding_function=BGEM3SparseEmbeddingFunction(),\n    overwrite=True,\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n","query_engine = index.as_query_engine(\n    vector_store_query_mode=\"hybrid\", similarity_top_k=5\n)\nresponse = query_engine.query(\"What did the author learn at Viaweb??\")\nprint(textwrap.fill(str(response), 100))\n","from FlagEmbedding import BGEM3FlagModel\nfrom typing import List\nfrom llama_index.vector_stores.milvus.utils import BaseSparseEmbeddingFunction\n\n\nclass ExampleEmbeddingFunction(BaseSparseEmbeddingFunction):\n    def __init__(self):\n        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n\n    def encode_queries(self, queries: List[str]):\n        outputs = self.model.encode(\n            queries,\n            return_dense=False,\n            return_sparse=True,\n            return_colbert_vecs=False,\n        )[\"lexical_weights\"]\n        return [self._to_standard_dict(output) for output in outputs]\n\n    def encode_documents(self, documents: List[str]):\n        outputs = self.model.encode(\n            documents,\n            return_dense=False,\n            return_sparse=True,\n            return_colbert_vecs=False,\n        )[\"lexical_weights\"]\n        return [self._to_standard_dict(output) for output in outputs]\n\n    def _to_standard_dict(self, raw_output):\n        result = {}\n        for k in raw_output:\n            result[int(k)] = raw_output[k]\n        return result\n","vector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    dim=1536,\n    overwrite=False,  # Use the existing collection created in the previous example\n    enable_sparse=True,\n    hybrid_ranker=\"WeightedRanker\",\n    hybrid_ranker_params={\"weights\": [1.0, 0.5]},\n)\nindex = VectorStoreIndex.from_vector_store(vector_store)\nquery_engine = index.as_query_engine(\n    vector_store_query_mode=\"hybrid\", similarity_top_k=5\n)\nresponse = query_engine.query(\"What did the author learn at Viaweb?\")\nprint(textwrap.fill(str(response), 100))\n"],"headingContent":"RAG using Hybrid Search with Milvus and LlamaIndex","anchorList":[{"label":"RAG nutzt hybride Suche mit Milvus und LlamaIndex","href":"RAG-using-Hybrid-Search-with-Milvus-and-LlamaIndex","type":1,"isActive":false},{"label":"Voraussetzungen","href":"Prerequisites","type":2,"isActive":false},{"label":"Hybride Suche mit BM25","href":"Hybrid-Search-with-BM25","type":2,"isActive":false},{"label":"Hybride Suche mit anderer sp√§rlicher Einbettung","href":"Hybrid-Search-with-Other-Sparse-Embedding","type":2,"isActive":false},{"label":"Anpassen des Hybrid-Rerankers","href":"Customize-hybrid-reranker","type":2,"isActive":false}]}