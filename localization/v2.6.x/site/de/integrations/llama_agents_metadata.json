{"codeList":["$ pip install llama-agents pymilvus openai python-dotenv\n","$ pip install llama-index-vector-stores-milvus llama-index-readers-file llama-index-llms-ollama llama-index-llms-mistralai llama-index-embeddings-mistralai\n","# NOTE: This is ONLY necessary in jupyter notebook.\n# Details: Jupyter runs an event-loop behind the scenes.\n#          This results in nested event-loops when we start an event-loop to make async queries.\n#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\nimport nest_asyncio\n\nnest_asyncio.apply()\n","\"\"\"\nload_dotenv reads key-value pairs from a .env file and can set them as environment variables.\nThis is useful to avoid leaking your API key for example :D\n\"\"\"\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n","$ mkdir -p 'data/10k/'\n$ wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n$ wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\n","from llama_index.core import Settings\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\n\n# Define the default Embedding model used in this Notebook.\n# We are using Mistral Models, so we are also using Mistral Embeddings\n\nSettings.embed_model = MistralAIEmbedding(model_name=\"mistral-embed\")\n","from llama_index.llms.ollama import Ollama\n\nSettings.llm = Ollama(\"mistral-nemo\")\n","from llama_index.vector_stores.milvus import MilvusVectorStore\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    StorageContext,\n    load_index_from_storage,\n)\nfrom llama_index.core.tools import QueryEngineTool, ToolMetadata\n\ninput_files = [\"./data/10k/lyft_2021.pdf\", \"./data/10k/uber_2021.pdf\"]\n\n# Create a single Milvus vector store\nvector_store = MilvusVectorStore(\n    uri=\"./milvus_demo.db\", dim=1024, overwrite=False, collection_name=\"companies_docs\"\n)\n\n# Create a storage context with the Milvus vector store\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Load data\ndocs = SimpleDirectoryReader(input_files=input_files).load_data()\n\n# Build index\nindex = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n\n# Define the query engine\ncompany_engine = index.as_query_engine(similarity_top_k=3)\n","# Define the different tools that can be used by our Agent.\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=company_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=(\n                \"Provides information about Lyft financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n                \"Do not attempt to interpret or summarize the data.\"\n            ),\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=company_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=(\n                \"Provides information about Uber financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n                \"Do not attempt to interpret or summarize the data.\"\n            ),\n        ),\n    ),\n]\n","from llama_index.llms.ollama import Ollama\nfrom llama_index.llms.mistralai import MistralAI\n\n# Set up the agent\nllm = Ollama(model=\"mistral-nemo\")\n\nresponse = llm.predict_and_call(\n    query_engine_tools,\n    user_msg=\"Could you please provide a comparison between Lyft and Uber's total revenues in 2021?\",\n    allow_parallel_tool_calls=True,\n)\n\n# Example usage without metadata filtering\nprint(\"Response without metadata filtering:\")\nprint(response)\n","from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters\n\n# Example usage with metadata filtering\nfilters = MetadataFilters(\n    filters=[ExactMatchFilter(key=\"file_name\", value=\"lyft_2021.pdf\")]\n)\n\nprint(f\"filters: {filters}\")\nfiltered_query_engine = index.as_query_engine(filters=filters)\n\n# Define query engine tools with the filtered query engine\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=filtered_query_engine,\n        metadata=ToolMetadata(\n            name=\"company_docs\",\n            description=(\n                \"Provides information about various companies' financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n                \"Use this tool to retrieve specific data points about a company. \"\n                \"Do not attempt to interpret or summarize the data.\"\n            ),\n        ),\n    ),\n]\n","# Set up the LLM we will use for Function Calling\n\nllm = Ollama(model=\"mistral-nemo\")\n","response = llm.predict_and_call(\n    query_engine_tools,\n    user_msg=\"How many employees does Uber have?\",\n    allow_parallel_tool_calls=True,\n)\nprint(response)\n","response = llm.predict_and_call(\n    query_engine_tools,\n    user_msg=\"What are the risk factors for Lyft?\",\n    allow_parallel_tool_calls=True,\n)\n\nprint(response)\n","> Question: What are the risk factors for Uber?\n\n> Response without metadata filtering:\nBased on the provided context, which pertains to Lyft's Risk Factors section in their Annual Report, some of the potential risk factors applicable to a company like Uber might include:\n\n- General economic factors such as the impact of global pandemics or other crises on ride-sharing demand.\n- Operational factors like competition in ride-hailing services, unpredictability in results of operations, and uncertainty about market growth for ridesharing and related services.\n- Risks related to attracting and retaining qualified drivers and riders.\n","from llama_index.core.prompts.base import PromptTemplate\n\n\n# Function to create a filtered query engine\ndef create_query_engine(question):\n    # Extract metadata filters from question using a language model\n    prompt_template = PromptTemplate(\n        \"Given the following question, extract relevant metadata filters.\\n\"\n        \"Consider company names, years, and any other relevant attributes.\\n\"\n        \"Don't write any other text, just the MetadataFilters object\"\n        \"Format it by creating a MetadataFilters like shown in the following\\n\"\n        \"MetadataFilters(filters=[ExactMatchFilter(key='file_name', value='lyft_2021.pdf')])\\n\"\n        \"If no specific filters are mentioned, returns an empty MetadataFilters()\\n\"\n        \"Question: {question}\\n\"\n        \"Metadata Filters:\\n\"\n    )\n\n    prompt = prompt_template.format(question=question)\n    llm = Ollama(model=\"mistral-nemo\")\n    response = llm.complete(prompt)\n\n    metadata_filters_str = response.text.strip()\n    if metadata_filters_str:\n        metadata_filters = eval(metadata_filters_str)\n        print(f\"eval: {metadata_filters}\")\n        return index.as_query_engine(filters=metadata_filters)\n    return index.as_query_engine()\n","response = create_query_engine(\n    \"What is Uber revenue? This should be in the file_name: uber_2021.pdf\"\n)\n","## Example usage with metadata filtering\nquestion = \"What is Uber revenue? This should be in the file_name: uber_2021.pdf\"\nfiltered_query_engine = create_query_engine(question)\n\n# Define query engine tools with the filtered query engine\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=filtered_query_engine,\n        metadata=ToolMetadata(\n            name=\"company_docs_filtering\",\n            description=(\n                \"Provides information about various companies' financials for year 2021. \"\n                \"Use a detailed plain text question as input to the tool.\"\n            ),\n        ),\n    ),\n]\n# Set up the agent with the updated query engine tools\nresponse = llm.predict_and_call(\n    query_engine_tools,\n    user_msg=question,\n    allow_parallel_tool_calls=True,\n)\n\nprint(\"Response with metadata filtering:\")\nprint(response)\n","from llama_agents import (\n    AgentService,\n    ToolService,\n    LocalLauncher,\n    MetaServiceTool,\n    ControlPlaneServer,\n    SimpleMessageQueue,\n    AgentOrchestrator,\n)\n\nfrom llama_index.core.agent import FunctionCallingAgentWorker\nfrom llama_index.llms.mistralai import MistralAI\n\n# create our multi-agent framework components\nmessage_queue = SimpleMessageQueue()\ncontrol_plane = ControlPlaneServer(\n    message_queue=message_queue,\n    orchestrator=AgentOrchestrator(llm=MistralAI(\"mistral-large-latest\")),\n)\n\n# define Tool Service\ntool_service = ToolService(\n    message_queue=message_queue,\n    tools=query_engine_tools,\n    running=True,\n    step_interval=0.5,\n)\n\n# define meta-tools here\nmeta_tools = [\n    await MetaServiceTool.from_tool_service(\n        t.metadata.name,\n        message_queue=message_queue,\n        tool_service=tool_service,\n    )\n    for t in query_engine_tools\n]\n\n# define Agent and agent service\nworker1 = FunctionCallingAgentWorker.from_tools(\n    meta_tools, llm=MistralAI(\"mistral-large-latest\")\n)\n\nagent1 = worker1.as_agent()\nagent_server_1 = AgentService(\n    agent=agent1,\n    message_queue=message_queue,\n    description=\"Used to answer questions over differnet companies for their Financial results\",\n    service_name=\"Companies_analyst_agent\",\n)\n","import logging\n\n# change logging level to enable or disable more verbose logging\nlogging.getLogger(\"llama_agents\").setLevel(logging.INFO)\n","## Define Launcher\nlauncher = LocalLauncher(\n    [agent_server_1, tool_service],\n    control_plane,\n    message_queue,\n)\n","query_str = \"What are the risk factors for Uber?\"\nresult = launcher.launch_single(query_str)\n","print(result)\n"],"headingContent":"Multi-agent Systems with Mistral AI, Milvus and Llama-agents","anchorList":[{"label":"Multi-agent Systems with Mistral AI, Milvus and Llama-agents","href":"Multi-agent-Systems-with-Mistral-AI-Milvus-and-Llama-agents","type":1,"isActive":false},{"label":"Goal of this Notebook","href":"Goal-of-this-Notebook","type":2,"isActive":false},{"label":"Milvus","href":"Milvus","type":2,"isActive":false},{"label":"llama-agents","href":"llama-agents","type":2,"isActive":false},{"label":"llama-index","href":"llama-index","type":2,"isActive":false},{"label":"Mistral AI","href":"Mistral-AI","type":2,"isActive":false},{"label":"Install Dependencies","href":"Install-Dependencies","type":2,"isActive":false},{"label":"Get your API Key for Mistral","href":"Get-your-API-Key-for-Mistral","type":2,"isActive":false},{"label":"Download data","href":"Download-data","type":2,"isActive":false},{"label":"Prepare Embedding Model","href":"Prepare-Embedding-Model","type":1,"isActive":false},{"label":"Define the LLM Model","href":"Define-the-LLM-Model","type":2,"isActive":false},{"label":"Instanciate Milvus and Load Data","href":"Instanciate-Milvus-and-Load-Data","type":2,"isActive":false},{"label":"Define Tools","href":"Define-Tools","type":2,"isActive":false},{"label":"Metadata Filtering","href":"Metadata-Filtering","type":2,"isActive":false},{"label":"Use Cases for Metadata Filtering","href":"Use-Cases-for-Metadata-Filtering","type":2,"isActive":false},{"label":"Example usage","href":"Example-usage","type":2,"isActive":false},{"label":"Function Calling","href":"Function-Calling","type":2,"isActive":false},{"label":"Interact with the Agent","href":"Interact-with-the-Agent","type":2,"isActive":false},{"label":"Example of Confusion Without Metadata Filtering","href":"Example-of-Confusion-Without-Metadata-Filtering","type":2,"isActive":false},{"label":"Using an Agent to Extract Metadata Filters","href":"Using-an-Agent-to-Extract-Metadata-Filters","type":2,"isActive":false},{"label":"Code Example","href":"Code-Example","type":2,"isActive":false},{"label":"Orchestrating the different services with Mistral Large","href":"Orchestrating-the-different-services-with-Mistral-Large","type":2,"isActive":false},{"label":"Conclusion","href":"Conclusion","type":2,"isActive":false}]}