{"codeList":["$ $pip install llama-index-vector-stores-milvus\n$ $pip install llama-index-embeddings-openai\n$ $pip install llama-index-llms-openai\n","import openai\n\nopenai.api_key = \"sk-\"\n","URI = \"http://localhost:19530\"\n# TOKEN = \"\"\n","$ mkdir -p 'data/paul_graham/'\n$ $wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\n","from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n\n# Let's take a look at the first document\nprint(\"Example document:\\n\", documents[0])\n","from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\nfrom llama_index.vector_stores.milvus.utils import BM25BuiltInFunction\nfrom llama_index.core import Settings\n\n# Skip dense embedding model\nSettings.embed_model = None\n\n# Build Milvus vector store creating a new collection\nvector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    enable_dense=False,\n    enable_sparse=True,  # Only enable sparse to demo full text search\n    sparse_embedding_function=BM25BuiltInFunction(),\n    overwrite=True,\n)\n\n# Store documents in Milvus\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n","import textwrap\n\nquery_engine = index.as_query_engine(\n    vector_store_query_mode=\"sparse\", similarity_top_k=5\n)\nanswer = query_engine.query(\"What did the author learn at Viaweb?\")\nprint(textwrap.fill(str(answer), 100))\n","bm25_function = BM25BuiltInFunction(\n    analyzer_params={\n        \"tokenizer\": \"standard\",\n        \"filter\": [\n            \"lowercase\",  # Built-in filter\n            {\"type\": \"length\", \"max\": 40},  # Custom cap size of a single token\n            {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom stopwords\n        ],\n    },\n    enable_match=True,\n)\n","# Create index over the documnts\nvector_store = MilvusVectorStore(\n    uri=URI,\n    # token=TOKEN,\n    # enable_dense=True,  # enable_dense defaults to True\n    dim=1536,\n    enable_sparse=True,\n    sparse_embedding_function=BM25BuiltInFunction(),\n    overwrite=True,\n    # hybrid_ranker=\"RRFRanker\",  # hybrid_ranker defaults to \"RRFRanker\"\n    # hybrid_ranker_params={},  # hybrid_ranker_params defaults to {}\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n    embed_model=\"default\",  # \"default\" will use OpenAI embedding\n)\n","# Query\nquery_engine = index.as_query_engine(\n    vector_store_query_mode=\"hybrid\", similarity_top_k=5\n)\nanswer = query_engine.query(\"What did the author learn at Viaweb?\")\nprint(textwrap.fill(str(answer), 100))\n"],"headingContent":"Using Full-Text Search with LlamaIndex and Milvus","anchorList":[{"label":"Verwendung der Volltextsuche mit LlamaIndex und Milvus","href":"Using-Full-Text-Search-with-LlamaIndex-and-Milvus","type":1,"isActive":false},{"label":"Voraussetzungen","href":"Prerequisites","type":2,"isActive":false},{"label":"RAG mit Volltextsuche","href":"RAG-with-Full-Text-Search","type":2,"isActive":false}]}