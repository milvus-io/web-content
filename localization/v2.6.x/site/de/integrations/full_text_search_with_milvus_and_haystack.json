{"codeList":["$ pip install --upgrade --quiet pymilvus milvus-haystack\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","from haystack import Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.utils import Secret\nfrom milvus_haystack import MilvusDocumentStore, MilvusSparseEmbeddingRetriever\nfrom haystack.document_stores.types import DuplicatePolicy\nfrom milvus_haystack.function import BM25BuiltInFunction\nfrom milvus_haystack import MilvusDocumentStore\nfrom milvus_haystack.milvus_embedding_retriever import MilvusHybridRetriever\n\nfrom haystack.utils import Secret\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack import Document\n\ndocuments = [\n    Document(content=\"Alice likes this apple\", meta={\"category\": \"fruit\"}),\n    Document(content=\"Bob likes swimming\", meta={\"category\": \"sport\"}),\n    Document(content=\"Charlie likes white dogs\", meta={\"category\": \"pets\"}),\n]\n","connection_args = {\"uri\": \"http://localhost:19530\"}\n# connection_args = {\"uri\": YOUR_ZILLIZ_CLOUD_URI, \"token\": Secret.from_env_var(\"ZILLIZ_CLOUD_API_KEY\")}\n\ndocument_store = MilvusDocumentStore(\n    connection_args=connection_args,\n    sparse_vector_field=\"sparse_vector\",  # The sparse vector field.\n    text_field=\"text\",\n    builtin_function=[\n        BM25BuiltInFunction(  # The BM25 function converts the text into a sparse vector.\n            input_field_names=\"text\",\n            output_field_names=\"sparse_vector\",\n        )\n    ],\n    consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`).\n    drop_old=True,  # Drop the old collection if it exists and recreate it.\n)\n","writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"writer\", writer)\nindexing_pipeline.run({\"writer\": {\"documents\": documents}})\n","retrieval_pipeline = Pipeline()\nretrieval_pipeline.add_component(\n    \"retriever\", MilvusSparseEmbeddingRetriever(document_store=document_store)\n)\n\nquestion = \"Who likes swimming?\"\n\nretrieval_results = retrieval_pipeline.run({\"retriever\": {\"query_text\": question}})\n\nretrieval_results[\"retriever\"][\"documents\"][0]\n","document_store = MilvusDocumentStore(\n    connection_args=connection_args,\n    vector_field=\"vector\",  # The dense vector field.\n    sparse_vector_field=\"sparse_vector\",  # The sparse vector field.\n    text_field=\"text\",\n    builtin_function=[\n        BM25BuiltInFunction(  # The BM25 function converts the text into a sparse vector.\n            input_field_names=\"text\",\n            output_field_names=\"sparse_vector\",\n        )\n    ],\n    consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`).\n    drop_old=True,  # Drop the old collection and recreate it.\n)\n","writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\n\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"dense_doc_embedder\", OpenAIDocumentEmbedder())\nindexing_pipeline.add_component(\"writer\", writer)\nindexing_pipeline.connect(\"dense_doc_embedder\", \"writer\")\nindexing_pipeline.run({\"dense_doc_embedder\": {\"documents\": documents}})\n\nprint(\"Number of documents:\", document_store.count_documents())\n","# from pymilvus import WeightedRanker\nretrieval_pipeline = Pipeline()\nretrieval_pipeline.add_component(\"dense_text_embedder\", OpenAITextEmbedder())\nretrieval_pipeline.add_component(\n    \"retriever\",\n    MilvusHybridRetriever(\n        document_store=document_store,\n        # top_k=3,\n        # reranker=WeightedRanker(0.5, 0.5),  # Default is RRFRanker()\n    ),\n)\n\nretrieval_pipeline.connect(\"dense_text_embedder.embedding\", \"retriever.query_embedding\")\n","question = \"Who likes swimming?\"\n\nretrieval_results = retrieval_pipeline.run(\n    {\n        \"dense_text_embedder\": {\"text\": question},\n        \"retriever\": {\"query_text\": question},\n    }\n)\n\nretrieval_results[\"retriever\"][\"documents\"][0]\n","analyzer_params_custom = {\n    \"tokenizer\": \"standard\",\n    \"filter\": [\n        \"lowercase\",  # Built-in filter\n        {\"type\": \"length\", \"max\": 40},  # Custom filter\n        {\"type\": \"stop\", \"stop_words\": [\"of\", \"to\"]},  # Custom filter\n    ],\n}\n\ndocument_store = MilvusDocumentStore(\n    connection_args=connection_args,\n    vector_field=\"vector\",\n    sparse_vector_field=\"sparse_vector\",\n    text_field=\"text\",\n    builtin_function=[\n        BM25BuiltInFunction(\n            input_field_names=\"text\",\n            output_field_names=\"sparse_vector\",\n            analyzer_params=analyzer_params_custom,  # Custom analyzer parameters.\n            enable_match=True,  # Whether to enable match.\n        )\n    ],\n    consistency_level=\"Strong\",\n    drop_old=True,\n)\n\n# write documents to the document store\nwriter = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.NONE)\nindexing_pipeline = Pipeline()\nindexing_pipeline.add_component(\"dense_doc_embedder\", OpenAIDocumentEmbedder())\nindexing_pipeline.add_component(\"writer\", writer)\nindexing_pipeline.connect(\"dense_doc_embedder\", \"writer\")\nindexing_pipeline.run({\"dense_doc_embedder\": {\"documents\": documents}})\n","document_store.col.schema\n","prompt_template = \"\"\"Answer the following query based on the provided context. If the context does\n                     not include an answer, reply with 'I don't know'.\\n\n                     Query: {{query}}\n                     Documents:\n                     {% for doc in documents %}\n                        {{ doc.content }}\n                     {% endfor %}\n                     Answer:\n                  \"\"\"\n\nrag_pipeline = Pipeline()\nrag_pipeline.add_component(\"text_embedder\", OpenAITextEmbedder())\nrag_pipeline.add_component(\n    \"retriever\", MilvusHybridRetriever(document_store=document_store, top_k=1)\n)\nrag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\nrag_pipeline.add_component(\n    \"generator\",\n    OpenAIGenerator(\n        api_key=Secret.from_token(os.getenv(\"OPENAI_API_KEY\")),\n        generation_kwargs={\"temperature\": 0},\n    ),\n)\nrag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\nrag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\nrag_pipeline.connect(\"prompt_builder\", \"generator\")\n\nresults = rag_pipeline.run(\n    {\n        \"text_embedder\": {\"text\": question},\n        \"retriever\": {\"query_text\": question},\n        \"prompt_builder\": {\"query\": question},\n    }\n)\nprint(\"RAG answer:\", results[\"generator\"][\"replies\"][0])\n"],"headingContent":"Full-text search with Milvus and Haystack","anchorList":[{"label":"Volltextsuche mit Milvus und Haystack","href":"Full-text-search-with-Milvus-and-Haystack","type":1,"isActive":false},{"label":"Voraussetzungen","href":"Prerequisites","type":2,"isActive":false},{"label":"BM25-Suche ohne Einbettung","href":"BM25-search-without-embedding","type":2,"isActive":false},{"label":"Hybride Suche mit semantischer Suche und Volltextsuche","href":"Hybrid-Search-with-semantic-search-and-full-text-search","type":2,"isActive":false},{"label":"Anpassen des Analyzers","href":"Customize-analyzer","type":2,"isActive":false},{"label":"Verwendung der hybriden Suche in der RAG-Pipeline","href":"Using-Hybrid-Search-in-RAG-pipeline","type":2,"isActive":false}]}