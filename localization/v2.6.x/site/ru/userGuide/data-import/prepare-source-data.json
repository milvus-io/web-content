{"codeList":["from pymilvus import MilvusClient, DataType\n\n# You need to work out a collection schema out of your dataset.\nschema = MilvusClient.create_schema(\n    auto_id=False,\n    enable_dynamic_field=True\n)\n\nDIM = 512\n\nschema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True),\nschema.add_field(field_name=\"bool\", datatype=DataType.BOOL),\nschema.add_field(field_name=\"int8\", datatype=DataType.INT8),\nschema.add_field(field_name=\"int16\", datatype=DataType.INT16),\nschema.add_field(field_name=\"int32\", datatype=DataType.INT32),\nschema.add_field(field_name=\"int64\", datatype=DataType.INT64),\nschema.add_field(field_name=\"float\", datatype=DataType.FLOAT),\nschema.add_field(field_name=\"double\", datatype=DataType.DOUBLE),\nschema.add_field(field_name=\"varchar\", datatype=DataType.VARCHAR, max_length=512),\nschema.add_field(field_name=\"json\", datatype=DataType.JSON),\nschema.add_field(field_name=\"array_str\", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.VARCHAR, max_length=128)\nschema.add_field(field_name=\"array_int\", datatype=DataType.ARRAY, max_capacity=100, element_type=DataType.INT64)\nschema.add_field(field_name=\"float_vector\", datatype=DataType.FLOAT_VECTOR, dim=DIM),\nschema.add_field(field_name=\"binary_vector\", datatype=DataType.BINARY_VECTOR, dim=DIM),\nschema.add_field(field_name=\"float16_vector\", datatype=DataType.FLOAT16_VECTOR, dim=DIM),\n# schema.add_field(field_name=\"bfloat16_vector\", datatype=DataType.BFLOAT16_VECTOR, dim=DIM),\nschema.add_field(field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n\nschema.verify()\n\nprint(schema)\n","import com.google.gson.Gson;\nimport com.google.gson.JsonObject;\nimport io.milvus.bulkwriter.BulkImport;\nimport io.milvus.bulkwriter.RemoteBulkWriter;\nimport io.milvus.bulkwriter.RemoteBulkWriterParam;\nimport io.milvus.bulkwriter.common.clientenum.BulkFileType;\nimport io.milvus.bulkwriter.common.clientenum.CloudStorage;\nimport io.milvus.bulkwriter.connect.S3ConnectParam;\nimport io.milvus.bulkwriter.connect.StorageConnectParam;\nimport io.milvus.bulkwriter.request.describe.MilvusDescribeImportRequest;\nimport io.milvus.bulkwriter.request.import_.MilvusImportRequest;\nimport io.milvus.bulkwriter.request.list.MilvusListImportJobsRequest;\nimport io.milvus.common.utils.Float16Utils;\nimport io.milvus.v2.client.ConnectConfig;\nimport io.milvus.v2.client.MilvusClientV2;\nimport io.milvus.v2.common.DataType;\nimport io.milvus.v2.service.collection.request.*;\n\nimport java.io.IOException;\nimport java.nio.ByteBuffer;\nimport java.util.*;\nimport java.util.concurrent.TimeUnit;\n\nprivate static final String MINIO_ENDPOINT = CloudStorage.MINIO.getEndpoint(\"http://127.0.0.1:9000\");\nprivate static final String BUCKET_NAME = \"a-bucket\";\nprivate static final String ACCESS_KEY = \"minioadmin\";\nprivate static final String SECRET_KEY = \"minioadmin\";\n\nprivate static final Integer DIM = 512;\nprivate static final Gson GSON_INSTANCE = new Gson();\n\nprivate static CreateCollectionReq.CollectionSchema createSchema() {\n    CreateCollectionReq.CollectionSchema schema = CreateCollectionReq.CollectionSchema.builder()\n        .enableDynamicField(true)\n        .build();\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"id\")\n            .dataType(io.milvus.v2.common.DataType.Int64)\n            .isPrimaryKey(Boolean.TRUE)\n            .autoID(false)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"bool\")\n            .dataType(DataType.Bool)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"int8\")\n            .dataType(DataType.Int8)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"int16\")\n            .dataType(DataType.Int16)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"int32\")\n            .dataType(DataType.Int32)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"int64\")\n            .dataType(DataType.Int64)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"float\")\n            .dataType(DataType.Float)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"double\")\n            .dataType(DataType.Double)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"varchar\")\n            .dataType(DataType.VarChar)\n            .maxLength(512)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"json\")\n            .dataType(io.milvus.v2.common.DataType.JSON)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"array_int\")\n            .dataType(io.milvus.v2.common.DataType.Array)\n            .maxCapacity(100)\n            .elementType(io.milvus.v2.common.DataType.Int64)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"array_str\")\n            .dataType(io.milvus.v2.common.DataType.Array)\n            .maxCapacity(100)\n            .elementType(io.milvus.v2.common.DataType.VarChar)\n            .maxLength(128)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"float_vector\")\n            .dataType(io.milvus.v2.common.DataType.FloatVector)\n            .dimension(DIM)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"binary_vector\")\n            .dataType(io.milvus.v2.common.DataType.BinaryVector)\n            .dimension(DIM)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"float16_vector\")\n            .dataType(io.milvus.v2.common.DataType.Float16Vector)\n            .dimension(DIM)\n            .build());\n    schema.addField(AddFieldReq.builder()\n            .fieldName(\"sparse_vector\")\n            .dataType(io.milvus.v2.common.DataType.SparseFloatVector)\n            .build());\n    \n    return schema;\n}\n","from pymilvus.bulk_writer import LocalBulkWriter, BulkFileType\n# Use `from pymilvus import LocalBulkWriter, BulkFileType` \n# when you use pymilvus earlier than 2.4.2 \n\nwriter = LocalBulkWriter(\n    schema=schema,\n    local_path='.',\n    segment_size=512 * 1024 * 1024, # Default value\n    file_type=BulkFileType.PARQUET\n)\n","import io.milvus.bulkwriter.LocalBulkWriter;\nimport io.milvus.bulkwriter.LocalBulkWriterParam;\nimport io.milvus.bulkwriter.common.clientenum.BulkFileType;\n\nLocalBulkWriterParam localBulkWriterParam = LocalBulkWriterParam.newBuilder()\n    .withCollectionSchema(schema)\n    .withLocalPath(\".\")\n    .withChunkSize(512 * 1024 * 1024)\n    .withFileType(BulkFileType.PARQUET)\n    .build();\n\nLocalBulkWriter localBulkWriter = new LocalBulkWriter(localBulkWriterParam);\n","from pymilvus.bulk_writer import RemoteBulkWriter\n# Use `from pymilvus import RemoteBulkWriter` \n# when you use pymilvus earlier than 2.4.2 \n\n# Third-party constants\nACCESS_KEY=\"minioadmin\"\nSECRET_KEY=\"minioadmin\"\nBUCKET_NAME=\"a-bucket\"\n\n# Connections parameters to access the remote bucket\nconn = RemoteBulkWriter.S3ConnectParam(\n    endpoint=\"localhost:9000\", # the default MinIO service started along with Milvus\n    access_key=ACCESS_KEY,\n    secret_key=SECRET_KEY,\n    bucket_name=BUCKET_NAME,\n    secure=False\n)\n\nfrom pymilvus.bulk_writer import BulkFileType\n# Use `from pymilvus import BulkFileType` \n# when you use pymilvus earlier than 2.4.2 \n\nwriter = RemoteBulkWriter(\n    schema=schema,\n    remote_path=\"/\",\n    connect_param=conn,\n    file_type=BulkFileType.PARQUET\n)\n\nprint('bulk writer created.')\n","private static RemoteBulkWriter createRemoteBulkWriter(CreateCollectionReq.CollectionSchema collectionSchema) throws IOException {\n    StorageConnectParam connectParam = S3ConnectParam.newBuilder()\n            .withEndpoint(MINIO_ENDPOINT)\n            .withBucketName(BUCKET_NAME)\n            .withAccessKey(ACCESS_KEY)\n            .withSecretKey(SECRET_KEY)\n            .build();\n    RemoteBulkWriterParam bulkWriterParam = RemoteBulkWriterParam.newBuilder()\n            .withCollectionSchema(collectionSchema)\n            .withRemotePath(\"/\")\n            .withConnectParam(connectParam)\n            .withFileType(BulkFileType.PARQUET)\n            .build();\n    return new RemoteBulkWriter(bulkWriterParam);\n}\n","from pymilvus.bulk_writer import BulkFileType\n# Use `from pymilvus import BulkFileType` \n# when you use pymilvus earlier than 2.4.2 \n\nwriter = RemoteBulkWriter(\n    schema=schema,\n    remote_path=\"/\",\n    connect_param=conn,\n    file_type=BulkFileType.PARQUET\n)\n","import io.milvus.bulkwriter.RemoteBulkWriter;\nimport io.milvus.bulkwriter.RemoteBulkWriterParam;\n\nRemoteBulkWriterParam remoteBulkWriterParam = RemoteBulkWriterParam.newBuilder()\n    .withCollectionSchema(schema)\n    .withConnectParam(storageConnectParam)\n    .withChunkSize(512 * 1024 * 1024)\n    .withRemotePath(\"/\")\n    .withFileType(BulkFileType.PARQUET)\n    .build();\n\nRemoteBulkWriter remoteBulkWriter = new RemoteBulkWriter(remoteBulkWriterParam);\n","import random, string, json\nimport numpy as np\nimport tensorflow as tf\n\ndef generate_random_str(length=5):\n    letters = string.ascii_uppercase\n    digits = string.digits\n    \n    return ''.join(random.choices(letters + digits, k=length))\n\n# optional input for binary vector:\n# 1. list of int such as [1, 0, 1, 1, 0, 0, 1, 0]\n# 2. numpy array of uint8\ndef gen_binary_vector(to_numpy_arr):\n    raw_vector = [random.randint(0, 1) for i in range(DIM)]\n    if to_numpy_arr:\n        return np.packbits(raw_vector, axis=-1)\n    return raw_vector\n\n# optional input for float vector:\n# 1. list of float such as [0.56, 1.859, 6.55, 9.45]\n# 2. numpy array of float32\ndef gen_float_vector(to_numpy_arr):\n    raw_vector = [random.random() for _ in range(DIM)]\n    if to_numpy_arr:\n        return np.array(raw_vector, dtype=\"float32\")\n    return raw_vector\n\n# # optional input for bfloat16 vector:\n# # 1. list of float such as [0.56, 1.859, 6.55, 9.45]\n# # 2. numpy array of bfloat16\n# def gen_bf16_vector(to_numpy_arr):\n#     raw_vector = [random.random() for _ in range(DIM)]\n#     if to_numpy_arr:\n#         return tf.cast(raw_vector, dtype=tf.bfloat16).numpy()\n#     return raw_vector\n\n# optional input for float16 vector:\n# 1. list of float such as [0.56, 1.859, 6.55, 9.45]\n# 2. numpy array of float16\ndef gen_fp16_vector(to_numpy_arr):\n    raw_vector = [random.random() for _ in range(DIM)]\n    if to_numpy_arr:\n        return np.array(raw_vector, dtype=np.float16)\n    return raw_vector\n\n# optional input for sparse vector:\n# only accepts dict like {2: 13.23, 45: 0.54} or {\"indices\": [1, 2], \"values\": [0.1, 0.2]}\n# note: no need to sort the keys\ndef gen_sparse_vector(pair_dict: bool):\n    raw_vector = {}\n    dim = random.randint(2, 20)\n    if pair_dict:\n        raw_vector[\"indices\"] = [i for i in range(dim)]\n        raw_vector[\"values\"] = [random.random() for _ in range(dim)]\n    else:\n        for i in range(dim):\n            raw_vector[i] = random.random()\n    return raw_vector\n\nfor i in range(10000):\n    writer.append_row({\n        \"id\": np.int64(i),\n        \"bool\": True if i % 3 == 0 else False,\n        \"int8\": np.int8(i%128),\n        \"int16\": np.int16(i%1000),\n        \"int32\": np.int32(i%100000),\n        \"int64\": np.int64(i),\n        \"float\": np.float32(i/3),\n        \"double\": np.float64(i/7),\n        \"varchar\": f\"varchar_{i}\",\n        \"json\": json.dumps({\"dummy\": i, \"ok\": f\"name_{i}\"}),\n        \"array_str\": np.array([f\"str_{k}\" for k in range(5)], np.dtype(\"str\")),\n        \"array_int\": np.array([k for k in range(10)], np.dtype(\"int64\")),\n        \"float_vector\": gen_float_vector(True),\n        \"binary_vector\": gen_binary_vector(True),\n        \"float16_vector\": gen_fp16_vector(True),\n        # \"bfloat16_vector\": gen_bf16_vector(True),\n        \"sparse_vector\": gen_sparse_vector(True),\n        f\"dynamic_{i}\": i,\n    })\n    if (i+1)%1000 == 0:\n        writer.commit()\n        print('committed')\n\nprint(writer.batch_files)\n","private static byte[] genBinaryVector() {\n    Random ran = new Random();\n    int byteCount = DIM / 8;\n    ByteBuffer vector = ByteBuffer.allocate(byteCount);\n    for (int i = 0; i < byteCount; ++i) {\n        vector.put((byte) ran.nextInt(Byte.MAX_VALUE));\n    }\n    return vector.array();\n}\n\nprivate static List<Float> genFloatVector() {\n    Random ran = new Random();\n    List<Float> vector = new ArrayList<>();\n    for (int i = 0; i < DIM; ++i) {\n        vector.add(ran.nextFloat());\n    }\n    return vector;\n}\n\nprivate static byte[] genFloat16Vector() {\n    List<Float> originalVector = genFloatVector();\n    return Float16Utils.f32VectorToFp16Buffer(originalVector).array();\n}\n\nprivate static SortedMap<Long, Float> genSparseVector() {\n    Random ran = new Random();\n    SortedMap<Long, Float> sparse = new TreeMap<>();\n    int dim = ran.nextInt(18) + 2; // [2, 20)\n    for (int i = 0; i < dim; ++i) {\n        sparse.put((long)ran.nextInt(1000000), ran.nextFloat());\n    }\n    return sparse;\n}\n\nprivate static List<String> genStringArray(int length) {\n    List<String> arr = new ArrayList<>();\n    for (int i = 0; i < length; i++) {\n        arr.add(\"str_\" + i);\n    }\n    return arr;\n}\n\nprivate static List<Long> genIntArray(int length) {\n    List<Long> arr = new ArrayList<>();\n    for (long i = 0; i < length; i++) {\n        arr.add(i);\n    }\n    return arr;\n}\n\nprivate static RemoteBulkWriter createRemoteBulkWriter(CreateCollectionReq.CollectionSchema collectionSchema) throws IOException {\n    StorageConnectParam connectParam = S3ConnectParam.newBuilder()\n            .withEndpoint(MINIO_ENDPOINT)\n            .withBucketName(BUCKET_NAME)\n            .withAccessKey(ACCESS_KEY)\n            .withSecretKey(SECRET_KEY)\n            .build();\n    RemoteBulkWriterParam bulkWriterParam = RemoteBulkWriterParam.newBuilder()\n            .withCollectionSchema(collectionSchema)\n            .withRemotePath(\"/\")\n            .withConnectParam(connectParam)\n            .withFileType(BulkFileType.PARQUET)\n            .build();\n    return new RemoteBulkWriter(bulkWriterParam);\n}\n\nprivate static List<List<String>> uploadData() throws Exception {\n    CreateCollectionReq.CollectionSchema collectionSchema = createSchema();\n    try (RemoteBulkWriter remoteBulkWriter = createRemoteBulkWriter(collectionSchema)) {\n        for (int i = 0; i < 10000; ++i) {\n            JsonObject rowObject = new JsonObject();\n\n            rowObject.addProperty(\"id\", i);\n            rowObject.addProperty(\"bool\", i % 3 == 0);\n            rowObject.addProperty(\"int8\", i % 128);\n            rowObject.addProperty(\"int16\", i % 1000);\n            rowObject.addProperty(\"int32\", i % 100000);\n            rowObject.addProperty(\"int64\", i);\n            rowObject.addProperty(\"float\", i / 3);\n            rowObject.addProperty(\"double\", i / 7);\n            rowObject.addProperty(\"varchar\", \"varchar_\" + i);\n            rowObject.addProperty(\"json\", String.format(\"{\\\"dummy\\\": %s, \\\"ok\\\": \\\"name_%s\\\"}\", i, i));\n            rowObject.add(\"array_str\", GSON_INSTANCE.toJsonTree(genStringArray(5)));\n            rowObject.add(\"array_int\", GSON_INSTANCE.toJsonTree(genIntArray(10)));\n            rowObject.add(\"float_vector\", GSON_INSTANCE.toJsonTree(genFloatVector()));\n            rowObject.add(\"binary_vector\", GSON_INSTANCE.toJsonTree(genBinaryVector()));\n            rowObject.add(\"float16_vector\", GSON_INSTANCE.toJsonTree(genFloat16Vector()));\n            rowObject.add(\"sparse_vector\", GSON_INSTANCE.toJsonTree(genSparseVector()));\n            rowObject.addProperty(\"dynamic\", \"dynamic_\" + i);\n\n            remoteBulkWriter.appendRow(rowObject);\n\n            if ((i+1)%1000 == 0) {\n                remoteBulkWriter.commit(false);\n            }\n        }\n\n        List<List<String>> batchFiles = remoteBulkWriter.getBatchFiles();\n        System.out.println(batchFiles);\n        return batchFiles;\n    } catch (Exception e) {\n        throw e;\n    }\n}\n\npublic static void main(String[] args) throws Exception {\n    List<List<String>> batchFiles = uploadData();\n}\n","print(writer.batch_files)\n\n# [['d4220a9e-45be-4ccb-8cb5-bf09304b9f23/1.parquet'],\n#  ['d4220a9e-45be-4ccb-8cb5-bf09304b9f23/2.parquet']]\n","// localBulkWriter.getBatchFiles();\nremoteBulkWriter.getBatchFiles();\n\n// \n\n// Close the BulkWriter\ntry {\n    localBulkWriter.close();\n    remoteBulkWriter.close();            \n} catch (Exception e) {\n    // TODO: handle exception\n    e.printStackTrace();\n}\n","# JSON\n├── folder\n│   └── 45ae1139-1d87-4aff-85f5-0039111f9e6b\n│       └── 1.json \n\n# Parquet\n├── folder\n│   └── 45ae1139-1d87-4aff-85f5-0039111f9e6b\n│       └── 1.parquet \n"],"headingContent":"Prepare Source Data","anchorList":[{"label":"Подготовка исходных данных","href":"Prepare-Source-Data","type":1,"isActive":false},{"label":"Перед началом работы","href":"Before-you-start","type":2,"isActive":false},{"label":"Настройка BulkWriter","href":"Set-up-BulkWriter","type":2,"isActive":false},{"label":"Начало записи","href":"Start-writing","type":2,"isActive":false},{"label":"Проверка результатов","href":"Verify-the-results","type":2,"isActive":false}]}