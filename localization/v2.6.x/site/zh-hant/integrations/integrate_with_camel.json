{"codeList":["$ pip install -U \"camel-ai[all]\" pymilvus\n","import os\nimport requests\n\nos.makedirs(\"local_data\", exist_ok=True)\n\nurl = \"https://arxiv.org/pdf/2303.17760.pdf\"\nresponse = requests.get(url)\nwith open(\"local_data/camel paper.pdf\", \"wb\") as file:\n    file.write(response.content)\n","os.environ[\"OPENAI_API_KEY\"] = \"Your Key\"\n","from camel.embeddings import OpenAIEmbedding\n\nembedding_instance = OpenAIEmbedding()\n","from camel.storages import MilvusStorage\n\nstorage_instance = MilvusStorage(\n    vector_dim=embedding_instance.get_output_dim(),\n    url_and_api_key=(\n        \"./milvus_demo.db\",  # Your Milvus connection URI\n        \"\",  # Your Milvus token\n    ),\n    collection_name=\"camel_paper\",\n)\n","from camel.retrievers import VectorRetriever\n\nvector_retriever = VectorRetriever(\n    embedding_model=embedding_instance, storage=storage_instance\n)\n","vector_retriever.process(content_input_path=\"local_data/camel paper.pdf\")\n","retrieved_info = vector_retriever.query(query=\"What is CAMEL?\", top_k=1)\nprint(retrieved_info)\n","retrieved_info_irrelevant = vector_retriever.query(\n    query=\"Compared with dumpling and rice, which should I take for dinner?\", top_k=1\n)\n\nprint(retrieved_info_irrelevant)\n","from camel.retrievers import AutoRetriever\nfrom camel.types import StorageType\n\nauto_retriever = AutoRetriever(\n    url_and_api_key=(\n        \"./milvus_demo.db\",  # Your Milvus connection URI\n        \"\",  # Your Milvus token\n    ),\n    storage_type=StorageType.MILVUS,\n    embedding_model=embedding_instance,\n)\n\nretrieved_info = auto_retriever.run_vector_retriever(\n    query=\"What is CAMEL-AI\",\n    content_input_paths=[\n        \"local_data/camel paper.pdf\",  # example local path\n        \"https://www.camel-ai.org/\",  # example remote url\n    ],\n    top_k=1,\n    return_detailed_info=True,\n)\n\nprint(retrieved_info)\n","from camel.agents import ChatAgent\nfrom camel.messages import BaseMessage\nfrom camel.types import RoleType\nfrom camel.retrievers import AutoRetriever\nfrom camel.types import StorageType\n\n\ndef single_agent(query: str) -> str:\n    # Set agent role\n    assistant_sys_msg = BaseMessage(\n        role_name=\"Assistant\",\n        role_type=RoleType.ASSISTANT,\n        meta_dict=None,\n        content=\"\"\"You are a helpful assistant to answer question,\n         I will give you the Original Query and Retrieved Context,\n        answer the Original Query based on the Retrieved Context,\n        if you can't answer the question just say I don't know.\"\"\",\n    )\n\n    # Add auto retriever\n    auto_retriever = AutoRetriever(\n        url_and_api_key=(\n            \"./milvus_demo.db\",  # Your Milvus connection URI\n            \"\",  # Your Milvus token\n        ),\n        storage_type=StorageType.MILVUS,\n        embedding_model=embedding_instance,\n    )\n\n    retrieved_info = auto_retriever.run_vector_retriever(\n        query=query,\n        content_input_paths=[\n            \"local_data/camel paper.pdf\",  # example local path\n            \"https://www.camel-ai.org/\",  # example remote url\n        ],\n        # vector_storage_local_path=\"storage_default_run\",\n        top_k=1,\n        return_detailed_info=True,\n    )\n\n    # Pass the retrieved infomation to agent\n    user_msg = BaseMessage.make_user_message(role_name=\"User\", content=retrieved_info)\n    agent = ChatAgent(assistant_sys_msg)\n\n    # Get response\n    assistant_response = agent.step(user_msg)\n    return assistant_response.msg.content\n\n\nprint(single_agent(\"What is CAMEL-AI\"))\n","from typing import List\nfrom colorama import Fore\n\nfrom camel.agents.chat_agent import FunctionCallingRecord\nfrom camel.configs import ChatGPTConfig\nfrom camel.functions import (\n    MATH_FUNCS,\n    RETRIEVAL_FUNCS,\n)\nfrom camel.societies import RolePlaying\nfrom camel.types import ModelType\nfrom camel.utils import print_text_animated\n\n\ndef role_playing_with_rag(\n    task_prompt, model_type=ModelType.GPT_4O, chat_turn_limit=10\n) -> None:\n    task_prompt = task_prompt\n\n    user_model_config = ChatGPTConfig(temperature=0.0)\n\n    function_list = [\n        *MATH_FUNCS,\n        *RETRIEVAL_FUNCS,\n    ]\n    assistant_model_config = ChatGPTConfig(\n        tools=function_list,\n        temperature=0.0,\n    )\n\n    role_play_session = RolePlaying(\n        assistant_role_name=\"Searcher\",\n        user_role_name=\"Professor\",\n        assistant_agent_kwargs=dict(\n            model_type=model_type,\n            model_config=assistant_model_config,\n            tools=function_list,\n        ),\n        user_agent_kwargs=dict(\n            model_type=model_type,\n            model_config=user_model_config,\n        ),\n        task_prompt=task_prompt,\n        with_task_specify=False,\n    )\n\n    print(\n        Fore.GREEN\n        + f\"AI Assistant sys message:\\n{role_play_session.assistant_sys_msg}\\n\"\n    )\n    print(Fore.BLUE + f\"AI User sys message:\\n{role_play_session.user_sys_msg}\\n\")\n\n    print(Fore.YELLOW + f\"Original task prompt:\\n{task_prompt}\\n\")\n    print(\n        Fore.CYAN\n        + f\"Specified task prompt:\\n{role_play_session.specified_task_prompt}\\n\"\n    )\n    print(Fore.RED + f\"Final task prompt:\\n{role_play_session.task_prompt}\\n\")\n\n    n = 0\n    input_msg = role_play_session.init_chat()\n    while n < chat_turn_limit:\n        n += 1\n        assistant_response, user_response = role_play_session.step(input_msg)\n\n        if assistant_response.terminated:\n            print(\n                Fore.GREEN\n                + (\n                    \"AI Assistant terminated. Reason: \"\n                    f\"{assistant_response.info['termination_reasons']}.\"\n                )\n            )\n            break\n        if user_response.terminated:\n            print(\n                Fore.GREEN\n                + (\n                    \"AI User terminated. \"\n                    f\"Reason: {user_response.info['termination_reasons']}.\"\n                )\n            )\n            break\n\n        # Print output from the user\n        print_text_animated(Fore.BLUE + f\"AI User:\\n\\n{user_response.msg.content}\\n\")\n\n        # Print output from the assistant, including any function\n        # execution information\n        print_text_animated(Fore.GREEN + \"AI Assistant:\")\n        tool_calls: List[FunctionCallingRecord] = assistant_response.info[\"tool_calls\"]\n        for func_record in tool_calls:\n            print_text_animated(f\"{func_record}\")\n        print_text_animated(f\"{assistant_response.msg.content}\\n\")\n\n        if \"CAMEL_TASK_DONE\" in user_response.msg.content:\n            break\n\n        input_msg = assistant_response.msg\n","role_playing_with_rag(\n    task_prompt=\"\"\"What is the main termination reasons for AI Society\n                   dataset, how many number of messages did camel decided to\n                   limit, what's the value plus 100? You should refer to the\n                   content in path camel/local_data/camel paper.pdf\"\"\"\n)\n"],"headingContent":"Retrieval-Augmented Generation (RAG) with Milvus and Camel","anchorList":[{"label":"Retrieval-Augmented Generation (RAG) with Milvus and Camel","href":"Retrieval-Augmented-Generation-RAG-with-Milvus-and-Camel","type":1,"isActive":false},{"label":"Load Data","href":"Load-Data","type":2,"isActive":false},{"label":"1. Customized RAG","href":"1-Customized-RAG","type":2,"isActive":false},{"label":"2. Auto RAG","href":"2-Auto-RAG","type":2,"isActive":false},{"label":"3. Single Agent with Auto RAG","href":"3-Single-Agent-with-Auto-RAG","type":2,"isActive":false},{"label":"4. Role-playing with Auto RAG","href":"4-Role-playing-with-Auto-RAG","type":2,"isActive":false}]}