{"codeList":["$ pip install --upgrade pymilvus openai requests tqdm pandas ragas\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","from typing import List\nfrom tqdm import tqdm\nfrom openai import OpenAI\nfrom pymilvus import MilvusClient\n\n\nclass RAG:\n    \"\"\"\n    RAG (Retrieval-Augmented Generation) class built upon OpenAI and Milvus.\n    \"\"\"\n\n    def __init__(self, openai_client: OpenAI, milvus_client: MilvusClient):\n        self._prepare_openai(openai_client)\n        self._prepare_milvus(milvus_client)\n\n    def _emb_text(self, text: str) -> List[float]:\n        return (\n            self.openai_client.embeddings.create(input=text, model=self.embedding_model)\n            .data[0]\n            .embedding\n        )\n\n    def _prepare_openai(\n        self,\n        openai_client: OpenAI,\n        embedding_model: str = \"text-embedding-3-small\",\n        llm_model: str = \"gpt-3.5-turbo\",\n    ):\n        self.openai_client = openai_client\n        self.embedding_model = embedding_model\n        self.llm_model = llm_model\n        self.SYSTEM_PROMPT = \"\"\"\nHuman: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n\"\"\"\n        self.USER_PROMPT = \"\"\"\nUse the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n<context>\n{context}\n</context>\n<question>\n{question}\n</question>\n\"\"\"\n\n    def _prepare_milvus(\n        self, milvus_client: MilvusClient, collection_name: str = \"rag_collection\"\n    ):\n        self.milvus_client = milvus_client\n        self.collection_name = collection_name\n        if self.milvus_client.has_collection(self.collection_name):\n            self.milvus_client.drop_collection(self.collection_name)\n        embedding_dim = len(self._emb_text(\"foo\"))\n        self.milvus_client.create_collection(\n            collection_name=self.collection_name,\n            dimension=embedding_dim,\n            metric_type=\"IP\",  # Inner product distance\n            consistency_level=\"Strong\",  # Strong consistency level\n        )\n\n    def load(self, texts: List[str]):\n        \"\"\"\n        Load the text data into Milvus.\n        \"\"\"\n        data = []\n        for i, line in enumerate(tqdm(texts, desc=\"Creating embeddings\")):\n            data.append({\"id\": i, \"vector\": self._emb_text(line), \"text\": line})\n\n        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n\n    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n        \"\"\"\n        Retrieve the most similar text data to the given question.\n        \"\"\"\n        search_res = self.milvus_client.search(\n            collection_name=self.collection_name,\n            data=[self._emb_text(question)],\n            limit=top_k,\n            search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n            output_fields=[\"text\"],  # Return the text field\n        )\n        retrieved_texts = [res[\"entity\"][\"text\"] for res in search_res[0]]\n        return retrieved_texts[:top_k]\n\n    def answer(\n        self,\n        question: str,\n        retrieval_top_k: int = 3,\n        return_retrieved_text: bool = False,\n    ):\n        \"\"\"\n        Answer the given question with the retrieved knowledge.\n        \"\"\"\n        retrieved_texts = self.retrieve(question, top_k=retrieval_top_k)\n        user_prompt = self.USER_PROMPT.format(\n            context=\"\\n\".join(retrieved_texts), question=question\n        )\n        response = self.openai_client.chat.completions.create(\n            model=self.llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        )\n        if not return_retrieved_text:\n            return response.choices[0].message.content\n        else:\n            return response.choices[0].message.content, retrieved_texts\n","openai_client = OpenAI()\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\nmy_rag = RAG(openai_client=openai_client, milvus_client=milvus_client)\n","import os\nimport urllib.request\n\nurl = \"https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md\"\nfile_path = \"./Milvus_DEVELOPMENT.md\"\n\nif not os.path.exists(file_path):\n    urllib.request.urlretrieve(url, file_path)\nwith open(file_path, \"r\") as file:\n    file_text = file.read()\n\n# We simply use \"# \" to separate the content in the file, which can roughly separate the content of each main part of the markdown file.\ntext_lines = file_text.split(\"# \")\nmy_rag.load(text_lines)  # Load the text data into RAG pipeline\n","question = \"what is the hardware requirements specification if I want to build Milvus and run from source code?\"\nmy_rag.answer(question, return_retrieved_text=True)\n","from ragas import EvaluationDataset\nfrom datasets import Dataset\nimport pandas as pd\n\nuser_input_list = [\n    \"what is the hardware requirements specification if I want to build Milvus and run from source code?\",\n    \"What is the programming language used to write Knowhere?\",\n    \"What should be ensured before running code coverage?\",\n]\nreference_list = [\n    \"If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.\",\n    \"The programming language used to write Knowhere is C++.\",\n    \"Before running code coverage, you should make sure that your code changes are covered by unit tests.\",\n]\nretrieved_contexts_list = []\nresponse_list = []\n\nfor user_input in tqdm(user_input_list, desc=\"Answering questions\"):\n    response, retrieved_context = my_rag.answer(user_input, return_retrieved_text=True)\n    retrieved_contexts_list.append(retrieved_context)\n    response_list.append(response)\n\ndf = pd.DataFrame(\n    {\n        \"user_input\": user_input_list,\n        \"retrieved_contexts\": retrieved_contexts_list,\n        \"response\": response_list,\n        \"reference\": reference_list,\n    }\n)\nrag_results = EvaluationDataset.from_pandas(df)\ndf\n","from ragas import evaluate\nfrom ragas.metrics import AnswerRelevancy, Faithfulness, ContextRecall, ContextPrecision\n\nfrom ragas.llms import LangchainLLMWrapper\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nevaluator_llm = LangchainLLMWrapper(llm)\n\nresults = evaluate(\n    dataset=rag_results,\n    metrics=[\n        AnswerRelevancy(llm=evaluator_llm),\n        Faithfulness(llm=evaluator_llm),\n        ContextRecall(llm=evaluator_llm),\n        ContextPrecision(llm=evaluator_llm),\n    ],\n)\nresults\n"],"headingContent":"Evaluation with Ragas","anchorList":[{"label":"使用 Ragas 进行评估","href":"Evaluation-with-Ragas","type":1,"isActive":false},{"label":"前提条件","href":"Prerequisites","type":2,"isActive":false},{"label":"定义 RAG 管道","href":"Define-the-RAG-pipeline","type":2,"isActive":false},{"label":"运行 RAG 管道并获取结果","href":"Run-the-RAG-pipeline-and-get-results","type":2,"isActive":false},{"label":"使用 Ragas 进行评估","href":"Evaluation-with-Ragas","type":2,"isActive":false}]}