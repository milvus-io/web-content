{"codeList":["$ pip install \"pymilvus[model]\"\n$ pip install tqdm\n$ pip install anthropic\n","$ wget https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/codebase_chunks.json\n$ wget https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/evaluation_set.jsonl\n","from pymilvus.model.dense import VoyageEmbeddingFunction\nfrom pymilvus.model.hybrid import BGEM3EmbeddingFunction\nfrom pymilvus.model.reranker import CohereRerankFunction\n\nfrom typing import List, Dict, Any\nfrom typing import Callable\nfrom pymilvus import (\n    MilvusClient,\n    DataType,\n    AnnSearchRequest,\n    RRFRanker,\n)\nfrom tqdm import tqdm\nimport json\nimport anthropic\n\n\nclass MilvusContextualRetriever:\n    def __init__(\n        self,\n        uri=\"milvus.db\",\n        collection_name=\"contexual_bgem3\",\n        dense_embedding_function=None,\n        use_sparse=False,\n        sparse_embedding_function=None,\n        use_contextualize_embedding=False,\n        anthropic_client=None,\n        use_reranker=False,\n        rerank_function=None,\n    ):\n        self.collection_name = collection_name\n\n        # For Milvus-lite, uri is a local path like \"./milvus.db\"\n        # For Milvus standalone service, uri is like \"http://localhost:19530\"\n        # For Zilliz Clond, please set `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.\n        self.client = MilvusClient(uri)\n\n        self.embedding_function = dense_embedding_function\n\n        self.use_sparse = use_sparse\n        self.sparse_embedding_function = None\n\n        self.use_contextualize_embedding = use_contextualize_embedding\n        self.anthropic_client = anthropic_client\n\n        self.use_reranker = use_reranker\n        self.rerank_function = rerank_function\n\n        if use_sparse is True and sparse_embedding_function:\n            self.sparse_embedding_function = sparse_embedding_function\n        elif sparse_embedding_function is False:\n            raise ValueError(\n                \"Sparse embedding function cannot be None if use_sparse is False\"\n            )\n        else:\n            pass\n\n    def build_collection(self):\n        schema = self.client.create_schema(\n            auto_id=True,\n            enable_dynamic_field=True,\n        )\n        schema.add_field(field_name=\"pk\", datatype=DataType.INT64, is_primary=True)\n        schema.add_field(\n            field_name=\"dense_vector\",\n            datatype=DataType.FLOAT_VECTOR,\n            dim=self.embedding_function.dim,\n        )\n        if self.use_sparse is True:\n            schema.add_field(\n                field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR\n            )\n\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"dense_vector\", index_type=\"FLAT\", metric_type=\"IP\"\n        )\n        if self.use_sparse is True:\n            index_params.add_index(\n                field_name=\"sparse_vector\",\n                index_type=\"SPARSE_INVERTED_INDEX\",\n                metric_type=\"IP\",\n            )\n\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            schema=schema,\n            index_params=index_params,\n            enable_dynamic_field=True,\n        )\n\n    def insert_data(self, chunk, metadata):\n        dense_vec = self.embedding_function([chunk])[0]\n        if self.use_sparse is True:\n            sparse_result = self.sparse_embedding_function.encode_documents([chunk])\n            if type(sparse_result) == dict:\n                sparse_vec = sparse_result[\"sparse\"][[0]]\n            else:\n                sparse_vec = sparse_result[[0]]\n            self.client.insert(\n                collection_name=self.collection_name,\n                data={\n                    \"dense_vector\": dense_vec,\n                    \"sparse_vector\": sparse_vec,\n                    **metadata,\n                },\n            )\n        else:\n            self.client.insert(\n                collection_name=self.collection_name,\n                data={\"dense_vector\": dense_vec, **metadata},\n            )\n\n    def insert_contextualized_data(self, doc, chunk, metadata):\n        contextualized_text, usage = self.situate_context(doc, chunk)\n        metadata[\"context\"] = contextualized_text\n        text_to_embed = f\"{chunk}\\n\\n{contextualized_text}\"\n        dense_vec = self.embedding_function([text_to_embed])[0]\n        if self.use_sparse is True:\n            sparse_vec = self.sparse_embedding_function.encode_documents(\n                [text_to_embed]\n            )[\"sparse\"][[0]]\n            self.client.insert(\n                collection_name=self.collection_name,\n                data={\n                    \"dense_vector\": dense_vec,\n                    \"sparse_vector\": sparse_vec,\n                    **metadata,\n                },\n            )\n        else:\n            self.client.insert(\n                collection_name=self.collection_name,\n                data={\"dense_vector\": dense_vec, **metadata},\n            )\n\n    def situate_context(self, doc: str, chunk: str):\n        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n        <document>\n        {doc_content}\n        </document>\n        \"\"\"\n\n        CHUNK_CONTEXT_PROMPT = \"\"\"\n        Here is the chunk we want to situate within the whole document\n        <chunk>\n        {chunk_content}\n        </chunk>\n\n        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n        Answer only with the succinct context and nothing else.\n        \"\"\"\n\n        response = self.anthropic_client.beta.prompt_caching.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=1000,\n            temperature=0.0,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n                            \"cache_control\": {\n                                \"type\": \"ephemeral\"\n                            },  # we will make use of prompt caching for the full documents\n                        },\n                        {\n                            \"type\": \"text\",\n                            \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n                        },\n                    ],\n                },\n            ],\n            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n        )\n        return response.content[0].text, response.usage\n\n    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n        dense_vec = self.embedding_function([query])[0]\n        if self.use_sparse is True:\n            sparse_vec = self.sparse_embedding_function.encode_queries([query])[\n                \"sparse\"\n            ][[0]]\n\n        req_list = []\n        if self.use_reranker:\n            k = k * 10\n        if self.use_sparse is True:\n            req_list = []\n            dense_search_param = {\n                \"data\": [dense_vec],\n                \"anns_field\": \"dense_vector\",\n                \"param\": {\"metric_type\": \"IP\"},\n                \"limit\": k * 2,\n            }\n            dense_req = AnnSearchRequest(**dense_search_param)\n            req_list.append(dense_req)\n\n            sparse_search_param = {\n                \"data\": [sparse_vec],\n                \"anns_field\": \"sparse_vector\",\n                \"param\": {\"metric_type\": \"IP\"},\n                \"limit\": k * 2,\n            }\n            sparse_req = AnnSearchRequest(**sparse_search_param)\n\n            req_list.append(sparse_req)\n\n            docs = self.client.hybrid_search(\n                self.collection_name,\n                req_list,\n                RRFRanker(),\n                k,\n                output_fields=[\n                    \"content\",\n                    \"original_uuid\",\n                    \"doc_id\",\n                    \"chunk_id\",\n                    \"original_index\",\n                    \"context\",\n                ],\n            )\n        else:\n            docs = self.client.search(\n                self.collection_name,\n                data=[dense_vec],\n                anns_field=\"dense_vector\",\n                limit=k,\n                output_fields=[\n                    \"content\",\n                    \"original_uuid\",\n                    \"doc_id\",\n                    \"chunk_id\",\n                    \"original_index\",\n                    \"context\",\n                ],\n            )\n        if self.use_reranker and self.use_contextualize_embedding:\n            reranked_texts = []\n            reranked_docs = []\n            for i in range(k):\n                if self.use_contextualize_embedding:\n                    reranked_texts.append(\n                        f\"{docs[0][i]['entity']['content']}\\n\\n{docs[0][i]['entity']['context']}\"\n                    )\n                else:\n                    reranked_texts.append(f\"{docs[0][i]['entity']['content']}\")\n            results = self.rerank_function(query, reranked_texts)\n            for result in results:\n                reranked_docs.append(docs[0][result.index])\n            docs[0] = reranked_docs\n        return docs\n\n\ndef evaluate_retrieval(\n    queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20\n) -> Dict[str, float]:\n    total_score = 0\n    total_queries = len(queries)\n    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n        query = query_item[\"query\"]\n        golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n\n        # Find all golden chunk contents\n        golden_contents = []\n        for doc_uuid, chunk_index in golden_chunk_uuids:\n            golden_doc = next(\n                (\n                    doc\n                    for doc in query_item[\"golden_documents\"]\n                    if doc[\"uuid\"] == doc_uuid\n                ),\n                None,\n            )\n            if not golden_doc:\n                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n                continue\n\n            golden_chunk = next(\n                (\n                    chunk\n                    for chunk in golden_doc[\"chunks\"]\n                    if chunk[\"index\"] == chunk_index\n                ),\n                None,\n            )\n            if not golden_chunk:\n                print(\n                    f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\"\n                )\n                continue\n\n            golden_contents.append(golden_chunk[\"content\"].strip())\n\n        if not golden_contents:\n            print(f\"Warning: No golden contents found for query: {query}\")\n            continue\n\n        retrieved_docs = retrieval_function(query, db, k=k)\n\n        # Count how many golden chunks are in the top k retrieved documents\n        chunks_found = 0\n        for golden_content in golden_contents:\n            for doc in retrieved_docs[0][:k]:\n                retrieved_content = doc[\"entity\"][\"content\"].strip()\n                if retrieved_content == golden_content:\n                    chunks_found += 1\n                    break\n\n        query_score = chunks_found / len(golden_contents)\n        total_score += query_score\n\n    average_score = total_score / total_queries\n    pass_at_n = average_score * 100\n    return {\n        \"pass_at_n\": pass_at_n,\n        \"average_score\": average_score,\n        \"total_queries\": total_queries,\n    }\n\n\ndef retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n    return db.search(query, k=k)\n\n\ndef load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n    with open(file_path, \"r\") as file:\n        return [json.loads(line) for line in file]\n\n\ndef evaluate_db(db, original_jsonl_path: str, k):\n    # Load the original JSONL data for queries and ground truth\n    original_data = load_jsonl(original_jsonl_path)\n\n    # Evaluate retrieval\n    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n    print(f\"Total Score: {results['average_score']}\")\n    print(f\"Total queries: {results['total_queries']}\")\n","dense_ef = VoyageEmbeddingFunction(api_key=\"your-voyage-api-key\", model_name=\"voyage-2\")\nsparse_ef = BGEM3EmbeddingFunction()\ncohere_rf = CohereRerankFunction(api_key=\"your-cohere-api-key\")\n","path = \"codebase_chunks.json\"\nwith open(path, \"r\") as f:\n    dataset = json.load(f)\n","standard_retriever = MilvusContextualRetriever(\n    uri=\"standard.db\", collection_name=\"standard\", dense_embedding_function=dense_ef\n)\n\nstandard_retriever.build_collection()\nfor doc in dataset:\n    doc_content = doc[\"content\"]\n    for chunk in doc[\"chunks\"]:\n        metadata = {\n            \"doc_id\": doc[\"doc_id\"],\n            \"original_uuid\": doc[\"original_uuid\"],\n            \"chunk_id\": chunk[\"chunk_id\"],\n            \"original_index\": chunk[\"original_index\"],\n            \"content\": chunk[\"content\"],\n        }\n        chunk_content = chunk[\"content\"]\n        standard_retriever.insert_data(chunk_content, metadata)\n","evaluate_db(standard_retriever, \"evaluation_set.jsonl\", 5)\n","hybrid_retriever = MilvusContextualRetriever(\n    uri=\"hybrid.db\",\n    collection_name=\"hybrid\",\n    dense_embedding_function=dense_ef,\n    use_sparse=True,\n    sparse_embedding_function=sparse_ef,\n)\n\nhybrid_retriever.build_collection()\nfor doc in dataset:\n    doc_content = doc[\"content\"]\n    for chunk in doc[\"chunks\"]:\n        metadata = {\n            \"doc_id\": doc[\"doc_id\"],\n            \"original_uuid\": doc[\"original_uuid\"],\n            \"chunk_id\": chunk[\"chunk_id\"],\n            \"original_index\": chunk[\"original_index\"],\n            \"content\": chunk[\"content\"],\n        }\n        chunk_content = chunk[\"content\"]\n        hybrid_retriever.insert_data(chunk_content, metadata)\n","evaluate_db(hybrid_retriever, \"evaluation_set.jsonl\", 5)\n","anthropic_client = anthropic.Anthropic(\n    api_key=\"your-anthropic-api-key\",\n)\n","contextual_retriever = MilvusContextualRetriever(\n    uri=\"contextual.db\",\n    collection_name=\"contextual\",\n    dense_embedding_function=dense_ef,\n    use_sparse=True,\n    sparse_embedding_function=sparse_ef,\n    use_contextualize_embedding=True,\n    anthropic_client=anthropic_client,\n)\n\ncontextual_retriever.build_collection()\nfor doc in dataset:\n    doc_content = doc[\"content\"]\n    for chunk in doc[\"chunks\"]:\n        metadata = {\n            \"doc_id\": doc[\"doc_id\"],\n            \"original_uuid\": doc[\"original_uuid\"],\n            \"chunk_id\": chunk[\"chunk_id\"],\n            \"original_index\": chunk[\"original_index\"],\n            \"content\": chunk[\"content\"],\n        }\n        chunk_content = chunk[\"content\"]\n        contextual_retriever.insert_contextualized_data(\n            doc_content, chunk_content, metadata\n        )\n","evaluate_db(contextual_retriever, \"evaluation_set.jsonl\", 5)\n","contextual_retriever.use_reranker = True\ncontextual_retriever.rerank_function = cohere_rf\n","evaluate_db(contextual_retriever, \"evaluation_set.jsonl\", 5)\n"],"headingContent":"Contextual Retrieval with Milvus","anchorList":[{"label":"Recupero contestuale con Milvus","href":"Contextual-Retrieval-with-Milvus","type":1,"isActive":false},{"label":"Preparazione","href":"Preparation","type":2,"isActive":false},{"label":"Scaricare i dati","href":"Download-Data","type":2,"isActive":false},{"label":"Definizione di Retriever","href":"Define-Retriever","type":2,"isActive":false},{"label":"Esperimento I: Recupero standard","href":"Experiment-I-Standard-Retrieval","type":2,"isActive":false},{"label":"Esperimento II: Recupero ibrido","href":"Experiment-II-Hybrid-Retrieval","type":2,"isActive":false},{"label":"Esperimento III: recupero contestuale","href":"Experiment-III-Contextual-Retrieval","type":2,"isActive":false},{"label":"Esperimento IV: Recupero contestuale con Reranker","href":"Experiment-IV-Contextual-Retrieval-with-Reranker","type":2,"isActive":false}]}