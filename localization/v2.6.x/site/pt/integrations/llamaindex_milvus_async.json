{"codeList":["$ pip install -U pymilvus llama-index-vector-stores-milvus llama-index nest-asyncio\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","import nest_asyncio\n\nnest_asyncio.apply()\n","$ mkdir -p 'data/'\n$ wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_essay.txt'\n$ wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/uber_2021.pdf'\n","import asyncio\nimport random\nimport time\n\nfrom llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\nfrom llama_index.core.vector_stores import VectorStoreQuery\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\n\nURI = \"http://localhost:19530\"\nDIM = 768\n","def init_vector_store():\n    return MilvusVectorStore(\n        uri=URI,\n        # token=TOKEN,\n        dim=DIM,\n        collection_name=\"test_collection\",\n        embedding_field=\"embedding\",\n        id_field=\"id\",\n        similarity_metric=\"COSINE\",\n        consistency_level=\"Bounded\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n        overwrite=True,  # To overwrite the collection if it already exists\n    )\n\n\nvector_store = init_vector_store()\n","from llama_index.core import SimpleDirectoryReader\n\n# load documents\ndocuments = SimpleDirectoryReader(\n    input_files=[\"./data/paul_graham_essay.txt\"]\n).load_data()\n\nprint(\"Document ID:\", documents[0].doc_id)\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n","# Create an index over the documents\nfrom llama_index.core import VectorStoreIndex, StorageContext\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n    embed_model=embed_model,\n    use_async=True,\n)\n","from llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n","query_engine = index.as_query_engine(use_async=True, llm=llm)\nresponse = await query_engine.aquery(\"What did the author learn?\")\n","print(response)\n","vector_store = init_vector_store()\n","def random_id():\n    random_num_str = \"\"\n    for _ in range(16):\n        random_digit = str(random.randint(0, 9))\n        random_num_str += random_digit\n    return random_num_str\n\n\ndef produce_nodes(num_adding):\n    node_list = []\n    for i in range(num_adding):\n        node = TextNode(\n            id_=random_id(),\n            text=f\"n{i}_text\",\n            embedding=[0.5] * (DIM - 1) + [random.random()],\n            relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id=f\"n{i+1}\")},\n        )\n        node_list.append(node)\n    return node_list\n","async def async_add(num_adding):\n    node_list = produce_nodes(num_adding)\n    start_time = time.time()\n    tasks = []\n    for i in range(num_adding):\n        sub_nodes = node_list[i]\n        task = vector_store.async_add([sub_nodes])  # use async_add()\n        tasks.append(task)\n    results = await asyncio.gather(*tasks)\n    end_time = time.time()\n    return end_time - start_time\n","add_counts = [10, 100, 1000]\n","loop = asyncio.get_event_loop()\n","for count in add_counts:\n\n    async def measure_async_add():\n        async_time = await async_add(count)\n        print(f\"Async add for {count} took {async_time:.2f} seconds\")\n        return async_time\n\n    loop.run_until_complete(measure_async_add())\n","vector_store = init_vector_store()\n","def sync_add(num_adding):\n    node_list = produce_nodes(num_adding)\n    start_time = time.time()\n    for node in node_list:\n        result = vector_store.add([node])\n    end_time = time.time()\n    return end_time - start_time\n","for count in add_counts:\n    sync_time = sync_add(count)\n    print(f\"Sync add for {count} took {sync_time:.2f} seconds\")\n","vector_store = init_vector_store()\nnode_list = produce_nodes(num_adding=1000)\ninserted_ids = vector_store.add(node_list)\n","async def async_search(num_queries):\n    start_time = time.time()\n    tasks = []\n    for _ in range(num_queries):\n        query = VectorStoreQuery(\n            query_embedding=[0.5] * (DIM - 1) + [0.6], similarity_top_k=3\n        )\n        task = vector_store.aquery(query=query)  # use aquery()\n        tasks.append(task)\n    results = await asyncio.gather(*tasks)\n    end_time = time.time()\n    return end_time - start_time\n","query_counts = [10, 100, 1000]\n","for count in query_counts:\n\n    async def measure_async_search():\n        async_time = await async_search(count)\n        print(f\"Async search for {count} queries took {async_time:.2f} seconds\")\n        return async_time\n\n    loop.run_until_complete(measure_async_search())\n","def sync_search(num_queries):\n    start_time = time.time()\n    for _ in range(num_queries):\n        query = VectorStoreQuery(\n            query_embedding=[0.5] * (DIM - 1) + [0.6], similarity_top_k=3\n        )\n        result = vector_store.query(query=query)\n    end_time = time.time()\n    return end_time - start_time\n","for count in query_counts:\n    sync_time = sync_search(count)\n    print(f\"Sync search for {count} queries took {sync_time:.2f} seconds\")\n"],"headingContent":"RAG with Milvus and LlamaIndex Async API","anchorList":[{"label":"RAG com Milvus e API assíncrona LlamaIndex","href":"RAG-with-Milvus-and-LlamaIndex-Async-API","type":1,"isActive":false},{"label":"Antes de começar","href":"Before-you-begin","type":2,"isActive":false},{"label":"Construir RAG com processamento assíncrono","href":"Build-RAG-with-Asynchronous-Processing","type":2,"isActive":false},{"label":"Explorar a API assíncrona","href":"Explore-the-Async-API","type":2,"isActive":false}]}