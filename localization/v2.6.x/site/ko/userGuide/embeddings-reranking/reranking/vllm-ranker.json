{"codeList":["# Replace YOUR_VLLM_ENDPOINT_URL with the actual URL (e.g., http://<service-ip>:<port>/v1/rerank)\n# Replace 'BAAI/bge-reranker-base' if you deployed a different model\n\ncurl -X 'POST' \\\n  'YOUR_VLLM_ENDPOINT_URL' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"model\": \"BAAI/bge-reranker-base\",\n  \"query\": \"What is the capital of France?\",\n  \"documents\": [\n    \"The capital of Brazil is Brasilia.\",\n    \"The capital of France is Paris.\",\n    \"Horses and cows are both animals\"\n  ]\n}'\n","from pymilvus import MilvusClient, Function, FunctionType\n\n# Connect to your Milvus server\nclient = MilvusClient(\n    uri=\"http://localhost:19530\"  # Replace with your Milvus server URI\n)\n\n# Create a vLLM Ranker function\nvllm_ranker = Function(\n    name=\"vllm_semantic_ranker\",    # Choose a descriptive name\n    input_field_names=[\"document\"],  # Field containing text to rerank\n    function_type=FunctionType.RERANK,  # Must be RERANK\n    params={\n        \"reranker\": \"model\",        # Specifies model-based reranking\n        \"provider\": \"vllm\",         # Specifies vLLM service\n        \"queries\": [\"renewable energy developments\"],  # Query text\n        \"endpoint\": \"http://localhost:8080\",  # vLLM service address\n        \"max_client_batch_size\": 32,              # Optional: batch size\n        \"truncate_prompt_tokens\": 256,  # Optional: Use last 256 tokens\n    }\n)\n","// java\n","// nodejs\n","// go\n","# restful\n","# Execute search with vLLM reranking\nresults = client.search(\n    collection_name=\"your_collection\",\n    data=[\"AI Research Progress\", \"What is AI\"],  # Search queries\n    anns_field=\"dense_vector\",                   # Vector field to search\n    limit=5,                                     # Number of results to return\n    output_fields=[\"document\"],                  # Include text field for reranking\n    #  highlight-next-line\n    ranker=vllm_ranker,                         # Apply vLLM reranking\n    consistency_level=\"Bounded\"\n)\n","// java\n","// nodejs\n","// go\n","# restful\n","from pymilvus import AnnSearchRequest\n\n# Configure dense vector search\ndense_search = AnnSearchRequest(\n    data=[\"AI Research Progress\", \"What is AI\"],\n    anns_field=\"dense_vector\",\n    param={},\n    limit=5\n)\n\n# Configure sparse vector search  \nsparse_search = AnnSearchRequest(\n    data=[\"AI Research Progress\", \"What is AI\"],\n    anns_field=\"sparse_vector\", \n    param={},\n    limit=5\n)\n\n# Execute hybrid search with vLLM reranking\nhybrid_results = client.hybrid_search(\n    collection_name=\"your_collection\",\n    [dense_search, sparse_search],              # Multiple search requests\n    ranker=vllm_ranker,                        # Apply vLLM reranking to combined results\n    #  highlight-next-line\n    limit=5,                                   # Final number of results\n    output_fields=[\"document\"]\n)\n","// java\n","// nodejs\n","// go\n","# restful\n"],"headingContent":"vLLM Ranker","anchorList":[{"label":"vLLM 랭커Compatible with Milvus 2.6.x","href":"vLLM-Ranker","type":1,"isActive":false},{"label":"전제 조건","href":"Prerequisites","type":2,"isActive":false},{"label":"vLLM 랭커 함수 만들기","href":"Create-a-vLLM-ranker-function","type":2,"isActive":false},{"label":"vLLM 랭커별 파라미터","href":"vLLM-ranker-specific-parameters","type":3,"isActive":false},{"label":"표준 벡터 검색에 적용하기","href":"Apply-to-standard-vector-search","type":2,"isActive":false},{"label":"하이브리드 검색에 적용하기","href":"Apply-to-hybrid-search","type":2,"isActive":false}]}