---
id: milvus_rag_with_vllm.md
summary: >-
  이 블로그에서는 Milvus, vLLM 및 Llama 3.1로 RAG를 구축하고 실행하는 방법을 보여드립니다. 보다 구체적으로,
  Milvus에서 텍스트 정보를 벡터 임베딩으로 임베딩하고 저장하는 방법과 이 벡터 저장소를 지식창고로 사용하여 사용자 질문과 관련된 텍스트
  청크를 효율적으로 검색하는 방법을 보여드리겠습니다.
title: 'Milvus, vLLM, Llama 3.1로 RAG 구축하기'
---
<h1 id="Building-RAG-with-Milvus-vLLM-and-Llama-31" class="common-anchor-header">Milvus, vLLM, Llama 3.1로 RAG 구축하기<button data-href="#Building-RAG-with-Milvus-vLLM-and-Llama-31" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h1><p>캘리포니아 대학교 버클리는 2024년 7월 인큐베이션 단계의 프로젝트로 LLM 추론 및 서빙을 위한 빠르고 사용하기 쉬운 라이브러리인 <a href="https://docs.vllm.ai/en/latest/index.html">vLLM을</a> <a href="https://lfaidata.foundation/">LF AI &amp; Data Foundation에</a> 기부했습니다. 동료 회원 프로젝트로서 LF AI &amp; Data의 가족이 된 vLLM을 환영합니다! 🎉</p>
<p>대규모 언어 모델<a href="https://zilliz.com/glossary/large-language-models-(llms)">(LLM)</a>과 <a href="https://zilliz.com/learn/what-is-vector-database">벡터 데이터베이스는</a> 일반적으로 <a href="https://zilliz.com/glossary/ai-hallucination">AI 환각을</a> 해결하기 위해 널리 사용되는 AI 애플리케이션 아키텍처인 검색 증강 생성<a href="https://zilliz.com/learn/Retrieval-Augmented-Generation">(RAG)</a>을 구축하기 위해 짝을 이룹니다. 이 블로그에서는 Milvus, vLLM 및 Llama 3.1을 사용하여 RAG를 구축하고 실행하는 방법을 보여드립니다. 보다 구체적으로, Milvus에서 텍스트 정보를 <a href="https://zilliz.com/glossary/vector-embeddings">벡터 임베딩으로 임베딩하고</a> 저장하는 방법과 이 벡터 저장소를 지식 베이스로 사용하여 사용자 질문과 관련된 텍스트 청크를 효율적으로 검색하는 방법을 보여드리겠습니다. 마지막으로, 검색된 텍스트로 증강된 답변을 생성하는 Meta의 Llama 3.1-8B 모델을 제공하기 위해 vLLM을 활용하겠습니다. 시작해 보겠습니다!</p>
<h2 id="Introduction-to-Milvus-vLLM-and-Meta’s-Llama-31" class="common-anchor-header">Milvus, vLLM 및 Meta의 Llama 3.1 소개<button data-href="#Introduction-to-Milvus-vLLM-and-Meta’s-Llama-31" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Milvus-vector-database" class="common-anchor-header">Milvus 벡터 데이터베이스</h3><p><a href="https://zilliz.com/what-is-milvus"><strong>Milvus는</strong></a> <a href="https://zilliz.com/blog/what-is-a-real-vector-database">특별히 제작된</a> 오픈 소스 분산형 벡터 데이터베이스로, <a href="https://zilliz.com/learn/generative-ai">생성 AI</a> (GenAI) 워크로드를 위한 벡터를 저장, 색인 및 검색하기 위한 것입니다. <a href="https://zilliz.com/blog/a-review-of-hybrid-search-in-milvus">하이브리드 검색,</a> <a href="https://zilliz.com/blog/what-is-new-with-metadata-filtering-in-milvus">메타데이터 필터링</a>, 재랭킹을 수행하고 수조 개의 벡터를 효율적으로 처리할 수 있는 Milvus는 AI 및 머신 러닝 워크로드를 위한 최고의 선택입니다. <a href="https://github.com/milvus-io/">Milvus는</a> 로컬, 클러스터에서 실행하거나 완전 관리형 <a href="https://zilliz.com/cloud">Zilliz Cloud에서</a> 호스팅할 수 있습니다.</p>
<h3 id="vLLM" class="common-anchor-header">vLLM</h3><p><a href="https://vllm.readthedocs.io/en/latest/index.html"><strong>vLLM은</strong></a> UC 버클리 스카이랩에서 시작된 오픈 소스 프로젝트로, LLM 서비스 성능 최적화에 중점을 두고 있습니다. PagedAttention, 연속 배칭, 최적화된 CUDA 커널을 통한 효율적인 메모리 관리를 사용합니다. 기존 방식에 비해 vLLM은 GPU 메모리 사용량을 절반으로 줄이면서 서빙 성능을 최대 24배까지 향상시킵니다.</p>
<p><a href="https://arxiv.org/abs/2309.06180">"페이지어텐션으로 대규모 언어 모델 서비스를 위한 효율적인 메모리 관리</a>" 논문에 따르면 KV 캐시는 GPU 메모리의 약 30%를 사용하므로 잠재적인 메모리 문제를 일으킬 수 있습니다. KV 캐시는 인접 메모리에 저장되지만 크기가 변경되면 메모리 조각화가 발생하여 계산에 비효율적일 수 있습니다.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="/docs/v2.6.x/assets/vllm_1.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>이미지 1. 기존 시스템에서의 KV 캐시 메모리 관리(2023년 페이징된 주의 <a href="https://arxiv.org/pdf/2309.06180">문서</a>)</em></p>
<p>KV 캐시에 가상 메모리를 사용하는 vLLM은 필요에 따라 물리적 GPU 메모리만 할당하므로 메모리 조각화를 제거하고 사전 할당을 피할 수 있습니다. 테스트 결과, vLLM은 NVIDIA A10G 및 A100 GPU에서 <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">허깅페이스 트랜스포머</a> (HF) 및 <a href="https://github.com/huggingface/text-generation-inference">텍스트 생성 추론</a> (TGI)보다 뛰어난 성능을 발휘하여 HF보다 최대 24배, TGI보다 3.5배 더 높은 처리량을 달성했습니다.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="/docs/v2.6.x/assets/vllm_2.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>이미지 2. 각 요청이 3개의 병렬 출력 완료를 요청할 때 처리량 제공. vLLM은 HF보다 8.5배~15배, TGI보다 3.3배~3.5배 높은 처리량을 달성합니다(2023년 <a href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM 블로그</a>).</em></p>
<h3 id="Meta’s-Llama-31" class="common-anchor-header">메타의 라마 3.1</h3><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models"><strong>메타의 라마 3.1은</strong></a> 2024년 7월 23일에 발표되었습니다. 405B 모델은 여러 공개 벤치마크에서 최첨단 성능을 제공하며, 다양한 상업적 사용이 허용된 128,000개의 입력 토큰 컨텍스트 창을 가지고 있습니다. 메타는 405억 개의 파라미터 모델과 함께 Llama3 70B(700억 개의 파라미터)와 8B(80억 개의 파라미터)의 업데이트 버전을 출시했습니다. 모델 가중치는 <a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VWbMJv2vnLfjW3Rh6L96gqS5YW7MhRLh5j9tjNN8BHR5W3qgyTW6N1vHY6lZ3l8N8htfRfqP8DzW72mhHB6vwYd2W77hFt886l4_PV22X226RPmZbW67mSH08gVp9MW2jcZvf24w97BW207Jmf8gPH0yW20YPQv261xxjW8nc6VW3jj-nNW6XdRhg5HhZk_W1QS0yL9dJZb0W818zFK1w62kdW8y-_4m1gfjfNW2jswrd3xbv-yW5mrvdk3n-KqyW45sLMF21qDrwW5TR3vr2MYxZ9W2hWhq23q-nQdW4blHqh3JlZWfW937hlZ58-KJCW82Pgv9384MbYW7yp56M6pvzd6f77wnH004">메타 웹사이트에서</a> 다운로드할 수 있습니다.</p>
<p>핵심 인사이트는 생성된 데이터를 미세 조정하면 성능을 향상시킬 수 있지만, 품질이 좋지 않은 예제는 성능을 저하시킬 수 있다는 것이었습니다. Llama 팀은 모델 자체, 보조 모델 및 기타 도구를 사용하여 이러한 불량 예제를 식별하고 제거하기 위해 광범위하게 작업했습니다.</p>
<h2 id="Build-and-Perform-the-RAG-Retrieval-with-Milvus" class="common-anchor-header">Milvus로 RAG 검색 구축 및 수행하기<button data-href="#Build-and-Perform-the-RAG-Retrieval-with-Milvus" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Prepare-your-dataset" class="common-anchor-header">데이터 집합을 준비합니다.</h3><p>이 데모에서는 공식 <a href="https://milvus.io/docs/">Milvus 설명서를</a> 다운로드하여 로컬에 저장한 데이터 세트를 사용했습니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> DirectoryLoader
<span class="hljs-comment"># Load HTML files already saved in a local directory</span>
path = <span class="hljs-string">&quot;../../RAG/rtdocs_new/&quot;</span>
global_pattern = <span class="hljs-string">&#x27;*.html&#x27;</span>
loader = DirectoryLoader(path=path, glob=global_pattern)
docs = loader.load()


<span class="hljs-comment"># Print num documents and a preview.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loaded <span class="hljs-subst">{<span class="hljs-built_in">len</span>(docs)}</span> documents&quot;</span>)
<span class="hljs-built_in">print</span>(docs[<span class="hljs-number">0</span>].page_content)
pprint.pprint(docs[<span class="hljs-number">0</span>].metadata)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">loaded 22 documents
Why Milvus Docs Tutorials Tools Blog Community Stars0 Try Managed Milvus FREE Search Home v2.4.x About ...
{&#x27;source&#x27;: &#x27;https://milvus.io/docs/quickstart.md&#x27;}
<button class="copy-code-btn"></button></code></pre>
<h3 id="Download-an-embedding-model" class="common-anchor-header">임베딩 모델을 다운로드합니다.</h3><p>다음으로, HuggingFace에서 무료 오픈 소스 <a href="https://zilliz.com/ai-models">임베딩 모델을</a> 다운로드합니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer


<span class="hljs-comment"># Initialize torch settings for device-agnostic code.</span>
N_GPU = torch.cuda.device_count()
DEVICE = torch.device(<span class="hljs-string">&#x27;cuda:N_GPU&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)


<span class="hljs-comment"># Download the model from huggingface model hub.</span>
model_name = <span class="hljs-string">&quot;BAAI/bge-large-en-v1.5&quot;</span>
encoder = SentenceTransformer(model_name, device=DEVICE)


<span class="hljs-comment"># Get the model parameters and save for later.</span>
EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()
MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()


<span class="hljs-comment"># Inspect model parameters.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;model_name: <span class="hljs-subst">{model_name}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;EMBEDDING_DIM: <span class="hljs-subst">{EMBEDDING_DIM}</span>&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;MAX_SEQ_LENGTH: <span class="hljs-subst">{MAX_SEQ_LENGTH}</span>&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">model_name: BAAI/bge-large-en-v1.5
EMBEDDING_DIM: 1024
MAX_SEQ_LENGTH: 512
<button class="copy-code-btn"></button></code></pre>
<h3 id="Chunk-and-encode-your-custom-data-as-vectors" class="common-anchor-header">사용자 지정 데이터를 벡터로 청크하고 인코딩합니다.</h3><p>저는 10% 겹치는 512자의 고정 길이를 사용하겠습니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter


CHUNK_SIZE = <span class="hljs-number">512</span>
chunk_overlap = np.<span class="hljs-built_in">round</span>(CHUNK_SIZE * <span class="hljs-number">0.10</span>, <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;chunk_size: <span class="hljs-subst">{CHUNK_SIZE}</span>, chunk_overlap: <span class="hljs-subst">{chunk_overlap}</span>&quot;</span>)


<span class="hljs-comment"># Define the splitter.</span>
child_splitter = RecursiveCharacterTextSplitter(
   chunk_size=CHUNK_SIZE,
   chunk_overlap=chunk_overlap)


<span class="hljs-comment"># Chunk the docs.</span>
chunks = child_splitter.split_documents(docs)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">len</span>(docs)}</span> docs split into <span class="hljs-subst">{<span class="hljs-built_in">len</span>(chunks)}</span> child documents.&quot;</span>)


<span class="hljs-comment"># Encoder input is doc.page_content as strings.</span>
list_of_strings = [doc.page_content <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> chunks <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(doc, <span class="hljs-string">&#x27;page_content&#x27;</span>)]


<span class="hljs-comment"># Embedding inference using HuggingFace encoder.</span>
embeddings = torch.tensor(encoder.encode(list_of_strings))


<span class="hljs-comment"># Normalize the embeddings.</span>
embeddings = np.array(embeddings / np.linalg.norm(embeddings))


<span class="hljs-comment"># Milvus expects a list of `numpy.ndarray` of `numpy.float32` numbers.</span>
converted_values = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(np.float32, embeddings))


<span class="hljs-comment"># Create dict_list for Milvus insertion.</span>
dict_list = []
<span class="hljs-keyword">for</span> chunk, vector <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(chunks, converted_values):
   <span class="hljs-comment"># Assemble embedding vector, original text chunk, metadata.</span>
   chunk_dict = {
       <span class="hljs-string">&#x27;chunk&#x27;</span>: chunk.page_content,
       <span class="hljs-string">&#x27;source&#x27;</span>: chunk.metadata.get(<span class="hljs-string">&#x27;source&#x27;</span>, <span class="hljs-string">&quot;&quot;</span>),
       <span class="hljs-string">&#x27;vector&#x27;</span>: vector,
   }
   dict_list.append(chunk_dict)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">chunk_size: 512, chunk_overlap: 51.0
22 docs split into 355 child documents.
<button class="copy-code-btn"></button></code></pre>
<h3 id="Save-the-vectors-in-Milvus" class="common-anchor-header">Milvus에 벡터를 저장합니다.</h3><p>인코딩된 벡터 임베딩을 Milvus 벡터 데이터베이스에서 수집합니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Connect a client to the Milvus Lite server.</span>
<span class="hljs-keyword">from</span> pymilvus <span class="hljs-keyword">import</span> MilvusClient
mc = MilvusClient(<span class="hljs-string">&quot;milvus_demo.db&quot;</span>)


<span class="hljs-comment"># Create a collection with flexible schema and AUTOINDEX.</span>
COLLECTION_NAME = <span class="hljs-string">&quot;MilvusDocs&quot;</span>
mc.create_collection(COLLECTION_NAME,
       EMBEDDING_DIM,
       consistency_level=<span class="hljs-string">&quot;Eventually&quot;</span>,
       auto_id=<span class="hljs-literal">True</span>, 
       overwrite=<span class="hljs-literal">True</span>)


<span class="hljs-comment"># Insert data into the Milvus collection.</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Start inserting entities&quot;</span>)
start_time = time.time()
mc.insert(
   COLLECTION_NAME,
   data=dict_list,
   progress_bar=<span class="hljs-literal">True</span>)


end_time = time.time()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Milvus insert time for <span class="hljs-subst">{<span class="hljs-built_in">len</span>(dict_list)}</span> vectors: &quot;</span>, end=<span class="hljs-string">&quot;&quot;</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{<span class="hljs-built_in">round</span>(end_time - start_time, <span class="hljs-number">2</span>)}</span> seconds&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">Start inserting entities
Milvus insert time for 355 vectors: 0.2 seconds
<button class="copy-code-btn"></button></code></pre>
<h3 id="Perform-a-vector-search" class="common-anchor-header">벡터 검색을 수행합니다.</h3><p>Milvus의 지식창고에서 질문을 하고 가장 가까운 이웃 청크를 검색합니다.</p>
<pre><code translate="no" class="language-python">SAMPLE_QUESTION = <span class="hljs-string">&quot;What do the parameters for HNSW mean?&quot;</span>


<span class="hljs-comment"># Embed the question using the same encoder.</span>
query_embeddings = torch.tensor(encoder.encode(SAMPLE_QUESTION))
<span class="hljs-comment"># Normalize embeddings to unit length.</span>
query_embeddings = F.normalize(query_embeddings, p=<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)
<span class="hljs-comment"># Convert the embeddings to list of list of np.float32.</span>
query_embeddings = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(np.float32, query_embeddings))


<span class="hljs-comment"># Define metadata fields you can filter on.</span>
OUTPUT_FIELDS = <span class="hljs-built_in">list</span>(dict_list[<span class="hljs-number">0</span>].keys())
OUTPUT_FIELDS.remove(<span class="hljs-string">&#x27;vector&#x27;</span>)


<span class="hljs-comment"># Define how many top-k results you want to retrieve.</span>
TOP_K = <span class="hljs-number">2</span>


<span class="hljs-comment"># Run semantic vector search using your query and the vector database.</span>
results = mc.search(
    COLLECTION_NAME,
    data=query_embeddings,
    output_fields=OUTPUT_FIELDS,
    limit=TOP_K,
    consistency_level=<span class="hljs-string">&quot;Eventually&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<p>검색된 결과는 아래와 같습니다.</p>
<pre><code translate="no" class="language-text">Retrieved result #1
distance = 0.7001987099647522
(&#x27;Chunk text: layer, finds the node closest to the target in this layer, and&#x27;
...
&#x27;outgoing&#x27;)
source: https://milvus.io/docs/index.md

Retrieved result #2
distance = 0.6953287124633789
(&#x27;Chunk text: this value can improve recall rate at the cost of increased&#x27;
...
&#x27;to the target&#x27;)
source: https://milvus.io/docs/index.md
<button class="copy-code-btn"></button></code></pre>
<h2 id="Build-and-Perform-the-RAG-Generation-with-vLLM-and-Llama-31-8B" class="common-anchor-header">vLLM 및 Llama 3.1-8B로 RAG 생성 빌드 및 수행하기<button data-href="#Build-and-Perform-the-RAG-Generation-with-vLLM-and-Llama-31-8B" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Install-vLLM-and-models-from-HuggingFace" class="common-anchor-header">HuggingFace에서 vLLM 및 모델 설치하기</h3><p>vLLM은 기본적으로 HuggingFace에서 대용량 언어 모델을 다운로드합니다. 일반적으로 허깅페이스에서 새로운 모델을 사용하려면 -업그레이드 또는 -유를 통해 pip를 설치해야 합니다. 또한 vLLM으로 Meta의 Llama 3.1 모델을 추론하려면 GPU가 필요합니다.</p>
<p>vLLM이 지원되는 모든 모델의 전체 목록은 이 <a href="https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models">문서 페이지를</a> 참조하세요.</p>
<pre><code translate="no" class="language-shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">(Recommended) Create a new conda environment.</span>
conda create -n myenv python=3.11 -y
conda activate myenv
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Install vLLM with CUDA 12.1.</span>
pip install -U vllm transformers torch


import vllm, torch
from vllm import LLM, SamplingParams
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Clear the GPU memory cache.</span>
torch.cuda.empty_cache()
<span class="hljs-meta prompt_">

# </span><span class="language-bash">Check the GPU.</span>
!nvidia-smi
<button class="copy-code-btn"></button></code></pre>
<p>vLLM을 설치하는 방법에 대한 자세한 내용은 <a href="https://docs.vllm.ai/en/latest/getting_started/installation.html">설치</a> 페이지를 참조하세요.</p>
<h3 id="Get-a-HuggingFace-token" class="common-anchor-header">허깅페이스 토큰 받기.</h3><p>메타 라마 3.1과 같은 HuggingFace의 일부 모델은 사용자가 라이선스를 수락해야 가중치를 다운로드할 수 있습니다. 따라서 허깅페이스 계정을 생성하고 모델의 라이선스를 수락한 후 토큰을 생성해야 합니다.</p>
<p>HuggingFace에서 이 <a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B">Llama3.1 페이지를</a> 방문하면 약관에 동의할지 묻는 메시지가 표시됩니다. 모델 가중치를 다운로드하기 전에 "<strong>라이선스 수</strong>락"을 클릭하여 메타 약관에 동의합니다. 승인은 보통 하루도 채 걸리지 않습니다.</p>
<p><strong>승인을 받은 후에는 새로운 HuggingFace 토큰을 생성해야 합니다. 이전 토큰은 새 권한으로 작동하지 않습니다.</strong></p>
<p>vLLM을 설치하기 전에 새 토큰으로 허깅페이스에 로그인하세요. 아래에서는 Colab 시크릿을 사용하여 토큰을 저장했습니다.</p>
<pre><code translate="no" class="language-shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Login to HuggingFace using your new token.</span>
from huggingface_hub import login
from google.colab import userdata
hf_token = userdata.get(&#x27;HF_TOKEN&#x27;)
login(token = hf_token, add_to_git_credential=True)
<button class="copy-code-btn"></button></code></pre>
<h3 id="Run-the-RAG-Generation" class="common-anchor-header">RAG-세대 실행</h3><p>데모에서는 스핀업에 GPU와 상당한 메모리가 필요한 <code translate="no">Llama-3.1-8B</code> 모델을 실행합니다. 다음 예제는 A100 GPU가 있는 Google Colab Pro(월 $10)에서 실행되었습니다. vLLM을 실행하는 방법에 대해 자세히 알아보려면 <a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html">빠른 시작 설명서를</a> 참조하세요.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># 1. Choose a model</span>
MODELTORUN = <span class="hljs-string">&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;</span>


<span class="hljs-comment"># 2. Clear the GPU memory cache, you&#x27;re going to need it all!</span>
torch.cuda.empty_cache()


<span class="hljs-comment"># 3. Instantiate a vLLM model instance.</span>
llm = LLM(model=MODELTORUN,
         enforce_eager=<span class="hljs-literal">True</span>,
         dtype=torch.bfloat16,
         gpu_memory_utilization=<span class="hljs-number">0.5</span>,
         max_model_len=<span class="hljs-number">1000</span>,
         seed=<span class="hljs-number">415</span>,
         max_num_batched_tokens=<span class="hljs-number">3000</span>)
<button class="copy-code-btn"></button></code></pre>
<p>Milvus에서 검색한 컨텍스트와 소스를 사용하여 프롬프트를 작성합니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Separate all the context together by space.</span>
contexts_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(contexts)
<span class="hljs-comment"># Lance Martin, LangChain, says put the best contexts at the end.</span>
contexts_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">reversed</span>(contexts))


<span class="hljs-comment"># Separate all the unique sources together by comma.</span>
source_combined = <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">dict</span>.fromkeys(sources))))


SYSTEM_PROMPT = <span class="hljs-string">f&quot;&quot;&quot;First, check if the provided Context is relevant to
the user&#x27;s question.  Second, only if the provided Context is strongly relevant, answer the question using the Context.  Otherwise, if the Context is not strongly relevant, answer the question without using the Context. 
Be clear, concise, relevant.  Answer clearly, in fewer than 2 sentences.
Grounding sources: <span class="hljs-subst">{source_combined}</span>
Context: <span class="hljs-subst">{contexts_combined}</span>
User&#x27;s question: <span class="hljs-subst">{SAMPLE_QUESTION}</span>
&quot;&quot;&quot;</span>


prompts = [SYSTEM_PROMPT]
<button class="copy-code-btn"></button></code></pre>
<p>이제 검색된 청크와 프롬프트에 채워진 원래 질문을 사용하여 답변을 생성합니다.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># Sampling parameters</span>
sampling_params = SamplingParams(temperature=<span class="hljs-number">0.2</span>, top_p=<span class="hljs-number">0.95</span>)


<span class="hljs-comment"># Invoke the vLLM model.</span>
outputs = llm.generate(prompts, sampling_params)


<span class="hljs-comment"># Print the outputs.</span>
<span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> outputs:
   prompt = output.prompt
   generated_text = output.outputs[<span class="hljs-number">0</span>].text
   <span class="hljs-comment"># !r calls repr(), which prints a string inside quotes.</span>
   <span class="hljs-built_in">print</span>()
   <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Question: <span class="hljs-subst">{SAMPLE_QUESTION!r}</span>&quot;</span>)
   pprint.pprint(<span class="hljs-string">f&quot;Generated text: <span class="hljs-subst">{generated_text!r}</span>&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<pre><code translate="no" class="language-text">Question: &#x27;What do the parameters for HNSW MEAN!?&#x27;
Generated text: &#x27;Answer: The parameters for HNSW (Hiera(rchical Navigable Small World Graph) are: &#x27;
&#x27;* M: The maximum degree of nodes on each layer oof the graph, which can improve &#x27;
&#x27;recall rate at the cost of increased search time. * efConstruction and ef: &#x27; 
&#x27;These parameters specify a search range when building or searching an index.&#x27;
<button class="copy-code-btn"></button></code></pre>
<p>위의 답변이 완벽해 보입니다!</p>
<p>이 데모에 관심이 있으시다면 직접 사용해 보시고 의견을 알려주세요. 또한 <a href="https://discord.com/invite/8uyFbECzPX">Discord의 Milvus 커뮤니티에</a> 가입하여 모든 GenAI 개발자와 직접 대화를 나눌 수도 있습니다.</p>
<h2 id="References" class="common-anchor-header">참고 자료<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><p>vLLM <a href="https://docs.vllm.ai/en/latest/getting_started/installation.html">공식 문서</a> 및 <a href="https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models">모델 페이지</a>.</p></li>
<li><p><a href="https://arxiv.org/pdf/2309.06180">호출 주의에 관한 2023 vLLM 논문</a></p></li>
<li><p>레이 서밋에서<a href="https://www.youtube.com/watch?v=80bIUggRJf4">2023년 vLLM 프레젠테이션</a> </p></li>
<li><p>vLLM 블로그: <a href="https://blog.vllm.ai/2023/06/20/vllm.html">원페이지어텐션으로 쉽고, 빠르고, 저렴한 LLM 서비스 제공: vLLM</a></p></li>
<li><p>vLLM 서버 실행에 대한 유용한 블로그입니다: <a href="https://ploomber.io/blog/vllm-deploy/">vLLM 배포: 단계별 가이드</a></p></li>
<li><p><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">라마 3 모델 무리 | 연구 - Meta의 AI</a></p></li>
</ul>
