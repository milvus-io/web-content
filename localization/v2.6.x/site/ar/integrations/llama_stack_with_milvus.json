{"codeList":["$ git clone https://github.com/meta-llama/llama-stack.git\n$ cd llama-stack\n","$ conda create -n stack python=3.10\n$ conda activate stack\n\n$ pip install -e .\n","vector_io:\n- provider_id: milvus\n  provider_type: inline::milvus\n  config:\n    db_path: ~/.llama/distributions/together/milvus_store.db\n\n#  - provider_id: milvus\n#    provider_type: remote::milvus\n#    config:\n#      uri: http://localhost:19530\n#      token: root:Milvus\n","$ llama stack build --template together --image-type conda\n","$ llama stack run --image-type conda ~/.llama/distributions/together/together-run.yaml\n","import uuid\nfrom llama_stack_client.types import Document\nfrom llama_stack_client.lib.agents.agent import Agent\nfrom llama_stack_client.types.agent_create_params import AgentConfig\n\n# See https://www.together.ai/models for all available models\nINFERENCE_MODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\nLLAMA_STACK_PORT = 8321\n\n\ndef create_http_client():\n    from llama_stack_client import LlamaStackClient\n\n    return LlamaStackClient(\n        base_url=f\"http://localhost:{LLAMA_STACK_PORT}\"  # Your Llama Stack Server URL\n    )\n\n\nclient = create_http_client()\n\n# Documents to be used for RAG\nurls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\ndocuments = [\n    Document(\n        document_id=f\"num-{i}\",\n        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n        mime_type=\"text/plain\",\n        metadata={},\n    )\n    for i, url in enumerate(urls)\n]\n\n# Register a vector database\nvector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\nclient.vector_dbs.register(\n    vector_db_id=vector_db_id,\n    embedding_model=\"all-MiniLM-L6-v2\",\n    embedding_dimension=384,\n    provider_id=\"milvus\",\n)\n\nprint(\"inserting...\")\n# Insert the documents into the vector database\nclient.tool_runtime.rag_tool.insert(\n    documents=documents, vector_db_id=vector_db_id, chunk_size_in_tokens=1024,\n)\n\nagent_config = AgentConfig(\n    model=INFERENCE_MODEL,\n    # Define instructions for the agent ( aka system prompt)\n    instructions=\"You are a helpful assistant\",\n    enable_session_persistence=False,\n    # Define tools available to the agent\n    toolgroups=[{\"name\": \"builtin::rag\", \"args\": {\"vector_db_ids\": [vector_db_id]}}],\n)\n\nrag_agent = Agent(client, agent_config)\nsession_id = rag_agent.create_session(\"test-session\")\nprint(\"finish init agent...\")\nuser_prompt = (\n    \"What are the top 5 topics that were explained? Only list succinct bullet points.\"\n)\n\n# Get the final answer from the agent\nresponse = rag_agent.create_turn(\n    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n    session_id=session_id,\n    stream=False,\n)\nprint(f\"Response: \")\nprint(response.output_message.content)\n","inserting...\nfinish init agent...\nResponse: \n* Fine-Tuning Llama3 with Chat Data\n* Evaluating fine-tuned Llama3-8B models with EleutherAI's Eval Harness\n* Generating text with our fine-tuned Llama3 model\n* Faster generation via quantization\n* Fine-tuning on a custom chat dataset\n"],"headingContent":"Build RAG with Llama Stack with Milvus","anchorList":[{"label":"بناء RAG مع Llama Stack مع Milvus","href":"Build-RAG-with-Llama-Stack-with-Milvus","type":1,"isActive":false},{"label":"إعداد البيئة","href":"Preparing-the-Environment","type":2,"isActive":false},{"label":"بدء تشغيل خادم لاما ستاك","href":"Start-Llama-Stack-Server","type":2,"isActive":false},{"label":"تنفيذ RAG من العميل","href":"Perform-RAG-from-client","type":2,"isActive":false}]}