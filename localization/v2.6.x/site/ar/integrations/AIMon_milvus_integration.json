{"codeList":["$ pip3 install -U gdown requests aimon llama-index-core llama-index-vector-stores-milvus pymilvus>=2.4.2 llama-index-postprocessor-aimon-rerank llama-index-embeddings-openai llama-index-llms-openai datasets fuzzywuzzy --quiet\n","import os\n\n# Check if the secrets are accessible\nfrom google.colab import userdata\n\n# Get this from the AIMon UI\naimon_key = userdata.get(\"AIMON_API_KEY\")\n\nopenai_key = userdata.get(\"OPENAI_API_KEY\")\n\n# Set OpenAI key as an environment variable as well\nos.environ[\"OPENAI_API_KEY\"] = openai_key\n","from openai import OpenAI\n\noai_client = OpenAI(api_key=openai_key)\n\n\ndef query_openai_with_context(query, context_documents, model=\"gpt-4o-mini\"):\n    \"\"\"\n    Sends a query along with context documents to the OpenAI API and returns the parsed response.\n\n    :param api_key: OpenAI API key\n    :param query: The user's query as a string\n    :param context_documents: A list of strings representing context documents\n    :param model: The OpenAI model to use (default is 'o3-mini')\n    :return: Response text from the OpenAI API\n    \"\"\"\n\n    # Combine context documents into a single string\n    context_text = \"\\n\\n\".join(context_documents)\n\n    # Construct the messages payload\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an AI assistant that provides accurate and helpful answers.\",\n        },\n        {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nQuestion:\\n{query}\"},\n    ]\n\n    # Call OpenAI API\n    completion = oai_client.chat.completions.create(model=model, messages=messages)\n\n    # Extract and return the response text\n    return completion.choices[0].message.content\n","# Delete the dataset folder if it already exists\n\nimport shutil\n\nfolder_path = \"/content/meetingbank_train_split.hf\"\n\nif os.path.exists(folder_path):\n    try:\n        shutil.rmtree(folder_path)\n        print(f\"Folder '{folder_path}' and its contents deleted successfully.\")\n    except Exception as e:\n        print(f\"Error deleting folder '{folder_path}': {e}\")\nelse:\n    print(f\"Folder '{folder_path}' does not exist.\")\n","# Download the dataset locally\n$ gdown https://drive.google.com/uc?id=1bs4kwwiD30DUeCjuqEdOeixCuI-3i9F5\n$ gdown https://drive.google.com/uc?id=1fkxaS8eltjfkzws5BRXpVXnxl2Qxwy5F\n","import tarfile\nfrom datasets import load_from_disk\n\n\ntar_file_path = \"/content/meetingbank_train_split.tar.gz\"\nextract_path = \"/content/\"\n\n# Extract the file\nwith tarfile.open(tar_file_path, \"r:gz\") as tar:\n    tar.extractall(path=extract_path)\n\nprint(f\"Extracted to: {extract_path}\")\n\ntrain_split = load_from_disk(extract_path + \"meetingbank_train_split.hf\")\n","len(train_split)\n","# Total number of token across the entire set of transcripts\n# This is approximately 15M tokens in size\ntotal_tokens = sum(len(example[\"transcript\"].split()) for example in train_split)\nprint(f\"Total number of tokens in train split: {total_tokens}\")\n","# number of words ~= # of tokens\nlen(train_split[1][\"transcript\"].split())\n","# Show the first 500 characters of the transcript\ntrain_split[1][\"transcript\"][:500]\n","# Average number of tokens per transcript\nimport statistics\n\nstatistics.mean(len(example[\"transcript\"].split()) for example in train_split)\n","import pandas as pd\n\nqueries_df = pd.read_csv(\"/content/score_metrics_relevant_examples_2.csv\")\n","len(queries_df[\"Query\"])\n","queries_df[\"Query\"].to_list()\n","# We will check the LLM response against these instructions\ninstructions_to_evaluate = \"\"\"\n1. Ensure that the response answers all parts of the query completely.\n2. Ensure that the length of the response is under 50 words.\n3. The response must not contain any abusive language or toxic content.\n4. The response must be in a friendly tone.\n\"\"\"\n","def compute_quality_score(aimon_response):\n    retrieval_rel_scores = aimon_response.detect_response.retrieval_relevance[0][\n        \"relevance_scores\"\n    ]\n    avg_retrieval_relevance_score = (\n        statistics.mean(retrieval_rel_scores) if len(retrieval_rel_scores) > 0 else 0.0\n    )\n    hall_score = aimon_response.detect_response.hallucination[\"score\"]\n    ia_score = aimon_response.detect_response.instruction_adherence[\"score\"]\n    return (\n        0.35 * (1.0 - hall_score)\n        + 0.35 * ia_score\n        + 0.3 * (avg_retrieval_relevance_score / 100)\n    ) * 100.0\n","from aimon import Detect\n\naimon_config = {\n    \"hallucination\": {\"detector_name\": \"default\"},\n    \"instruction_adherence\": {\"detector_name\": \"default\"},\n    \"retrieval_relevance\": {\"detector_name\": \"default\"},\n}\ntask_definition = \"\"\"\nYour task is to grade the relevance of context document against a specified user query.\nThe domain here is a meeting transcripts.\n\"\"\"\n\nvalues_returned = [\n    \"context\",\n    \"user_query\",\n    \"instructions\",\n    \"generated_text\",\n    \"task_definition\",\n]\n\ndetect = Detect(\n    values_returned=values_returned,\n    api_key=userdata.get(\"AIMON_API_KEY\"),\n    config=aimon_config,\n    publish=True,  # This publishes results to the AIMon UI\n    application_name=\"meeting_bot_app\",\n    model_name=\"OpenAI-gpt-4o-mini\",\n)\n","from fuzzywuzzy import process\nimport time\n\n# List of documents\ndocuments = [t[\"transcript\"] for t in train_split]\n\n\n@detect\ndef get_fuzzy_match_response(query, docs):\n    response = query_openai_with_context(query, docs)\n    return docs, query, instructions_to_evaluate, response, task_definition\n\n\nst = time.time()\nquality_scores_bf = []\navg_retrieval_rel_scores_bf = []\nresponses = {}\nfor user_query in queries_df[\"Query\"].to_list():\n    best_match = process.extractBests(user_query, documents)\n    matched_docs = [b[0][:2000] for b in best_match]\n    _, _, _, llm_res, _, aimon_response = get_fuzzy_match_response(\n        user_query, matched_docs[:1]\n    )\n    # These show the average retrieval relevance scores per query.\n    retrieval_rel_scores = aimon_response.detect_response.retrieval_relevance[0][\n        \"relevance_scores\"\n    ]\n    avg_retrieval_rel_score_per_query = (\n        statistics.mean(retrieval_rel_scores) if len(retrieval_rel_scores) > 0 else 0.0\n    )\n    avg_retrieval_rel_scores_bf.append(avg_retrieval_rel_score_per_query)\n    print(\n        \"Avg. Retrieval relevance score across chunks: {} for query: {}\".format(\n            avg_retrieval_rel_score_per_query, user_query\n        )\n    )\n    quality_scores_bf.append(compute_quality_score(aimon_response))\n    responses[user_query] = llm_res\nprint(\"Time taken: {} seconds\".format(time.time() - st))\n","# This is the average quality score.\navg_quality_score_bf = statistics.mean(quality_scores_bf)\nprint(\"Average Quality score for brute force approach: {}\".format(avg_quality_score_bf))\n","# This is the average retrieval relevance score.\navg_retrieval_rel_score_bf = statistics.mean(avg_retrieval_rel_scores_bf)\nprint(\n    \"Average retrieval relevance score for brute force approach: {}\".format(\n        avg_retrieval_rel_score_bf\n    )\n)\n","import json\nimport requests\nimport pandas as pd\nfrom llama_index.core import Document\n\n\n## Function to preprocess text.\ndef preprocess_text(text):\n    text = \" \".join(text.split())\n    return text\n\n\n## Function to process all URLs and create LlamaIndex Documents.\ndef extract_and_create_documents(transcripts):\n\n    documents = []\n\n    for indx, t in enumerate(transcripts):\n        try:\n            clean_text = preprocess_text(t)\n            doc = Document(text=clean_text, metadata={\"index\": indx})\n            documents.append(doc)\n        except Exception as e:\n            print(f\"Failed to process transcript number {indx}: {str(e)}\")\n\n    return documents\n\n\ndocuments = extract_and_create_documents(train_split[\"transcript\"])\n","from llama_index.embeddings.openai import OpenAIEmbedding\n\nembedding_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\", embed_batch_size=100, max_retries=3\n)\n","from llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores.milvus import MilvusVectorStore\n\nvector_store = MilvusVectorStore(\n    uri=\"./aimon_embeddings.db\",\n    collection_name=\"meetingbanks\",\n    dim=1536,\n    overwrite=True,\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(\n    documents=documents, storage_context=storage_context\n)\n","from llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\n\nretriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n\n# The system prompt that will be used for the LLM\nsystem_prompt = \"\"\"\n                Please be professional and polite.\n                Answer the user's question in a single line.\n                \"\"\"\n","## OpenAI's LLM, we will use GPT-4o-mini here since it is a fast and cheap LLM\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1, system_prompt=system_prompt)\n","from llama_index.core.query_engine import RetrieverQueryEngine\n\nquery_engine = RetrieverQueryEngine.from_args(retriever, llm)\n","import logging\n\n\n@detect\ndef ask_and_validate(user_query, user_instructions, query_engine=query_engine):\n\n    response = query_engine.query(user_query)\n\n    ## Nested function to retrieve context and relevance scores from the LLM response.\n    def get_source_docs(chat_response):\n        contexts = []\n        relevance_scores = []\n        if hasattr(chat_response, \"source_nodes\"):\n            for node in chat_response.source_nodes:\n                if (\n                    hasattr(node, \"node\")\n                    and hasattr(node.node, \"text\")\n                    and hasattr(node, \"score\")\n                    and node.score is not None\n                ):\n                    contexts.append(node.node.text)\n                    relevance_scores.append(node.score)\n                elif (\n                    hasattr(node, \"text\")\n                    and hasattr(node, \"score\")\n                    and node.score is not None\n                ):\n                    contexts.append(node.text)\n                    relevance_scores.append(node.score)\n                else:\n                    logging.info(\"Node does not have required attributes.\")\n        else:\n            logging.info(\"No source_nodes attribute found in the chat response.\")\n        return contexts, relevance_scores\n\n    context, relevance_scores = get_source_docs(response)\n    return context, user_query, user_instructions, response.response, task_definition\n","# Quick check to ensure everything is working with the vector DB\nask_and_validate(\"Councilman Lopez\", instructions_to_evaluate)\n","import time\n\nquality_scores_vdb = []\navg_retrieval_rel_scores_vdb = []\nresponses_adb = {}\nast = time.time()\nfor user_query in queries_df[\"Query\"].to_list():\n    _, _, _, llm_res, _, aimon_response = ask_and_validate(\n        user_query, instructions_to_evaluate\n    )\n    # These show the average retrieval relevance scores per query. Compare this to the previous brute force method.\n    retrieval_rel_scores = aimon_response.detect_response.retrieval_relevance[0][\n        \"relevance_scores\"\n    ]\n    avg_retrieval_rel_score_per_query = (\n        statistics.mean(retrieval_rel_scores) if len(retrieval_rel_scores) > 0 else 0.0\n    )\n    avg_retrieval_rel_scores_vdb.append(avg_retrieval_rel_score_per_query)\n    print(\n        \"Avg. Retrieval relevance score across chunks: {} for query: {}\".format(\n            avg_retrieval_rel_score_per_query, user_query\n        )\n    )\n    quality_scores_vdb.append(compute_quality_score(aimon_response))\n    responses_adb[user_query] = llm_res\nprint(\"Time elapsed: {} seconds\".format(time.time() - ast))\n","# This is the average quality score.\navg_quality_score_vdb = statistics.mean(quality_scores_vdb)\nprint(\"Average Quality score for vector DB approach: {}\".format(avg_quality_score_vdb))\n","# This is the average retrieval relevance score.\navg_retrieval_rel_score_vdb = statistics.mean(avg_retrieval_rel_scores_vdb)\nprint(\n    \"Average retrieval relevance score for vector DB approach: {}\".format(\n        avg_retrieval_rel_score_vdb\n    )\n)\n","# Setup AIMon's reranker\n\nfrom llama_index.postprocessor.aimon_rerank import AIMonRerank\n\n# This is a simple task_definition, you can polish and customize it for your use cases as needed\ntask_definition = \"\"\"\nYour task is to match documents for a specific query.\nThe documents are transcripts of meetings of city councils of 6 major U.S. cities.\n\"\"\"\n\naimon_rerank = AIMonRerank(\n    top_n=2,\n    api_key=userdata.get(\"AIMON_API_KEY\"),\n    task_definition=task_definition,\n)\n","# Setup a new query engine but now with a reranker added as a post processor after retrieval\n\nquery_engine_with_reranking = RetrieverQueryEngine.from_args(\n    retriever, llm, node_postprocessors=[aimon_rerank]\n)\n","import time\n\nqual_scores_rr = []\navg_retrieval_rel_scores_rr = []\nresponses_adb_rr = {}\nast_rr = time.time()\nfor user_query in queries_df[\"Query\"].to_list():\n    _, _, _, llm_res, _, aimon_response = ask_and_validate(\n        user_query, instructions_to_evaluate, query_engine=query_engine_with_reranking\n    )\n    # These show the average retrieval relevance scores per query. Compare this to the previous method without the re-ranker\n    retrieval_rel_scores = aimon_response.detect_response.retrieval_relevance[0][\n        \"relevance_scores\"\n    ]\n    avg_retrieval_rel_score_per_query = (\n        statistics.mean(retrieval_rel_scores) if len(retrieval_rel_scores) > 0 else 0.0\n    )\n    avg_retrieval_rel_scores_rr.append(avg_retrieval_rel_score_per_query)\n    print(\n        \"Avg. Retrieval relevance score across chunks: {} for query: {}\".format(\n            avg_retrieval_rel_score_per_query, user_query\n        )\n    )\n    qual_scores_rr.append(compute_quality_score(aimon_response))\n    responses_adb_rr[user_query] = llm_res\nprint(\"Time elapsed: {} seconds\".format(time.time() - ast_rr))\n","# This is the average quality score.\navg_quality_score_rr = statistics.mean(qual_scores_rr)\nprint(\n    \"Average Quality score for AIMon Re-ranking approach: {}\".format(\n        avg_quality_score_rr\n    )\n)\n","# This is the average retrieval relevance score.\navg_retrieval_rel_score_rr = statistics.mean(avg_retrieval_rel_scores_rr)\nprint(\n    \"Average retrieval relevance score for AIMon Re-ranking approach: {}\".format(\n        avg_retrieval_rel_score_rr\n    )\n)\n","import pandas as pd\n\ndf_scores = pd.DataFrame(\n    {\n        \"Approach\": [\"Brute-Force\", \"VectorDB\", \"AIMon-Rerank\"],\n        \"Quality Score\": [\n            avg_quality_score_bf,\n            avg_quality_score_vdb,\n            avg_quality_score_rr,\n        ],\n        \"Retrieval Relevance Score\": [\n            avg_retrieval_rel_score_bf,\n            avg_retrieval_rel_score_vdb,\n            avg_retrieval_rel_score_rr,\n        ],\n    }\n)\n\n# % increase of quality scores relative to Brute-Force\ndf_scores[\"Increase in Quality Score (%)\"] = (\n    (df_scores[\"Quality Score\"] - avg_quality_score_bf) / avg_quality_score_bf\n) * 100\ndf_scores.loc[0, \"Increase in Quality Score (%)\"] = 0\n\n# % increase of retrieval relative scores relative to Brute-Force\ndf_scores[\"Increase in Retrieval Relevance Score (%)\"] = (\n    (df_scores[\"Retrieval Relevance Score\"] - avg_retrieval_rel_score_bf)\n    / avg_retrieval_rel_score_bf\n) * 100\ndf_scores.loc[0, \"Increase in Retrieval Relevance Score (%)\"] = 0\n\ndf_scores\n"],"headingContent":"Improve retrieval quality of your LLM Application with AIMon and Milvus","anchorList":[{"label":"ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ØªØ·Ø¨ÙŠÙ‚ LLM Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ø¹ AIMon Ùˆ Milvus","href":"Improve-retrieval-quality-of-your-LLM-Application-with-AIMon-and-Milvus","type":1,"isActive":false},{"label":"Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©","href":"Overview","type":2,"isActive":false},{"label":"Ø§Ù„Ù…ÙƒØ¯Ø³ Ø§Ù„ØªÙ‚Ù†ÙŠ","href":"Tech-Stack","type":2,"isActive":false},{"label":"Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©","href":"Pre-requisites","type":1,"isActive":false},{"label":"Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©","href":"Utility-Functions","type":2,"isActive":false},{"label":"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª","href":"Dataset","type":1,"isActive":false},{"label":"ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³","href":"Metric-Definition","type":1,"isActive":false},{"label":"Ø¥Ø¹Ø¯Ø§Ø¯ AIMon","href":"Setup-AIMon","type":1,"isActive":false},{"label":"1. Ù†Ù‡Ø¬ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„ØºØ§Ø´Ù…Ø© Ø§Ù„Ø¨Ø³ÙŠØ·","href":"1-Simple-brute-force-approach","type":1,"isActive":false},{"label":"2. Ø§Ø³ØªØ®Ø¯Ø§Ù… VectorDB (Milvus) Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª","href":"2-Use-a-VectorDB-Milvus-for-document-retrieval","type":1,"isActive":false},{"label":"ğŸ‰ ØªØ­Ø³Ù†Øª Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©!","href":"ğŸ‰-Quality-Score-improved","type":2,"isActive":false},{"label":"3. Ø¥Ø¶Ø§ÙØ© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø¥Ù„Ù‰ Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ùƒ","href":"3-Add-Re-ranking-to-your-retrieval","type":1,"isActive":false},{"label":"ğŸ‰ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ ØªØ­Ø³Ù†Øª Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©!","href":"ğŸ‰-Again-Quality-Score-improved","type":2,"isActive":false}]}