{"codeList":["$ pip install 'feast[milvus]' openai -U -q\n","import os\nfrom openai import OpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-**************\"\n\nllm_client = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n","feature_repo/\n│── data/                  # Contains pre-processed Wikipedia city data in Parquet format\n│── example_repo.py        # Defines feature views and entities for the city data\n│── feature_store.yaml     # Configures Milvus and feature store settings\n│── test_workflow.py       # Example workflow for Feast operations\n","project: rag\nprovider: local\nregistry: data/registry.db\n\nonline_store:\n  type: milvus            # Uses Milvus for vector storage\n  path: data/online_store.db\n  vector_enabled: true    # Enables vector similarity search\n  embedding_dim: 384      # Dimension of our embeddings\n  index_type: \"FLAT\"      # Vector index type\n  metric_type: \"COSINE\"   # Similarity metric\n\noffline_store:\n  type: file              # Uses file-based offline storage\n","import pandas as pd\n\ndf = pd.read_parquet(\n    \"/path/to/feature_repo/data/city_wikipedia_summaries_with_embeddings.parquet\"\n)\ndf[\"vector\"] = df[\"vector\"].apply(lambda x: x.tolist())\nembedding_length = len(df[\"vector\"][0])\nprint(f\"embedding length = {embedding_length}\")\n","from IPython.display import display\n\ndisplay(df.head())\n","feast apply\n","from datetime import datetime\nfrom feast import FeatureStore\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nstore = FeatureStore(repo_path=\"/path/to/feature_repo\")\n","store.write_to_online_store(feature_view_name=\"city_embeddings\", df=df)\n","pymilvus_client = store._provider._online_store._connect(store.config)\nCOLLECTION_NAME = pymilvus_client.list_collections()[0]\n\nmilvus_query_result = pymilvus_client.query(\n    collection_name=COLLECTION_NAME,\n    filter=\"item_id == '0'\",\n)\npd.DataFrame(milvus_query_result[0]).head()\n","import torch\nimport torch.nn.functional as F\nfrom feast import FeatureStore\nfrom pymilvus import MilvusClient, DataType, FieldSchema\nfrom transformers import AutoTokenizer, AutoModel\nfrom example_repo import city_embeddings_feature_view, item\n\nTOKENIZER = \"sentence-transformers/all-MiniLM-L6-v2\"\nMODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\n        0\n    ]  # First element of model_output contains all token embeddings\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\ndef run_model(sentences, tokenizer, model):\n    encoded_input = tokenizer(\n        sentences, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n    return sentence_embeddings\n","question = \"Which city has the largest population in New York?\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\nmodel = AutoModel.from_pretrained(MODEL)\nquery_embedding = run_model(question, tokenizer, model)\nquery = query_embedding.detach().cpu().numpy().tolist()[0]\n","from IPython.display import display\n\n# Retrieve top k documents\ncontext_data = store.retrieve_online_documents_v2(\n    features=[\n        \"city_embeddings:vector\",\n        \"city_embeddings:item_id\",\n        \"city_embeddings:state\",\n        \"city_embeddings:sentence_chunks\",\n        \"city_embeddings:wiki_summary\",\n    ],\n    query=query,\n    top_k=3,\n    distance_metric=\"COSINE\",\n).to_df()\ndisplay(context_data)\n","def format_documents(context_df):\n    output_context = \"\"\n    unique_documents = context_df.drop_duplicates().apply(\n        lambda x: \"City & State = {\"\n        + x[\"state\"]\n        + \"}\\nSummary = {\"\n        + x[\"wiki_summary\"].strip()\n        + \"}\",\n        axis=1,\n    )\n    for i, document_text in enumerate(unique_documents):\n        output_context += f\"****START DOCUMENT {i}****\\n{document_text.strip()}\\n****END DOCUMENT {i}****\"\n    return output_context\n\n\nRAG_CONTEXT = format_documents(context_data[[\"state\", \"wiki_summary\"]])\nprint(RAG_CONTEXT)\n","FULL_PROMPT = f\"\"\"\nYou are an assistant for answering questions about states. You will be provided documentation from Wikipedia. Provide a conversational answer.\nIf you don't know the answer, just say \"I do not know.\" Don't make up an answer.\n\nHere are document(s) you should use when answer the users question:\n{RAG_CONTEXT}\n\"\"\"\n","response = llm_client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"system\", \"content\": FULL_PROMPT},\n        {\"role\": \"user\", \"content\": question},\n    ],\n)\n\nprint(\"\\n\".join([c.message.content for c in response.choices]))\n"],"headingContent":"Build RAG with Milvus and Feast","anchorList":[{"label":"بناء RAG باستخدام Milvus وFeast","href":"Build-RAG-with-Milvus-and-Feast","type":1,"isActive":false},{"label":"لماذا العيد؟","href":"Why-Feast","type":1,"isActive":false},{"label":"التحضير","href":"Preparation","type":2,"isActive":false},{"label":"إعداد البيانات","href":"Prepare-the-Data","type":2,"isActive":false},{"label":"فحص البيانات","href":"Inspect-the-Data","type":2,"isActive":false},{"label":"تسجيل تعريفات الميزات ونشر مخزن الميزات","href":"Register-Feature-Definitions-and-Deploy-the-Feature-Store","type":2,"isActive":false},{"label":"تحميل الميزات في ملفوس","href":"Load-Features-into-Milvus","type":2,"isActive":false},{"label":"بناء RAG","href":"Build-RAG","type":2,"isActive":false}]}