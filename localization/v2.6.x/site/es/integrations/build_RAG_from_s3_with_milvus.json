{"codeList":["$ pip install --upgrade --quiet pymilvus openai requests tqdm boto3 langchain langchain-core langchain-community langchain-text-splitters langchain-milvus langchain-openai bs4\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n","os.environ[\"AWS_ACCESS_KEY_ID\"] = \"your-aws-access-key-id\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your-aws-secret-access-key\"\n","from langchain_community.document_loaders import S3FileLoader\n\nloader = S3FileLoader(\n    bucket=\"milvus-s3-example\",  # Replace with your S3 bucket name\n    key=\"WhatIsMilvus.docx\",  # Replace with your document file name\n    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n)\n","documents = loader.load()\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Initialize a RecursiveCharacterTextSplitter for splitting text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n\n# Split the documents into chunks using the text_splitter\ndocs = text_splitter.split_documents(documents)\n\n# Let's take a look at the first document\ndocs[1]\n","from langchain_milvus import Milvus\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n\nvectorstore = Milvus.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    connection_args={\n        \"uri\": \"./milvus_demo.db\",\n    },\n    drop_old=False,  # Drop the old Milvus collection if it exists\n)\n","query = \"How can Milvus be deployed\"\nvectorstore.similarity_search(query, k=1)\n","from langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the OpenAI language model for response generation\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# Define the prompt template for generating AI responses\nPROMPT_TEMPLATE = \"\"\"\nHuman: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\nUse the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n<context>\n{context}\n</context>\n\n<question>\n{question}\n</question>\n\nThe response should be specific and use statistics or numbers when possible.\n\nAssistant:\"\"\"\n\n# Create a PromptTemplate instance with the defined template and input variables\nprompt = PromptTemplate(\n    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n)\n# Convert the vector store to a retriever\nretriever = vectorstore.as_retriever()\n\n\n# Define a function to format the retrieved documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n\nres = rag_chain.invoke(query)\nres\n","\n"],"headingContent":"Building a RAG Pipeline: Loading Data from S3 into Milvus","anchorList":[{"label":"Creaci贸n de una canalizaci贸n RAG: Carga de datos de S3 en Milvus","href":"Building-a-RAG-Pipeline-Loading-Data-from-S3-into-Milvus","type":1,"isActive":false},{"label":"Preparaci贸n","href":"Preparation","type":2,"isActive":false},{"label":"Configuraci贸n de S3","href":"S3-Configuration","type":2,"isActive":false},{"label":"Dividir documentos en trozos","href":"Split-Documents-into-Chunks","type":2,"isActive":false},{"label":"Construir la cadena RAG con Milvus Vector Store","href":"Build-RAG-chain-with-Milvus-Vector-Store","type":2,"isActive":false}]}