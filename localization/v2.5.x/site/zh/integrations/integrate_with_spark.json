{"codeList":["wget https://github.com/zilliztech/spark-milvus/raw/1.0.0-SNAPSHOT/output/spark-milvus-1.0.0-SNAPSHOT.jar\n","./bin/pyspark --jars spark-milvus-1.0.0-SNAPSHOT.jar\n","./bin/spark-shell --jars spark-milvus-1.0.0-SNAPSHOT.jar\n","from pyspark.sql import SparkSession\n\ncolumns = [\"id\", \"text\", \"vec\"]\ndata = [(1, \"a\", [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0]),\n    (2, \"b\", [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0]),\n    (3, \"c\", [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0]),\n    (4, \"d\", [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])]\nsample_df = spark.sparkContext.parallelize(data).toDF(columns)\nsample_df.write \\\n    .mode(\"append\") \\\n    .option(\"milvus.host\", \"localhost\") \\\n    .option(\"milvus.port\", \"19530\") \\\n    .option(\"milvus.collection.name\", \"hello_spark_milvus\") \\\n    .option(\"milvus.collection.vectorField\", \"vec\") \\\n    .option(\"milvus.collection.vectorDim\", \"8\") \\\n    .option(\"milvus.collection.primaryKeyField\", \"id\") \\\n    .format(\"milvus\") \\\n    .save()\n","import org.apache.spark.sql.{SaveMode, SparkSession}\n\nobject Hello extends App {\n\n  val spark = SparkSession.builder().master(\"local[*]\")\n    .appName(\"HelloSparkMilvus\")\n    .getOrCreate()\n\n  import spark.implicits._\n\n  // Create DataFrame\n  val sampleDF = Seq(\n    (1, \"a\", Seq(1.0,2.0,3.0,4.0,5.0)),\n    (2, \"b\", Seq(1.0,2.0,3.0,4.0,5.0)),\n    (3, \"c\", Seq(1.0,2.0,3.0,4.0,5.0)),\n    (4, \"d\", Seq(1.0,2.0,3.0,4.0,5.0))\n  ).toDF(\"id\", \"text\", \"vec\")\n\n  // set milvus options\n  val milvusOptions = Map(\n      \"milvus.host\" -> \"localhost\" -> uri,\n      \"milvus.port\" -> \"19530\",\n      \"milvus.collection.name\" -> \"hello_spark_milvus\",\n      \"milvus.collection.vectorField\" -> \"vec\",\n      \"milvus.collection.vectorDim\" -> \"5\",\n      \"milvus.collection.primaryKeyField\", \"id\"\n    )\n    \n  sampleDF.write.format(\"milvus\")\n    .options(milvusOptions)\n    .mode(SaveMode.Append)\n    .save()\n}\n","val df = spark.read\n  .format(\"milvusbinlog\")\n  .load(path)\n  .withColumnRenamed(\"val\", \"embedding\")\n","{\"book_id\": 101, \"word_count\": 13, \"book_intro\": [1.1, 1.2]}\n{\"book_id\": 102, \"word_count\": 25, \"book_intro\": [2.1, 2.2]}\n{\"book_id\": 103, \"word_count\": 7, \"book_intro\": [3.1, 3.2]}\n{\"book_id\": 104, \"word_count\": 12, \"book_intro\": [4.1, 4.2]}\n{\"book_id\": 105, \"word_count\": 34, \"book_intro\": [5.1, 5.2]}\n","{\n    \"rows\":[\n        {\"book_id\": 101, \"word_count\": 13, \"book_intro\": [1.1, 1.2]},\n        {\"book_id\": 102, \"word_count\": 25, \"book_intro\": [2.1, 2.2]},\n        {\"book_id\": 103, \"word_count\": 7, \"book_intro\": [3.1, 3.2]},\n        {\"book_id\": 104, \"word_count\": 12, \"book_intro\": [4.1, 4.2]},\n        {\"book_id\": 105, \"word_count\": 34, \"book_intro\": [5.1, 5.2]}\n    ]\n}\n","val collectionDF = MilvusUtils.readMilvusCollection(spark, milvusOptions)\n","df.write.format(\"parquet\").save(outputPath)\nMilvusUtils.bulkInsertFromSpark(spark, milvusOptions, outputPath, \"parquet\")\n","spark-shell --jars spark-milvus-1.0.0-SNAPSHOT.jar,mysql-connector-j-x.x.x.jar\n","import org.apache.spark.ml.feature.{Tokenizer, Word2Vec}\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.{SaveMode, SparkSession}\nimport zilliztech.spark.milvus.MilvusOptions._\n\nimport org.apache.spark.ml.linalg.Vector\n\nobject Mysql2MilvusDemo  extends App {\n\n  val spark = SparkSession.builder().master(\"local[*]\")\n    .appName(\"Mysql2MilvusDemo\")\n    .getOrCreate()\n\n  import spark.implicits._\n\n  // Create DataFrame\n  val sampleDF = Seq(\n    (1, \"Milvus was created in 2019 with a singular goal: store, index, and manage massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\"),\n    (2, \"As a database specifically designed to handle queries over input vectors, it is capable of indexing vectors on a trillion scale. \"),\n    (3, \"Unlike existing relational databases which mainly deal with structured data following a pre-defined pattern, Milvus is designed from the bottom-up to handle embedding vectors converted from unstructured data.\"),\n    (4, \"As the Internet grew and evolved, unstructured data became more and more common, including emails, papers, IoT sensor data, Facebook photos, protein structures, and much more.\")\n  ).toDF(\"id\", \"text\")\n\n  // Write to MySQL Table\n  sampleDF.write\n    .mode(SaveMode.Append)\n    .format(\"jdbc\")\n    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\")\n    .option(\"url\", \"jdbc:mysql://localhost:3306/test\")\n    .option(\"dbtable\", \"demo\")\n    .option(\"user\", \"root\")\n    .option(\"password\", \"123456\")\n    .save()\n\n  // Read from MySQL Table\n  val dfMysql = spark.read\n    .format(\"jdbc\")\n    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\")\n    .option(\"url\", \"jdbc:mysql://localhost:3306/test\")\n    .option(\"dbtable\", \"demo\")\n    .option(\"user\", \"root\")\n    .option(\"password\", \"123456\")\n    .load()\n\n  val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"tokens\")\n  val tokenizedDf = tokenizer.transform(dfMysql)\n\n  // Learn a mapping from words to Vectors.\n  val word2Vec = new Word2Vec()\n    .setInputCol(\"tokens\")\n    .setOutputCol(\"vectors\")\n    .setVectorSize(128)\n    .setMinCount(0)\n  val model = word2Vec.fit(tokenizedDf)\n\n  val result = model.transform(tokenizedDf)\n\n  val vectorToArrayUDF = udf((v: Vector) => v.toArray)\n  // Apply the UDF to the DataFrame\n  val resultDF = result.withColumn(\"embedding\", vectorToArrayUDF($\"vectors\"))\n  val milvusDf = resultDF.drop(\"tokens\").drop(\"vectors\")\n\n  milvusDf.write.format(\"milvus\")\n    .option(MILVUS_HOST, \"localhost\")\n    .option(MILVUS_PORT, \"19530\")\n    .option(MILVUS_COLLECTION_NAME, \"text_embedding\")\n    .option(MILVUS_COLLECTION_VECTOR_FIELD, \"embedding\")\n    .option(MILVUS_COLLECTION_VECTOR_DIM, \"128\")\n    .option(MILVUS_COLLECTION_PRIMARY_KEY, \"id\")\n    .mode(SaveMode.Append)\n    .save()\n}\n","import org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.util.CaseInsensitiveStringMap\nimport zilliztech.spark.milvus.{MilvusOptions, MilvusUtils}\n\nimport scala.collection.JavaConverters._\n\nobject TransformDemo extends App {\n  val sparkConf = new SparkConf().setMaster(\"local\")\n  val spark = SparkSession.builder().config(sparkConf).getOrCreate()\n\n  import spark.implicits._\n\n  val host = \"localhost\"\n  val port = 19530\n  val user = \"root\"\n  val password = \"Milvus\"\n  val fs = \"s3a://\"\n  val bucketName = \"a-bucket\"\n  val rootPath = \"files\"\n  val minioAK = \"minioadmin\"\n  val minioSK = \"minioadmin\"\n  val minioEndpoint = \"localhost:9000\"\n  val collectionName = \"hello_spark_milvus1\"\n  val targetCollectionName = \"hello_spark_milvus2\"\n\n  val properties = Map(\n    MilvusOptions.MILVUS_HOST -> host,\n    MilvusOptions.MILVUS_PORT -> port.toString,\n    MilvusOptions.MILVUS_COLLECTION_NAME -> collectionName,\n    MilvusOptions.MILVUS_BUCKET -> bucketName,\n    MilvusOptions.MILVUS_ROOTPATH -> rootPath,\n    MilvusOptions.MILVUS_FS -> fs,\n    MilvusOptions.MILVUS_STORAGE_ENDPOINT -> minioEndpoint,\n    MilvusOptions.MILVUS_STORAGE_USER -> minioAK,\n    MilvusOptions.MILVUS_STORAGE_PASSWORD -> minioSK,\n  )\n\n  // 1, configurations\n  val milvusOptions = new MilvusOptions(new CaseInsensitiveStringMap(properties.asJava))\n\n  // 2, batch read milvus collection data to dataframe\n  //  Schema: dim of `embeddings` is 8\n  // +-+------------+------------+------------------+\n  // | | field name | field type | other attributes |\n  // +-+------------+------------+------------------+\n  // |1|    \"pk\"    |    Int64   |  is_primary=True |\n  // | |            |            |   auto_id=False  |\n  // +-+------------+------------+------------------+\n  // |2|  \"random\"  |    Double  |                  |\n  // +-+------------+------------+------------------+\n  // |3|\"embeddings\"| FloatVector|     dim=8        |\n  // +-+------------+------------+------------------+\n  val arrayToVectorUDF = udf((arr: Seq[Double]) => Vectors.dense(arr.toArray[Double]))\n  val collectionDF = MilvusUtils.readMilvusCollection(spark, milvusOptions)\n    .withColumn(\"embeddings_vec\", arrayToVectorUDF($\"embeddings\"))\n    .drop(\"embeddings\")\n  \n  // 3. Use PCA to reduce dim of vector\n  val dim = 4\n  val pca = new PCA()\n    .setInputCol(\"embeddings_vec\")\n    .setOutputCol(\"pca_vec\")\n    .setK(dim)\n    .fit(collectionDF)\n  val vectorToArrayUDF = udf((v: Vector) => v.toArray)\n  // embeddings dim number reduce to 4\n  // +-+------------+------------+------------------+\n  // | | field name | field type | other attributes |\n  // +-+------------+------------+------------------+\n  // |1|    \"pk\"    |    Int64   |  is_primary=True |\n  // | |            |            |   auto_id=False  |\n  // +-+------------+------------+------------------+\n  // |2|  \"random\"  |    Double  |                  |\n  // +-+------------+------------+------------------+\n  // |3|\"embeddings\"| FloatVector|     dim=4        |\n  // +-+------------+------------+------------------+\n  val pcaDf = pca.transform(collectionDF)\n    .withColumn(\"embeddings\", vectorToArrayUDF($\"pca_vec\"))\n    .select(\"pk\", \"random\", \"embeddings\")\n\n  // 4. Write PCAed data to S3\n  val outputPath = \"s3a://a-bucket/result\"\n  pcaDf.write\n    .mode(\"overwrite\")\n    .format(\"parquet\")\n    .save(outputPath)\n\n  // 5. Config MilvusOptions of target table  \n  val targetProperties = Map(\n    MilvusOptions.MILVUS_HOST -> host,\n    MilvusOptions.MILVUS_PORT -> port.toString,\n    MilvusOptions.MILVUS_COLLECTION_NAME -> targetCollectionName,\n    MilvusOptions.MILVUS_BUCKET -> bucketName,\n    MilvusOptions.MILVUS_ROOTPATH -> rootPath,\n    MilvusOptions.MILVUS_FS -> fs,\n    MilvusOptions.MILVUS_STORAGE_ENDPOINT -> minioEndpoint,\n    MilvusOptions.MILVUS_STORAGE_USER -> minioAK,\n    MilvusOptions.MILVUS_STORAGE_PASSWORD -> minioSK,\n  )\n  val targetMilvusOptions = new MilvusOptions(new CaseInsensitiveStringMap(targetProperties.asJava))\n  \n  // 6. Bulkinsert Spark output files into milvus\n  MilvusUtils.bulkInsertFromSpark(spark, targetMilvusOptions, outputPath, \"parquet\")\n}\n","// Write the data in batch into the Milvus bucket storage.\nval outputPath = \"s3://my-temp-bucket/result\"\ndf.write\n  .mode(\"overwrite\")\n  .format(\"mjson\")\n  .save(outputPath)\n// Specify Milvus options.\nval targetProperties = Map(\n  MilvusOptions.MILVUS_URI -> zilliz_uri,\n  MilvusOptions.MILVUS_TOKEN -> zilliz_token,\n  MilvusOptions.MILVUS_COLLECTION_NAME -> targetCollectionName,\n  MilvusOptions.MILVUS_BUCKET -> bucketName,\n  MilvusOptions.MILVUS_ROOTPATH -> rootPath,\n  MilvusOptions.MILVUS_FS -> fs,\n  MilvusOptions.MILVUS_STORAGE_ENDPOINT -> minioEndpoint,\n  MilvusOptions.MILVUS_STORAGE_USER -> minioAK,\n  MilvusOptions.MILVUS_STORAGE_PASSWORD -> minioSK,\n)\nval targetMilvusOptions = new MilvusOptions(new CaseInsensitiveStringMap(targetProperties.asJava))\n  \n// Bulk insert Spark output files into Milvus\nMilvusUtils.bulkInsertFromSpark(spark, targetMilvusOptions, outputPath, \"mjson\")\n"],"headingContent":"Spark-Milvus Connector User Guide","anchorList":[{"label":"Spark-Milvus Connector User Guide","href":"Spark-Milvus-Connector-User-Guide","type":1,"isActive":false},{"label":"Quick start","href":"Quick-start","type":2,"isActive":false},{"label":"Features & concepts","href":"Features--concepts","type":2,"isActive":false},{"label":"Milvus data format","href":"Milvus-data-format","type":2,"isActive":false},{"label":"MilvusUtils","href":"MilvusUtils","type":2,"isActive":false},{"label":"Advanced Usage","href":"Advanced-Usage","type":2,"isActive":false},{"label":"Hands-on","href":"Hands-on","type":2,"isActive":false}]}