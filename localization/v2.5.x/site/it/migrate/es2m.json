{"codeList":["dumper: # configs for the migration job.\n  worker:\n    workMode: \"elasticsearch\" # operational mode of the migration job.\n    reader:\n      bufferSize: 2500 # buffer size to read from Elasticsearch in each batch. A value ranging from 2000 to 4000 is recommended.\nmeta: # meta configs for the source Elasticsearch index and target Milvus 2.x collection.\n  mode: \"config\" # specifies the source for meta configs. currently, onlly `config` is supported.\n  version: \"8.9.1\"\n  index: \"qatest_index\" # identifies the Elasticsearch index to migrate data from.\n  fields: # fields within the Elasticsearch index to be migrated.\n  - name: \"my_vector\" # name of the Elasticsearch field.\n    type: \"dense_vector\" # data type of the Elasticsearch field.\n    dims: 128 # dimension of the vector field. required only when `type` is `dense_vector`.\n  - name: \"id\"\n    pk: true # specifies if the field serves as a primary key.\n    type: \"long\"\n  - name: \"num\"\n    type: \"integer\"\n  - name: \"double1\"\n    type: \"double\"\n  - name: \"text1\"\n    maxLen: 1000 # max. length of data fields. required only for `keyword` and `text` data types.\n    type: \"text\"\n  - name: \"bl1\"\n    type: \"boolean\"\n  - name: \"float1\"\n    type: \"float\"\n  milvus: # configs specific to creating the collection in Milvus 2.x\n    collection: \"Collection_01\" # name of the Milvus collection. defaults to the Elasticsearch index name if not specified.\n    closeDynamicField: false # specifies whether to disable the dynamic field in the collection. defaults to `false`.\n    shardNum: 2 # number of shards to be created in the collection.\n    consistencyLevel: Strong # consistency level for Milvus collection.\nsource: # connection configs for the source Elasticsearch server\n  es:\n    urls:\n    - \"http://10.15.1.***:9200\" # address of the source Elasticsearch server.\n    username: \"\" # username for the Elasticsearch server.\n    password: \"\" # password for the Elasticsearch server.\ntarget:\n  mode: \"remote\" # storage location for dumped files. valid values: `remote` and `local`.\n  remote: # configs for remote storage\n    outputDir: \"migration/milvus/test\" # output directory path in the cloud storage bucket.\n    cloud: \"aws\" # cloud storage service provider. Examples: `aws`, `gcp`, `azure`, etc.\n    region: \"us-west-2\" # region of the cloud storage; can be any value if using local Minio.\n    bucket: \"zilliz-aws-us-****-*-********\" # bucket name for storing data; must align with configs in milvus.yaml for Milvus 2.x.\n    useIAM: true # whether to use an IAM Role for connection.\n    checkBucket: false # checks if the specified bucket exists in the storage.\n  milvus2x: # connection configs for the target Milvus 2.x server\n    endpoint: \"http://10.102.*.**:19530\" # address of the target Milvus server.\n    username: \"****\" # username for the Milvus 2.x server.\n    password: \"******\" # password for the Milvus 2.x server.\n","./milvus-migration start --config=/{YourConfigFilePath}/migration.yaml\n","[task/load_base_task.go:94] [\"[LoadTasker] Dec Task Processing-------------->\"] [Count=0] [fileName=testfiles/output/zwh/migration/test_mul_field4/data_1_1.json] [taskId=442665677354739304]\n[task/load_base_task.go:76] [\"[LoadTasker] Progress Task --------------->\"] [fileName=testfiles/output/zwh/migration/test_mul_field4/data_1_1.json] [taskId=442665677354739304]\n[dbclient/cus_field_milvus2x.go:86] [\"[Milvus2x] begin to ShowCollectionRows\"]\n[loader/cus_milvus2x_loader.go:66] [\"[Loader] Static: \"] [collection=test_mul_field4_rename1] [beforeCount=50000] [afterCount=100000] [increase=50000]\n[loader/cus_milvus2x_loader.go:66] [\"[Loader] Static Total\"] [\"Total Collections\"=1] [beforeTotalCount=50000] [afterTotalCount=100000] [totalIncrease=50000]\n[migration/es_starter.go:25] [\"[Starter] migration ES to Milvus finish!!!\"] [Cost=80.009174459]\n[starter/starter.go:106] [\"[Starter] Migration Success!\"] [Cost=80.00928425]\n[cleaner/remote_cleaner.go:27] [\"[Remote Cleaner] Begin to clean files\"] [bucket=a-bucket] [rootPath=testfiles/output/zwh/migration]\n[cmd/start.go:32] [\"[Cleaner] clean file success!\"]\n"],"headingContent":"From Elasticsearch","anchorList":[{"label":"From Elasticsearch","href":"From-Elasticsearch","type":1,"isActive":false},{"label":"Prerequisites","href":"Prerequisites","type":2,"isActive":false},{"label":"Configure the migration file","href":"Configure-the-migration-file","type":2,"isActive":false},{"label":"Start the migration task","href":"Start-the-migration-task","type":2,"isActive":false},{"label":"Verify the result","href":"Verify-the-result","type":2,"isActive":false},{"label":"Field mapping reference","href":"Field-mapping-reference","type":2,"isActive":false}]}