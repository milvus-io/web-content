{"codeList":["$ pip install --upgrade pymilvus openai datasets opencv-python timm einops ftfy peft tqdm\n","$ git clone https://github.com/FlagOpen/FlagEmbedding.git\n$ pip install -e FlagEmbedding\n","$ wget https://github.com/milvus-io/bootcamp/releases/download/data/amazon_reviews_2023_subset.tar.gz\n$ tar -xzf amazon_reviews_2023_subset.tar.gz\n","$ wget https://huggingface.co/BAAI/bge-visualized/resolve/main/Visualized_base_en_v1.5.pth\n","import torch\nfrom FlagEmbedding.visual.modeling import Visualized_BGE\n\n\nclass Encoder:\n    def __init__(self, model_name: str, model_path: str):\n        self.model = Visualized_BGE(model_name_bge=model_name, model_weight=model_path)\n        self.model.eval()\n\n    def encode_query(self, image_path: str, text: str) -> list[float]:\n        with torch.no_grad():\n            query_emb = self.model.encode(image=image_path, text=text)\n        return query_emb.tolist()[0]\n\n    def encode_image(self, image_path: str) -> list[float]:\n        with torch.no_grad():\n            query_emb = self.model.encode(image=image_path)\n        return query_emb.tolist()[0]\n\n\nmodel_name = \"BAAI/bge-base-en-v1.5\"\nmodel_path = \"./Visualized_base_en_v1.5.pth\"  # Change to your own value if using a different model path\nencoder = Encoder(model_name, model_path)\n","import os\nfrom tqdm import tqdm\nfrom glob import glob\n\n\n# Generate embeddings for the image dataset\ndata_dir = (\n    \"./images_folder\"  # Change to your own value if using a different data directory\n)\nimage_list = glob(\n    os.path.join(data_dir, \"images\", \"*.jpg\")\n)  # We will only use images ending with \".jpg\"\nimage_dict = {}\nfor image_path in tqdm(image_list, desc=\"Generating image embeddings: \"):\n    try:\n        image_dict[image_path] = encoder.encode_image(image_path)\n    except Exception as e:\n        print(f\"Failed to generate embedding for {image_path}. Skipped.\")\n        continue\nprint(\"Number of encoded images:\", len(image_dict))\n","from pymilvus import MilvusClient\n\n\ndim = len(list(image_dict.values())[0])\ncollection_name = \"multimodal_rag_demo\"\n\n# Connect to Milvus client given URI\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\n# Create Milvus Collection\n# By default, vector field name is \"vector\"\nmilvus_client.create_collection(\n    collection_name=collection_name,\n    auto_id=True,\n    dimension=dim,\n    enable_dynamic_field=True,\n)\n\n# Insert data into collection\nmilvus_client.insert(\n    collection_name=collection_name,\n    data=[{\"image_path\": k, \"vector\": v} for k, v in image_dict.items()],\n)\n","query_image = os.path.join(\n    data_dir, \"leopard.jpg\"\n)  # Change to your own query image path\nquery_text = \"phone case with this image theme\"\n\n# Generate query embedding given image and text instructions\nquery_vec = encoder.encode_query(image_path=query_image, text=query_text)\n\nsearch_results = milvus_client.search(\n    collection_name=collection_name,\n    data=[query_vec],\n    output_fields=[\"image_path\"],\n    limit=9,  # Max number of search results to return\n    search_params={\"metric_type\": \"COSINE\", \"params\": {}},  # Search parameters\n)[0]\n\nretrieved_images = [hit.get(\"entity\").get(\"image_path\") for hit in search_results]\nprint(retrieved_images)\n","import numpy as np\nimport cv2\n\nimg_height = 300\nimg_width = 300\nrow_count = 3\n\n\ndef create_panoramic_view(query_image_path: str, retrieved_images: list) -> np.ndarray:\n    \"\"\"\n    creates a 5x5 panoramic view image from a list of images\n\n    args:\n        images: list of images to be combined\n\n    returns:\n        np.ndarray: the panoramic view image\n    \"\"\"\n    panoramic_width = img_width * row_count\n    panoramic_height = img_height * row_count\n    panoramic_image = np.full(\n        (panoramic_height, panoramic_width, 3), 255, dtype=np.uint8\n    )\n\n    # create and resize the query image with a blue border\n    query_image_null = np.full((panoramic_height, img_width, 3), 255, dtype=np.uint8)\n    query_image = Image.open(query_image_path).convert(\"RGB\")\n    query_array = np.array(query_image)[:, :, ::-1]\n    resized_image = cv2.resize(query_array, (img_width, img_height))\n\n    border_size = 10\n    blue = (255, 0, 0)  # blue color in BGR\n    bordered_query_image = cv2.copyMakeBorder(\n        resized_image,\n        border_size,\n        border_size,\n        border_size,\n        border_size,\n        cv2.BORDER_CONSTANT,\n        value=blue,\n    )\n\n    query_image_null[img_height * 2 : img_height * 3, 0:img_width] = cv2.resize(\n        bordered_query_image, (img_width, img_height)\n    )\n\n    # add text \"query\" below the query image\n    text = \"query\"\n    font_scale = 1\n    font_thickness = 2\n    text_org = (10, img_height * 3 + 30)\n    cv2.putText(\n        query_image_null,\n        text,\n        text_org,\n        cv2.FONT_HERSHEY_SIMPLEX,\n        font_scale,\n        blue,\n        font_thickness,\n        cv2.LINE_AA,\n    )\n\n    # combine the rest of the images into the panoramic view\n    retrieved_imgs = [\n        np.array(Image.open(img).convert(\"RGB\"))[:, :, ::-1] for img in retrieved_images\n    ]\n    for i, image in enumerate(retrieved_imgs):\n        image = cv2.resize(image, (img_width - 4, img_height - 4))\n        row = i // row_count\n        col = i % row_count\n        start_row = row * img_height\n        start_col = col * img_width\n\n        border_size = 2\n        bordered_image = cv2.copyMakeBorder(\n            image,\n            border_size,\n            border_size,\n            border_size,\n            border_size,\n            cv2.BORDER_CONSTANT,\n            value=(0, 0, 0),\n        )\n        panoramic_image[\n            start_row : start_row + img_height, start_col : start_col + img_width\n        ] = bordered_image\n\n        # add red index numbers to each image\n        text = str(i)\n        org = (start_col + 50, start_row + 30)\n        (font_width, font_height), baseline = cv2.getTextSize(\n            text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2\n        )\n\n        top_left = (org[0] - 48, start_row + 2)\n        bottom_right = (org[0] - 48 + font_width + 5, org[1] + baseline + 5)\n\n        cv2.rectangle(\n            panoramic_image, top_left, bottom_right, (255, 255, 255), cv2.FILLED\n        )\n        cv2.putText(\n            panoramic_image,\n            text,\n            (start_col + 10, start_row + 30),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            1,\n            (0, 0, 255),\n            2,\n            cv2.LINE_AA,\n        )\n\n    # combine the query image with the panoramic view\n    panoramic_image = np.hstack([query_image_null, panoramic_image])\n    return panoramic_image\n","from PIL import Image\n\ncombined_image_path = os.path.join(data_dir, \"combined_image.jpg\")\npanoramic_image = create_panoramic_view(query_image, retrieved_images)\ncv2.imwrite(combined_image_path, panoramic_image)\n\ncombined_image = Image.open(combined_image_path)\nshow_combined_image = combined_image.resize((300, 300))\nshow_combined_image.show()\n","import requests\nimport base64\n\nopenai_api_key = \"sk-***\"  # Change to your OpenAI API Key\n\n\ndef generate_ranking_explanation(\n    combined_image_path: str, caption: str, infos: dict = None\n) -> tuple[list[int], str]:\n    with open(combined_image_path, \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    information = (\n        \"You are responsible for ranking results for a Composed Image Retrieval. \"\n        \"The user retrieves an image with an 'instruction' indicating their retrieval intent. \"\n        \"For example, if the user queries a red car with the instruction 'change this car to blue,' a similar type of car in blue would be ranked higher in the results. \"\n        \"Now you would receive instruction and query image with blue border. Every item has its red index number in its top left. Do not misunderstand it. \"\n        f\"User instruction: {caption} \\n\\n\"\n    )\n\n    # add additional information for each image\n    if infos:\n        for i, info in enumerate(infos[\"product\"]):\n            information += f\"{i}. {info}\\n\"\n\n    information += (\n        \"Provide a new ranked list of indices from most suitable to least suitable, followed by an explanation for the top 1 most suitable item only. \"\n        \"The format of the response has to be 'Ranked list: []' with the indices in brackets as integers, followed by 'Reasons:' plus the explanation why this most fit user's query intent.\"\n    )\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {openai_api_key}\",\n    }\n\n    payload = {\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": information},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n                    },\n                ],\n            }\n        ],\n        \"max_tokens\": 300,\n    }\n\n    response = requests.post(\n        \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload\n    )\n    result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n\n    # parse the ranked indices from the response\n    start_idx = result.find(\"[\")\n    end_idx = result.find(\"]\")\n    ranked_indices_str = result[start_idx + 1 : end_idx].split(\",\")\n    ranked_indices = [int(index.strip()) for index in ranked_indices_str]\n\n    # extract explanation\n    explanation = result[end_idx + 1 :].strip()\n\n    return ranked_indices, explanation\n","ranked_indices, explanation = generate_ranking_explanation(\n    combined_image_path, query_text\n)\n","print(explanation)\n\nbest_index = ranked_indices[0]\nbest_img = Image.open(retrieved_images[best_index])\nbest_img = best_img.resize((150, 150))\nbest_img.show()\n"],"headingContent":"Multimodal RAG with Milvus","anchorList":[{"label":"Multimodal RAG with Milvus","href":"Multimodal-RAG-with-Milvus","type":1,"isActive":false},{"label":"Preparation","href":"Preparation","type":2,"isActive":false},{"label":"Load Data","href":"Load-Data","type":2,"isActive":false},{"label":"Multimodal Search with Generative Reranker","href":"Multimodal-Search-with-Generative-Reranker","type":2,"isActive":false}]}