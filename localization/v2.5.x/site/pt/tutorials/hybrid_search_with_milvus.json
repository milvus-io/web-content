{"codeList":["$ pip install --upgrade pymilvus \"pymilvus[model]\"\n","# Run this cell to download the dataset\n$ wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n","import pandas as pd\n\nfile_path = \"quora_duplicate_questions.tsv\"\ndf = pd.read_csv(file_path, sep=\"\\t\")\nquestions = set()\nfor _, row in df.iterrows():\n    obj = row.to_dict()\n    questions.add(obj[\"question1\"][:512])\n    questions.add(obj[\"question2\"][:512])\n    if len(questions) > 500:  # Skip this if you want to use the full dataset\n        break\n\ndocs = list(questions)\n\n# example question\nprint(docs[0])\n","from milvus_model.hybrid import BGEM3EmbeddingFunction\n\nef = BGEM3EmbeddingFunction(use_fp16=False, device=\"cpu\")\ndense_dim = ef.dim[\"dense\"]\n\n# Generate embeddings using BGE-M3 model\ndocs_embeddings = ef(docs)\n","from pymilvus import (\n    connections,\n    utility,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    Collection,\n)\n\n# Connect to Milvus given URI\nconnections.connect(uri=\"./milvus.db\")\n\n# Specify the data schema for the new Collection\nfields = [\n    # Use auto generated id as primary key\n    FieldSchema(\n        name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100\n    ),\n    # Store the original text to retrieve based on semantically distance\n    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=512),\n    # Milvus now supports both sparse and dense vectors,\n    # we can store each in a separate field to conduct hybrid search on both vectors\n    FieldSchema(name=\"sparse_vector\", dtype=DataType.SPARSE_FLOAT_VECTOR),\n    FieldSchema(name=\"dense_vector\", dtype=DataType.FLOAT_VECTOR, dim=dense_dim),\n]\nschema = CollectionSchema(fields)\n\n# Create collection (drop the old one if exists)\ncol_name = \"hybrid_demo\"\nif utility.has_collection(col_name):\n    Collection(col_name).drop()\ncol = Collection(col_name, schema, consistency_level=\"Strong\")\n\n# To make vector search efficient, we need to create indices for the vector fields\nsparse_index = {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}\ncol.create_index(\"sparse_vector\", sparse_index)\ndense_index = {\"index_type\": \"AUTOINDEX\", \"metric_type\": \"IP\"}\ncol.create_index(\"dense_vector\", dense_index)\ncol.load()\n","# For efficiency, we insert 50 records in each small batch\nfor i in range(0, len(docs), 50):\n    batched_entities = [\n        docs[i : i + 50],\n        docs_embeddings[\"sparse\"][i : i + 50],\n        docs_embeddings[\"dense\"][i : i + 50],\n    ]\n    col.insert(batched_entities)\nprint(\"Number of entities inserted:\", col.num_entities)\n","# Enter your search query\nquery = input(\"Enter your search query: \")\nprint(query)\n\n# Generate embeddings for the query\nquery_embeddings = ef([query])\n# print(query_embeddings)\n","from pymilvus import (\n    AnnSearchRequest,\n    WeightedRanker,\n)\n\n\ndef dense_search(col, query_dense_embedding, limit=10):\n    search_params = {\"metric_type\": \"IP\", \"params\": {}}\n    res = col.search(\n        [query_dense_embedding],\n        anns_field=\"dense_vector\",\n        limit=limit,\n        output_fields=[\"text\"],\n        param=search_params,\n    )[0]\n    return [hit.get(\"text\") for hit in res]\n\n\ndef sparse_search(col, query_sparse_embedding, limit=10):\n    search_params = {\n        \"metric_type\": \"IP\",\n        \"params\": {},\n    }\n    res = col.search(\n        [query_sparse_embedding],\n        anns_field=\"sparse_vector\",\n        limit=limit,\n        output_fields=[\"text\"],\n        param=search_params,\n    )[0]\n    return [hit.get(\"text\") for hit in res]\n\n\ndef hybrid_search(\n    col,\n    query_dense_embedding,\n    query_sparse_embedding,\n    sparse_weight=1.0,\n    dense_weight=1.0,\n    limit=10,\n):\n    dense_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n    dense_req = AnnSearchRequest(\n        [query_dense_embedding], \"dense_vector\", dense_search_params, limit=limit\n    )\n    sparse_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n    sparse_req = AnnSearchRequest(\n        [query_sparse_embedding], \"sparse_vector\", sparse_search_params, limit=limit\n    )\n    rerank = WeightedRanker(sparse_weight, dense_weight)\n    res = col.hybrid_search(\n        [sparse_req, dense_req], rerank=rerank, limit=limit, output_fields=[\"text\"]\n    )[0]\n    return [hit.get(\"text\") for hit in res]\n","dense_results = dense_search(col, query_embeddings[\"dense\"][0])\nsparse_results = sparse_search(col, query_embeddings[\"sparse\"]._getrow(0))\nhybrid_results = hybrid_search(\n    col,\n    query_embeddings[\"dense\"][0],\n    query_embeddings[\"sparse\"]._getrow(0),\n    sparse_weight=0.7,\n    dense_weight=1.0,\n)\n","def doc_text_formatting(ef, query, docs):\n    tokenizer = ef.model.tokenizer\n    query_tokens_ids = tokenizer.encode(query, return_offsets_mapping=True)\n    query_tokens = tokenizer.convert_ids_to_tokens(query_tokens_ids)\n    formatted_texts = []\n\n    for doc in docs:\n        ldx = 0\n        landmarks = []\n        encoding = tokenizer.encode_plus(doc, return_offsets_mapping=True)\n        tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])[1:-1]\n        offsets = encoding[\"offset_mapping\"][1:-1]\n        for token, (start, end) in zip(tokens, offsets):\n            if token in query_tokens:\n                if len(landmarks) != 0 and start == landmarks[-1]:\n                    landmarks[-1] = end\n                else:\n                    landmarks.append(start)\n                    landmarks.append(end)\n        close = False\n        formatted_text = \"\"\n        for i, c in enumerate(doc):\n            if ldx == len(landmarks):\n                pass\n            elif i == landmarks[ldx]:\n                if close:\n                    formatted_text += \"</span>\"\n                else:\n                    formatted_text += \"<span style='color:red'>\"\n                close = not close\n                ldx = ldx + 1\n            formatted_text += c\n        if close is True:\n            formatted_text += \"</span>\"\n        formatted_texts.append(formatted_text)\n    return formatted_texts\n","from IPython.display import Markdown, display\n\n# Dense search results\ndisplay(Markdown(\"**Dense Search Results:**\"))\nformatted_results = doc_text_formatting(ef, query, dense_results)\nfor result in dense_results:\n    display(Markdown(result))\n\n# Sparse search results\ndisplay(Markdown(\"\\n**Sparse Search Results:**\"))\nformatted_results = doc_text_formatting(ef, query, sparse_results)\nfor result in formatted_results:\n    display(Markdown(result))\n\n# Hybrid search results\ndisplay(Markdown(\"\\n**Hybrid Search Results:**\"))\nformatted_results = doc_text_formatting(ef, query, hybrid_results)\nfor result in formatted_results:\n    display(Markdown(result))\n"],"headingContent":"Hybrid Search with Milvus","anchorList":[{"label":"Hybrid Search with Milvus","href":"Hybrid-Search-with-Milvus","type":1,"isActive":false}]}