{"codeList":["pip install \"pymilvus[model]\"\n","from pymilvus import model\n\n# This will download \"all-MiniLM-L6-v2\", a light weight model.\nef = model.DefaultEmbeddingFunction()\n\n# Data from which embeddings are to be generated \ndocs = [\n    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n    \"Alan Turing was the first person to conduct substantial research in AI.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n]\n\nembeddings = ef.encode_documents(docs)\n\n# Print embeddings\nprint(\"Embeddings:\", embeddings)\n# Print dimension and shape of embeddings\nprint(\"Dim:\", ef.dim, embeddings[0].shape)\n","Embeddings: [array([-3.09392996e-02, -1.80662833e-02,  1.34775648e-02,  2.77156215e-02,\n       -4.86349640e-03, -3.12581174e-02, -3.55921760e-02,  5.76934684e-03,\n        2.80773244e-03,  1.35783911e-01,  3.59678417e-02,  6.17732145e-02,\n...\n       -4.61330153e-02, -4.85207550e-02,  3.13997865e-02,  7.82178566e-02,\n       -4.75336798e-02,  5.21207601e-02,  9.04406682e-02, -5.36676683e-02],\n      dtype=float32)]\nDim: 384 (384,)\n","from pymilvus.model.hybrid import BGEM3EmbeddingFunction\nfrom pymilvus import (\n    utility,\n    FieldSchema, CollectionSchema, DataType,\n    Collection, AnnSearchRequest, RRFRanker, connections,\n)\n","# 1. prepare a small corpus to search\ndocs = [\n    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n    \"Alan Turing was the first person to conduct substantial research in AI.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n]\nquery = \"Who started AI research?\"\n\n# BGE-M3 model can embed texts as dense and sparse vectors.\n# It is included in the optional `model` module in pymilvus, to install it,\n# simply run \"pip install pymilvus[model]\".\n\nbge_m3_ef = BGEM3EmbeddingFunction(use_fp16=False, device=\"cpu\")\n\ndocs_embeddings = bge_m3_ef(docs)\nquery_embeddings = bge_m3_ef([query])\n","from pymilvus.model.sparse import BM25EmbeddingFunction\n","# 1. prepare a small corpus to search\ndocs = [\n    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n    \"Alan Turing was the first person to conduct substantial research in AI.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n]\nquery = \"Where was Turing born?\"\nbm25_ef = BM25EmbeddingFunction()\n\n# 2. fit the corpus to get BM25 model parameters on your documents.\nbm25_ef.fit(docs)\n\n# 3. store the fitted parameters to disk to expedite future processing.\nbm25_ef.save(\"bm25_params.json\")\n\n# 4. load the saved params\nnew_bm25_ef = BM25EmbeddingFunction()\nnew_bm25_ef.load(\"bm25_params.json\")\n\ndocs_embeddings = new_bm25_ef.encode_documents(docs)\nquery_embeddings = new_bm25_ef.encode_queries([query])\nprint(\"Dim:\", new_bm25_ef.dim, list(docs_embeddings)[0].shape)\n","Dim: 21 (1, 21)\n"],"headingContent":"Embedding Overview","anchorList":[{"label":"Embedding Overview","href":"Embedding-Overview","type":1,"isActive":false},{"label":"Example 1: Use default embedding function to generate dense vectors","href":"Example-1-Use-default-embedding-function-to-generate-dense-vectors","type":2,"isActive":false},{"label":"Example 2: Generate dense and sparse vectors in one call with BGE M3 model","href":"Example-2-Generate-dense-and-sparse-vectors-in-one-call-with-BGE-M3-model","type":2,"isActive":false},{"label":"Example 3: Generate  sparse vectors using BM25 model","href":"Example-3-Generate--sparse-vectors-using-BM25-model","type":2,"isActive":false}]}