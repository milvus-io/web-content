{"codeList":["$ pip install --upgrade pymilvus openai requests tqdm pandas \"arize-phoenix>=4.29.0\" nest_asyncio\n","import os\n\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-*****************\"\n","from typing import List\nfrom tqdm import tqdm\nfrom openai import OpenAI\nfrom pymilvus import MilvusClient\n\n\nclass RAG:\n    \"\"\"\n    RAG(Retrieval-Augmented Generation) class built upon OpenAI and Milvus.\n    \"\"\"\n\n    def __init__(self, openai_client: OpenAI, milvus_client: MilvusClient):\n        self._prepare_openai(openai_client)\n        self._prepare_milvus(milvus_client)\n\n    def _emb_text(self, text: str) -> List[float]:\n        return (\n            self.openai_client.embeddings.create(input=text, model=self.embedding_model)\n            .data[0]\n            .embedding\n        )\n\n    def _prepare_openai(\n        self,\n        openai_client: OpenAI,\n        embedding_model: str = \"text-embedding-3-small\",\n        llm_model: str = \"gpt-4o-mini\",\n    ):\n        self.openai_client = openai_client\n        self.embedding_model = embedding_model\n        self.llm_model = llm_model\n        self.SYSTEM_PROMPT = \"\"\"\n            Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n        \"\"\"\n        self.USER_PROMPT = \"\"\"\n            Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n            <context>\n            {context}\n            </context>\n            <question>\n            {question}\n            </question>\n        \"\"\"\n\n    def _prepare_milvus(\n        self, milvus_client: MilvusClient, collection_name: str = \"rag_collection\"\n    ):\n        self.milvus_client = milvus_client\n        self.collection_name = collection_name\n        if self.milvus_client.has_collection(self.collection_name):\n            self.milvus_client.drop_collection(self.collection_name)\n        embedding_dim = len(self._emb_text(\"demo\"))\n        self.milvus_client.create_collection(\n            collection_name=self.collection_name,\n            dimension=embedding_dim,\n            metric_type=\"IP\",\n            consistency_level=\"Strong\",\n        )\n\n    def load(self, texts: List[str]):\n        \"\"\"\n        Load the text data into Milvus.\n        \"\"\"\n        data = []\n        for i, line in enumerate(tqdm(texts, desc=\"Creating embeddings\")):\n            data.append({\"id\": i, \"vector\": self._emb_text(line), \"text\": line})\n        self.milvus_client.insert(collection_name=self.collection_name, data=data)\n\n    def retrieve(self, question: str, top_k: int = 3) -> List[str]:\n        \"\"\"\n        Retrieve the most similar text data to the given question.\n        \"\"\"\n        search_res = self.milvus_client.search(\n            collection_name=self.collection_name,\n            data=[self._emb_text(question)],\n            limit=top_k,\n            search_params={\"metric_type\": \"IP\", \"params\": {}},  # inner product distance\n            output_fields=[\"text\"],  # Return the text field\n        )\n        retrieved_texts = [res[\"entity\"][\"text\"] for res in search_res[0]]\n        return retrieved_texts[:top_k]\n\n    def answer(\n        self,\n        question: str,\n        retrieval_top_k: int = 3,\n        return_retrieved_text: bool = False,\n    ):\n        \"\"\"\n        Answer the given question with the retrieved knowledge.\n        \"\"\"\n        retrieved_texts = self.retrieve(question, top_k=retrieval_top_k)\n        user_prompt = self.USER_PROMPT.format(\n            context=\"\\n\".join(retrieved_texts), question=question\n        )\n        response = self.openai_client.chat.completions.create(\n            model=self.llm_model,\n            messages=[\n                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        )\n        if not return_retrieved_text:\n            return response.choices[0].message.content\n        else:\n            return response.choices[0].message.content, retrieved_texts\n","openai_client = OpenAI()\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\nmy_rag = RAG(openai_client=openai_client, milvus_client=milvus_client)\n","import urllib.request\nimport os\n\nurl = \"https://raw.githubusercontent.com/milvus-io/milvus/master/DEVELOPMENT.md\"\nfile_path = \"./Milvus_DEVELOPMENT.md\"\n\nif not os.path.exists(file_path):\n    urllib.request.urlretrieve(url, file_path)\nwith open(file_path, \"r\") as file:\n    file_text = file.read()\n\ntext_lines = file_text.split(\"# \")\nmy_rag.load(text_lines)\n","question = \"what is the hardware requirements specification if I want to build Milvus and run from source code?\"\nmy_rag.answer(question, return_retrieved_text=True)\n","from datasets import Dataset\nimport pandas as pd\n\nquestion_list = [\n    \"what is the hardware requirements specification if I want to build Milvus and run from source code?\",\n    \"What is the programming language used to write Knowhere?\",\n    \"What should be ensured before running code coverage?\",\n]\nground_truth_list = [\n    \"If you want to build Milvus and run from source code, the recommended hardware requirements specification is:\\n\\n- 8GB of RAM\\n- 50GB of free disk space.\",\n    \"The programming language used to write Knowhere is C++.\",\n    \"Before running code coverage, you should make sure that your code changes are covered by unit tests.\",\n]\ncontexts_list = []\nanswer_list = []\nfor question in tqdm(question_list, desc=\"Answering questions\"):\n    answer, contexts = my_rag.answer(question, return_retrieved_text=True)\n    contexts_list.append(contexts)\n    answer_list.append(answer)\n\ndf = pd.DataFrame(\n    {\n        \"question\": question_list,\n        \"contexts\": contexts_list,\n        \"answer\": answer_list,\n        \"ground_truth\": ground_truth_list,\n    }\n)\nrag_results = Dataset.from_pandas(df)\ndf\n","import phoenix as px\nfrom phoenix.trace.openai import OpenAIInstrumentor\n\n# To view traces in Phoenix, you will first have to start a Phoenix server. You can do this by running the following:\nsession = px.launch_app()\n\n# Initialize OpenAI auto-instrumentation\nOpenAIInstrumentor().instrument()\n","import nest_asyncio\n\nfrom phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n\nnest_asyncio.apply()  # This is needed for concurrency in notebook environments\n\n# Set your OpenAI API key\neval_model = OpenAIModel(model=\"gpt-4o\")\n\n# Define your evaluators\nhallucination_evaluator = HallucinationEvaluator(eval_model)\nqa_evaluator = QAEvaluator(eval_model)\n\n# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\ndf[\"context\"] = df[\"contexts\"]\ndf[\"reference\"] = df[\"contexts\"]\ndf.rename(columns={\"question\": \"input\", \"answer\": \"output\"}, inplace=True)\nassert all(\n    column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"]\n)\n\n# Run the evaluators, each evaluator will return a dataframe with evaluation results\n# We upload the evaluation results to Phoenix in the next step\nhallucination_eval_df, qa_eval_df = run_evals(\n    dataframe=df,\n    evaluators=[hallucination_evaluator, qa_evaluator],\n    provide_explanation=True,\n)\n","results_df = df.copy()\nresults_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\nresults_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\nresults_df[\"qa_eval\"] = qa_eval_df[\"label\"]\nresults_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\nresults_df.head()\n"],"headingContent":"Evaluation with Arize Pheonix","anchorList":[{"label":"Evaluation with Arize Pheonix","href":"Evaluation-with-Arize-Pheonix","type":1,"isActive":false},{"label":"Prerequisites","href":"Prerequisites","type":2,"isActive":false},{"label":"Define the RAG pipeline","href":"Define-the-RAG-pipeline","type":2,"isActive":false},{"label":"Run the RAG pipeline and get results","href":"Run-the-RAG-pipeline-and-get-results","type":2,"isActive":false},{"label":"Evaluation with Arize Phoenix","href":"Evaluation-with-Arize-Phoenix","type":2,"isActive":false}]}