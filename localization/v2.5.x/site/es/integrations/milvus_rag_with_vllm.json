{"codeList":["from langchain.document_loaders import DirectoryLoader\n# Load HTML files already saved in a local directory\npath = \"../../RAG/rtdocs_new/\"\nglobal_pattern = '*.html'\nloader = DirectoryLoader(path=path, glob=global_pattern)\ndocs = loader.load()\n\n\n# Print num documents and a preview.\nprint(f\"loaded {len(docs)} documents\")\nprint(docs[0].page_content)\npprint.pprint(docs[0].metadata)\n","loaded 22 documents\nWhy Milvus Docs Tutorials Tools Blog Community Stars0 Try Managed Milvus FREE Search Home v2.4.x About ...\n{'source': 'https://milvus.io/docs/quickstart.md'}\n","import torch\nfrom sentence_transformers import SentenceTransformer\n\n\n# Initialize torch settings for device-agnostic code.\nN_GPU = torch.cuda.device_count()\nDEVICE = torch.device('cuda:N_GPU' if torch.cuda.is_available() else 'cpu')\n\n\n# Download the model from huggingface model hub.\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nencoder = SentenceTransformer(model_name, device=DEVICE)\n\n\n# Get the model parameters and save for later.\nEMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\nMAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length()\n\n\n# Inspect model parameters.\nprint(f\"model_name: {model_name}\")\nprint(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\nprint(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")\n","model_name: BAAI/bge-large-en-v1.5\nEMBEDDING_DIM: 1024\nMAX_SEQ_LENGTH: 512\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\nCHUNK_SIZE = 512\nchunk_overlap = np.round(CHUNK_SIZE * 0.10, 0)\nprint(f\"chunk_size: {CHUNK_SIZE}, chunk_overlap: {chunk_overlap}\")\n\n\n# Define the splitter.\nchild_splitter = RecursiveCharacterTextSplitter(\n   chunk_size=CHUNK_SIZE,\n   chunk_overlap=chunk_overlap)\n\n\n# Chunk the docs.\nchunks = child_splitter.split_documents(docs)\nprint(f\"{len(docs)} docs split into {len(chunks)} child documents.\")\n\n\n# Encoder input is doc.page_content as strings.\nlist_of_strings = [doc.page_content for doc in chunks if hasattr(doc, 'page_content')]\n\n\n# Embedding inference using HuggingFace encoder.\nembeddings = torch.tensor(encoder.encode(list_of_strings))\n\n\n# Normalize the embeddings.\nembeddings = np.array(embeddings / np.linalg.norm(embeddings))\n\n\n# Milvus expects a list of `numpy.ndarray` of `numpy.float32` numbers.\nconverted_values = list(map(np.float32, embeddings))\n\n\n# Create dict_list for Milvus insertion.\ndict_list = []\nfor chunk, vector in zip(chunks, converted_values):\n   # Assemble embedding vector, original text chunk, metadata.\n   chunk_dict = {\n       'chunk': chunk.page_content,\n       'source': chunk.metadata.get('source', \"\"),\n       'vector': vector,\n   }\n   dict_list.append(chunk_dict)\n","chunk_size: 512, chunk_overlap: 51.0\n22 docs split into 355 child documents.\n","# Connect a client to the Milvus Lite server.\nfrom pymilvus import MilvusClient\nmc = MilvusClient(\"milvus_demo.db\")\n\n\n# Create a collection with flexible schema and AUTOINDEX.\nCOLLECTION_NAME = \"MilvusDocs\"\nmc.create_collection(COLLECTION_NAME,\n       EMBEDDING_DIM,\n       consistency_level=\"Eventually\",\n       auto_id=True, \n       overwrite=True)\n\n\n# Insert data into the Milvus collection.\nprint(\"Start inserting entities\")\nstart_time = time.time()\nmc.insert(\n   COLLECTION_NAME,\n   data=dict_list,\n   progress_bar=True)\n\n\nend_time = time.time()\nprint(f\"Milvus insert time for {len(dict_list)} vectors: \", end=\"\")\nprint(f\"{round(end_time - start_time, 2)} seconds\")\n","Start inserting entities\nMilvus insert time for 355 vectors: 0.2 seconds\n","SAMPLE_QUESTION = \"What do the parameters for HNSW mean?\"\n\n\n# Embed the question using the same encoder.\nquery_embeddings = torch.tensor(encoder.encode(SAMPLE_QUESTION))\n# Normalize embeddings to unit length.\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n# Convert the embeddings to list of list of np.float32.\nquery_embeddings = list(map(np.float32, query_embeddings))\n\n\n# Define metadata fields you can filter on.\nOUTPUT_FIELDS = list(dict_list[0].keys())\nOUTPUT_FIELDS.remove('vector')\n\n\n# Define how many top-k results you want to retrieve.\nTOP_K = 2\n\n\n# Run semantic vector search using your query and the vector database.\nresults = mc.search(\n    COLLECTION_NAME,\n    data=query_embeddings,\n    output_fields=OUTPUT_FIELDS,\n    limit=TOP_K,\n    consistency_level=\"Eventually\")\n","Retrieved result #1\ndistance = 0.7001987099647522\n('Chunk text: layer, finds the node closest to the target in this layer, and'\n...\n'outgoing')\nsource: https://milvus.io/docs/index.md\n\nRetrieved result #2\ndistance = 0.6953287124633789\n('Chunk text: this value can improve recall rate at the cost of increased'\n...\n'to the target')\nsource: https://milvus.io/docs/index.md\n","# (Recommended) Create a new conda environment.\nconda create -n myenv python=3.11 -y\nconda activate myenv\n\n\n# Install vLLM with CUDA 12.1.\npip install -U vllm transformers torch\n\n\nimport vllm, torch\nfrom vllm import LLM, SamplingParams\n\n\n# Clear the GPU memory cache.\ntorch.cuda.empty_cache()\n\n\n# Check the GPU.\n!nvidia-smi\n","# Login to HuggingFace using your new token.\nfrom huggingface_hub import login\nfrom google.colab import userdata\nhf_token = userdata.get('HF_TOKEN')\nlogin(token = hf_token, add_to_git_credential=True)\n","# 1. Choose a model\nMODELTORUN = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n\n# 2. Clear the GPU memory cache, you're going to need it all!\ntorch.cuda.empty_cache()\n\n\n# 3. Instantiate a vLLM model instance.\nllm = LLM(model=MODELTORUN,\n         enforce_eager=True,\n         dtype=torch.bfloat16,\n         gpu_memory_utilization=0.5,\n         max_model_len=1000,\n         seed=415,\n         max_num_batched_tokens=3000)\n","# Separate all the context together by space.\ncontexts_combined = ' '.join(contexts)\n# Lance Martin, LangChain, says put the best contexts at the end.\ncontexts_combined = ' '.join(reversed(contexts))\n\n\n# Separate all the unique sources together by comma.\nsource_combined = ' '.join(reversed(list(dict.fromkeys(sources))))\n\n\nSYSTEM_PROMPT = f\"\"\"First, check if the provided Context is relevant to\nthe user's question.  Second, only if the provided Context is strongly relevant, answer the question using the Context.  Otherwise, if the Context is not strongly relevant, answer the question without using the Context. \nBe clear, concise, relevant.  Answer clearly, in fewer than 2 sentences.\nGrounding sources: {source_combined}\nContext: {contexts_combined}\nUser's question: {SAMPLE_QUESTION}\n\"\"\"\n\n\nprompts = [SYSTEM_PROMPT]\n","# Sampling parameters\nsampling_params = SamplingParams(temperature=0.2, top_p=0.95)\n\n\n# Invoke the vLLM model.\noutputs = llm.generate(prompts, sampling_params)\n\n\n# Print the outputs.\nfor output in outputs:\n   prompt = output.prompt\n   generated_text = output.outputs[0].text\n   # !r calls repr(), which prints a string inside quotes.\n   print()\n   print(f\"Question: {SAMPLE_QUESTION!r}\")\n   pprint.pprint(f\"Generated text: {generated_text!r}\")\n","Question: 'What do the parameters for HNSW MEAN!?'\nGenerated text: 'Answer: The parameters for HNSW (Hiera(rchical Navigable Small World Graph) are: '\n'* M: The maximum degree of nodes on each layer oof the graph, which can improve '\n'recall rate at the cost of increased search time. * efConstruction and ef: ' \n'These parameters specify a search range when building or searching an index.'\n"],"headingContent":"Building RAG with Milvus, vLLM, and Llama 3.1","anchorList":[{"label":"Building RAG with Milvus, vLLM, and Llama 3.1","href":"Building-RAG-with-Milvus-vLLM-and-Llama-31","type":1,"isActive":false},{"label":"Introduction to Milvus, vLLM, and Meta’s Llama 3.1","href":"Introduction-to-Milvus-vLLM-and-Meta’s-Llama-31","type":2,"isActive":false},{"label":"Build and Perform the RAG-Retrieval with Milvus","href":"Build-and-Perform-the-RAG-Retrieval-with-Milvus","type":2,"isActive":false},{"label":"Build and Perform the RAG-Generation with vLLM and Llama 3.1-8B","href":"Build-and-Perform-the-RAG-Generation-with-vLLM-and-Llama-31-8B","type":2,"isActive":false},{"label":"References","href":"References","type":2,"isActive":false}]}