{"codeList":["$ sudo apt update\n$ sudo apt install python-is-python3 python3-pip python3-venv -y\n","$ python -m venv venv\n$ source venv/bin/activate\n","$ pip install --upgrade pymilvus openai requests langchain-huggingface huggingface_hub tqdm\n","from pymilvus import MilvusClient\n\nmilvus_client = MilvusClient(\n    uri=\"<your_zilliz_public_endpoint>\", token=\"<your_zilliz_api_key>\"\n)\n\ncollection_name = \"my_rag_collection\"\n\n","if milvus_client.has_collection(collection_name):\n    milvus_client.drop_collection(collection_name)\n","milvus_client.create_collection(\n    collection_name=collection_name,\n    dimension=384,\n    metric_type=\"IP\",  # Inner product distance\n    consistency_level=\"Strong\",  # Strong consistency level\n)\n","$ wget https://github.com/milvus-io/milvus-docs/releases/download/v2.4.6-preview/milvus_docs_2.4.x_en.zip\n$ unzip -q milvus_docs_2.4.x_en.zip -d milvus_docs\n","from glob import glob\n\ntext_lines = []\n\nfor file_path in glob(\"milvus_docs/en/faq/*.md\", recursive=True):\n    with open(file_path, \"r\") as file:\n        file_text = file.read()\n\n    text_lines += file_text.split(\"# \")\n","from langchain_huggingface import HuggingFaceEmbeddings\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","from tqdm import tqdm\n\ndata = []\n\ntext_embeddings = embedding_model.embed_documents(text_lines)\n\nfor i, (line, embedding) in enumerate(\n    tqdm(zip(text_lines, text_embeddings), desc=\"Creating embeddings\")\n):\n    data.append({\"id\": i, \"vector\": embedding, \"text\": line})\n\nmilvus_client.insert(collection_name=collection_name, data=data)\n","$ sudo apt install make cmake -y\n$ sudo apt install gcc g++ -y\n$ sudo apt install build-essential -y\n","$ git clone https://github.com/ggerganov/llama.cpp\n","$ cd llama.cpp\n$ make GGML_NO_LLAMAFILE=1 -j$(nproc)\n","$ ./llama-cli -h\n","$ huggingface-cli download cognitivecomputations/dolphin-2.9.4-llama3.1-8b-gguf dolphin-2.9.4-llama3.1-8b-Q4_0.gguf --local-dir . --local-dir-use-symlinks False\n","$ ./llama-quantize --allow-requantize dolphin-2.9.4-llama3.1-8b-Q4_0.gguf dolphin-2.9.4-llama3.1-8b-Q4_0_8_8.gguf Q4_0_8_8\n","$ ./llama-server -m dolphin-2.9.4-llama3.1-8b-Q4_0_8_8.gguf -n 2048 -t 64 -c 65536  --port 8080\n","from openai import OpenAI\n\nllm_client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"no-key\")\n","test_embedding = embedding_model.embed_query(\"This is a test\")\nembedding_dim = len(test_embedding)\nprint(embedding_dim)\nprint(test_embedding[:10])\n","question = \"How is data stored in milvus?\"\n","search_res = milvus_client.search(\n    collection_name=collection_name,\n    data=[\n        embedding_model.embed_query(question)\n    ],  # Use the `emb_text` function to convert the question to an embedding vector\n    limit=3,  # Return top 3 results\n    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n    output_fields=[\"text\"],  # Return the text field\n)\n","import json\n\nretrieved_lines_with_distances = [\n    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n]\nprint(json.dumps(retrieved_lines_with_distances, indent=4))\n","context = \"\\n\".join(\n    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n)\n","response = llm_client.chat.completions.create(\n    model=\"not-used\",\n    messages=[\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_PROMPT},\n    ],\n)\nprint(response.choices[0].message.content)\n\n"],"headingContent":"Build RAG on Arm Architecture","anchorList":[{"label":"Build RAG on Arm Architecture","href":"Build-RAG-on-Arm-Architecture","type":1,"isActive":false},{"label":"Prerequisite","href":"Prerequisite","type":2,"isActive":false},{"label":"Offline Data Loading","href":"Offline-Data-Loading","type":2,"isActive":false},{"label":"Launch LLM Service on Arm","href":"Launch-LLM-Service-on-Arm","type":2,"isActive":false},{"label":"Online RAG","href":"Online-RAG","type":2,"isActive":false}]}