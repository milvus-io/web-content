{"codeList":["$ pip install datasets numpy pandas pymilvus sentence-transformers tqdm\n","$ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n","$ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","import functools\n\nfrom datasets import load_dataset\nimport numpy as np\nimport pandas as pd\nimport pymilvus\nfrom pymilvus import MilvusClient\nfrom pymilvus import FieldSchema, CollectionSchema, DataType\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n","model = SentenceTransformer(\n    # Remove 'device='mps' if running on non-Mac device\n    \"nomic-ai/nomic-embed-text-v1.5\",\n    trust_remote_code=True,\n    device=\"mps\",\n)\n","ds = load_dataset(\"vishnupriyavr/wiki-movie-plots-with-summaries\", split=\"train\")\nprint(ds)\n","embedding_dim = 768\nsearch_dim = 128\ncollection_name = \"movie_embeddings\"\n\nclient = MilvusClient(uri=\"./wiki-movie-plots-matryoshka.db\")\n\nfields = [\n    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=256),\n    # First sixth of unnormalized embedding vector\n    FieldSchema(name=\"head_embedding\", dtype=DataType.FLOAT_VECTOR, dim=search_dim),\n    # Entire unnormalized embedding vector\n    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n]\n\nschema = CollectionSchema(fields=fields, enable_dynamic_field=False)\nclient.create_collection(collection_name=collection_name, schema=schema)\n","index_params = client.prepare_index_params()\nindex_params.add_index(\n    field_name=\"head_embedding\", index_type=\"FLAT\", metric_type=\"COSINE\"\n)\nindex_params.add_index(field_name=\"embedding\", index_type=\"FLAT\", metric_type=\"COSINE\")\nclient.create_index(collection_name, index_params)\n","for batch in tqdm(ds.batch(batch_size=512)):\n    # This particular model requires us to prefix 'search_document:' to stored entities\n    plot_summary = [\"search_document: \" + x.strip() for x in batch[\"PlotSummary\"]]\n\n    # Output of embedding model is unnormalized\n    embeddings = model.encode(plot_summary, convert_to_tensor=True)\n    head_embeddings = embeddings[:, :search_dim]\n\n    data = [\n        {\n            \"title\": title,\n            \"head_embedding\": head.cpu().numpy(),\n            \"embedding\": embedding.cpu().numpy(),\n        }\n        for title, head, embedding in zip(batch[\"Title\"], head_embeddings, embeddings)\n    ]\n    res = client.insert(collection_name=collection_name, data=data)\n","queries = [\n    \"An archaeologist searches for ancient artifacts while fighting Nazis.\",\n    \"A teenager fakes illness to get off school and have adventures with two friends.\",\n    \"A young couple with a kid look after a hotel during winter and the husband goes insane.\",\n]\n\n\n# Search the database based on input text\ndef embed_search(data):\n    embeds = model.encode(data)\n    return [x for x in embeds]\n\n\n# This particular model requires us to prefix 'search_query:' to queries\ninstruct_queries = [\"search_query: \" + q.strip() for q in queries]\nsearch_data = embed_search(instruct_queries)\n\n# Normalize head embeddings\nhead_search = [x[:search_dim] for x in search_data]\n\n# Perform standard vector search on first sixth of embedding dimensions\nres = client.search(\n    collection_name=collection_name,\n    data=head_search,\n    anns_field=\"head_embedding\",\n    limit=128,\n    output_fields=[\"title\", \"head_embedding\", \"embedding\"],\n)\n","for query, hits in zip(queries, res):\n    rows = [x[\"entity\"] for x in hits][:5]\n\n    print(\"Query:\", query)\n    print(\"Results:\")\n    for row in rows:\n        print(row[\"title\"].strip())\n    print()\n","def hits_to_dataframe(hits: pymilvus.client.abstract.Hits) -> pd.DataFrame:\n    \"\"\"\n    Convert a Milvus search result to a Pandas dataframe. This function is specific to our data schema.\n\n    \"\"\"\n    rows = [x[\"entity\"] for x in hits]\n    rows_dict = [\n        {\"title\": x[\"title\"], \"embedding\": torch.tensor(x[\"embedding\"])} for x in rows\n    ]\n    return pd.DataFrame.from_records(rows_dict)\n\n\ndfs = [hits_to_dataframe(hits) for hits in res]\n","# An optimized implementation would vectorize the calculation of similarity scores across rows (using a matrix)\ndef calculate_score(row, query_emb=None, dims=768):\n    emb = F.normalize(row[\"embedding\"][:dims], dim=-1)\n    return (emb @ query_emb).item()\n\n\n# You could also add a top-K parameter as a termination condition\ndef funnel_search(\n    df: pd.DataFrame, query_emb, scales=[256, 512, 768], prune_ratio=0.5\n) -> pd.DataFrame:\n    # Loop over increasing prefixes of the embeddings\n    for dims in scales:\n        # Query vector must be normalized for each new dimensionality\n        emb = torch.tensor(query_emb[:dims] / np.linalg.norm(query_emb[:dims]))\n\n        # Score\n        scores = df.apply(\n            functools.partial(calculate_score, query_emb=emb, dims=dims), axis=1\n        )\n        df[\"scores\"] = scores\n\n        # Re-rank\n        df = df.sort_values(by=\"scores\", ascending=False)\n\n        # Prune (in our case, remove half of candidates at each step)\n        df = df.head(int(prune_ratio * len(df)))\n\n    return df\n\n\ndfs_results = [\n    {\"query\": query, \"results\": funnel_search(df, query_emb)}\n    for query, df, query_emb in zip(queries, dfs, search_data)\n]\n","for d in dfs_results:\n    print(d[\"query\"], \"\\n\", d[\"results\"][:5][\"title\"], \"\\n\")\n","# Search on entire embeddings\nres = client.search(\n    collection_name=collection_name,\n    data=search_data,\n    anns_field=\"embedding\",\n    limit=5,\n    output_fields=[\"title\", \"embedding\"],\n)\n","for query, hits in zip(queries, res):\n    rows = [x[\"entity\"] for x in hits]\n\n    print(\"Query:\", query)\n    print(\"Results:\")\n    for row in rows:\n        print(row[\"title\"].strip())\n    print()\n","queries2 = [\n    \"A teenager fakes illness to get off school and have adventures with two friends.\"\n]\n\n\n# Search the database based on input text\ndef embed_search(data):\n    embeds = model.encode(data)\n    return [x for x in embeds]\n\n\ninstruct_queries = [\"search_query: \" + q.strip() for q in queries2]\nsearch_data2 = embed_search(instruct_queries)\nhead_search2 = [x[:search_dim] for x in search_data2]\n\n# Perform standard vector search on subset of embeddings\nres = client.search(\n    collection_name=collection_name,\n    data=head_search2,\n    anns_field=\"head_embedding\",\n    limit=256,\n    output_fields=[\"title\", \"head_embedding\", \"embedding\"],\n)\n","for query, hits in zip(queries, res):\n    rows = [x[\"entity\"] for x in hits]\n\n    print(\"Query:\", queries2[0])\n    for idx, row in enumerate(rows):\n        if row[\"title\"].strip() == \"Ferris Bueller's Day Off\":\n            print(f\"Row {idx}: Ferris Bueller's Day Off\")\n","dfs = [hits_to_dataframe(hits) for hits in res]\n\ndfs_results = [\n    {\"query\": query, \"results\": funnel_search(df, query_emb)}\n    for query, df, query_emb in zip(queries2, dfs, search_data2)\n]\n\nfor d in dfs_results:\n    print(d[\"query\"], \"\\n\", d[\"results\"][:7][\"title\"].to_string(index=False), \"\\n\")\n","client = MilvusClient(uri=\"./wikiplots-matryoshka-flipped.db\")\n\nfields = [\n    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=256),\n    FieldSchema(name=\"head_embedding\", dtype=DataType.FLOAT_VECTOR, dim=search_dim),\n    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n]\n\nschema = CollectionSchema(fields=fields, enable_dynamic_field=False)\nclient.create_collection(collection_name=collection_name, schema=schema)\n\nindex_params = client.prepare_index_params()\nindex_params.add_index(\n    field_name=\"head_embedding\", index_type=\"FLAT\", metric_type=\"COSINE\"\n)\nclient.create_index(collection_name, index_params)\n","for batch in tqdm(ds.batch(batch_size=512)):\n    plot_summary = [\"search_document: \" + x.strip() for x in batch[\"PlotSummary\"]]\n\n    # Encode and flip embeddings\n    embeddings = model.encode(plot_summary, convert_to_tensor=True)\n    embeddings = torch.flip(embeddings, dims=[-1])\n    head_embeddings = embeddings[:, :search_dim]\n\n    data = [\n        {\n            \"title\": title,\n            \"head_embedding\": head.cpu().numpy(),\n            \"embedding\": embedding.cpu().numpy(),\n        }\n        for title, head, embedding in zip(batch[\"Title\"], head_embeddings, embeddings)\n    ]\n    res = client.insert(collection_name=collection_name, data=data)\n","# Normalize head embeddings\n\nflip_search_data = [\n    torch.flip(torch.tensor(x), dims=[-1]).cpu().numpy() for x in search_data\n]\nflip_head_search = [x[:search_dim] for x in flip_search_data]\n\n# Perform standard vector search on subset of embeddings\nres = client.search(\n    collection_name=collection_name,\n    data=flip_head_search,\n    anns_field=\"head_embedding\",\n    limit=128,\n    output_fields=[\"title\", \"head_embedding\", \"embedding\"],\n)\n","dfs = [hits_to_dataframe(hits) for hits in res]\n\ndfs_results = [\n    {\"query\": query, \"results\": funnel_search(df, query_emb)}\n    for query, df, query_emb in zip(queries, dfs, flip_search_data)\n]\n\nfor d in dfs_results:\n    print(\n        d[\"query\"],\n        \"\\n\",\n        d[\"results\"][:7][\"title\"].to_string(index=False, header=False),\n        \"\\n\",\n    )\n"],"headingContent":"Funnel Search with Matryoshka Embeddings","anchorList":[{"label":"البحث القمعي مع تضمينات ماتريوشكا","href":"Funnel-Search-with-Matryoshka-Embeddings","type":1,"isActive":false},{"label":"التحضير","href":"Preparation","type":2,"isActive":false},{"label":"تحميل نموذج تضمين Matryoshka","href":"Load-Matryoshka-Embedding-Model","type":2,"isActive":false},{"label":"تحميل مجموعة البيانات، وتضمين العناصر، وبناء قاعدة بيانات المتجهات","href":"Loading-Dataset-Embedding-Items-and-Building-Vector-Database","type":2,"isActive":false},{"label":"إجراء بحث قمعي","href":"Performing-Funnel-Search","type":2,"isActive":false},{"label":"مقارنة البحث القمعي بالبحث العادي","href":"Comparing-Funnel-Search-to-Regular-Search","type":2,"isActive":false},{"label":"التحقيق في فشل استرجاع البحث القمعي في يوم عطلة فيريس بويلر","href":"Investigating-Funnel-Search-Recall-Failure-for-Ferris-Buellers-Day-Off","type":2,"isActive":false},{"label":"هل الترتيب مهم؟ تضمين البادئة مقابل تضمين اللاحقة.","href":"Does-the-order-matter-Prefix-vs-suffix-embeddings","type":2,"isActive":false},{"label":"ملخص","href":"Summary","type":2,"isActive":false}]}