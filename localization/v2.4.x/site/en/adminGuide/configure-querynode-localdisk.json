{"codeList":["$ lsblk | grep nvme\nnvme0n1     259:0    0 250.0G  0 disk \nnvme1n1     259:1    0 250.0G  0 disk \n","MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"\n\n--==MYBOUNDARY==\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n\n#!/bin/bash\necho \"Running custom user data script\"\nif ( lsblk | fgrep -q nvme1n1 ); then\n    mkdir -p /mnt/data /var/lib/kubelet /var/lib/docker\n    mkfs.xfs /dev/nvme1n1\n    mount /dev/nvme1n1 /mnt/data\n    chmod 0755 /mnt/data\n    mv /var/lib/kubelet /mnt/data/\n    mv /var/lib/docker /mnt/data/\n    ln -sf /mnt/data/kubelet /var/lib/kubelet\n    ln -sf /mnt/data/docker /var/lib/docker\n    UUID=$(lsblk -f | grep nvme1n1 | awk '{print $3}')\n    echo \"UUID=$UUID     /mnt/data   xfs    defaults,noatime  1   1\" >> /etc/fstab\nfi\necho 10485760 > /proc/sys/fs/aio-max-nr\n\n--==MYBOUNDARY==--\n","gcloud container node-pools create ${POOL_NAME} \\\n    --cluster=${CLUSTER_NAME} \\\n    --ephemeral-storage-local-ssd count=${NUMBER_OF_DISKS} \\\n    --machine-type=${MACHINE_TYPE}\n","mdadm -Cv /dev/md0 -l0 -n2 /dev/nvme0n1 /dev/nvme1n1\nmdadm -Ds > /etc/mdadm/mdadm.conf \nupdate-initramfs -u\n\nmkfs.xfs /dev/md0\nmkdir -p /var/lib/kubelet\necho '/dev/md0 /var/lib/kubelet xfs defaults 0 0' >> /etc/fstab\nmount -a\n","#!/bin/bash\necho \"nvme init start...\"\nmkfs.xfs /dev/nvme0n1\nmkdir -p /mnt/data\necho '/dev/nvme0n1 /mnt/data/ xfs defaults 0 0' >> /etc/fstab\nmount -a\n\nmkdir -p /mnt/data/kubelet /mnt/data/containerd /mnt/data/log/pods\nmkdir -p  /var/lib/kubelet /var/lib/containerd /var/log/pods\n\necho '/mnt/data/kubelet /var/lib/kubelet none defaults,bind 0 0' >> /etc/fstab\necho '/mnt/data/containerd /var/lib/containerd none defaults,bind 0 0' >> /etc/fstab\necho '/mnt/data/log/pods /var/log/pods none defaults,bind 0 0' >> /etc/fstab\nmount -a\n\necho \"nvme init end...\"\n","sudo mkdir -p /mnt/nvme/containerd /mnt/nvme/containerd/state\nsudo vim /etc/containerd/config.toml\n","[plugins.\"io.containerd.grpc.v1.cri\".containerd]\nsnapshotter = \"overlayfs\"\nroot = \"/mnt/nvme/containerd\"\nstate = \"/mnt/nvme/containerd/state\"\n","sudo systemctl restart containerd\n","kubectl create -f ubuntu.yaml\n","apiVersion: v1\nkind: Pod\nmetadata:\nname: ubuntu\nspec:\ncontainers:\n- name: ubuntu\n    image: ubuntu:latest\n    command: [\"sleep\", \"86400\"]\n    volumeMounts:\n    - name: data-volume\n        mountPath: /data\nvolumes:\n    - name: data-volume\n    emptyDir: {}\n","# enter the container\nkubectl exec pod/ubuntu -it bash\n\n# in container\napt-get update\napt-get install fio -y\n\n# change to the mounted dir\ncd /data\n\n# write 10GB\nfio -direct=1 -iodepth=128 -rw=randwrite -ioengine=libaio -bs=4K -size=10G -numjobs=10 -runtime=600 -group_reporting -filename=test -name=Rand_Write_IOPS_Test\n\n# verify the read speed\n# compare with the disk performance indicators provided by various cloud providers.\nfio --filename=test --direct=1 --rw=randread --bs=4k --ioengine=libaio --iodepth=64 --runtime=120 --numjobs=128 --time_based --group_reporting --name=iops-test-job --eta-newline=1  --readonly\n","Jobs: 128 (f=128): [r(128)][100.0%][r=1458MiB/s][r=373k IOPS][eta 00m:00s]\niops-test-job: (groupid=0, jobs=128): err= 0: pid=768: Mon Jun 24 09:35:06 2024\nread: IOPS=349k, BW=1364MiB/s (1430MB/s)(160GiB/120067msec)\n    slat (nsec): min=765, max=530621k, avg=365836.09, stdev=4765464.96\n    clat (usec): min=35, max=1476.0k, avg=23096.78, stdev=45409.13\n    lat (usec): min=36, max=1571.6k, avg=23462.62, stdev=46296.74\n    clat percentiles (usec):\n    |  1.00th=[    69],  5.00th=[    79], 10.00th=[    85], 20.00th=[    95],\n    | 30.00th=[   106], 40.00th=[   123], 50.00th=[   149], 60.00th=[ 11469],\n    | 70.00th=[ 23462], 80.00th=[ 39584], 90.00th=[ 70779], 95.00th=[103285],\n    | 99.00th=[189793], 99.50th=[244319], 99.90th=[497026], 99.95th=[591397],\n    | 99.99th=[767558]\nbw (  MiB/s): min=  236, max= 4439, per=100.00%, avg=1365.82, stdev= 5.02, samples=30591\niops        : min=60447, max=1136488, avg=349640.62, stdev=1284.65, samples=30591\nlat (usec)   : 50=0.01%, 100=24.90%, 250=30.47%, 500=0.09%, 750=0.31%\nlat (usec)   : 1000=0.08%\nlat (msec)   : 2=0.32%, 4=0.59%, 10=1.86%, 20=8.20%, 50=17.29%\nlat (msec)   : 100=10.62%, 250=4.80%, 500=0.38%, 750=0.09%, 1000=0.01%\nlat (msec)   : 2000=0.01%\ncpu          : usr=0.20%, sys=0.48%, ctx=838085, majf=0, minf=9665\nIO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%\n    submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n    complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%\n    issued rwts: total=41910256,0,0,0 short=0,0,0,0 dropped=0,0,0,0\n    latency   : target=0, window=0, percentile=100.00%, depth=64\n","...\nspec:\n  components:\n    queryNode:\n      volumeMounts:\n      - mountPath: /var/lib/milvus/data\n        name: data\n      volumes:\n      - emptyDir:\n        name: data\n"],"headingContent":"Configure Milvus QueryNode with Local Disk","anchorList":[{"label":"Configure Milvus QueryNode with Local Disk","href":"Configure-Milvus-QueryNode-with-Local-Disk","type":1,"isActive":false},{"label":"Overview","href":"Overview","type":2,"isActive":false},{"label":"Configure Kubernetes to use local disk","href":"Configure-Kubernetes-to-use-local-disk","type":2,"isActive":false},{"label":"Verify disk performance","href":"Verify-disk-performance","type":2,"isActive":false},{"label":"Deploy Milvus Distributed","href":"Deploy-Milvus-Distributed","type":2,"isActive":false}]}