{"codeList":["$ pip install pymilvus ollama\n","$ wget https://github.com/milvus-io/milvus-docs/releases/download/v2.4.6-preview/milvus_docs_2.4.x_en.zip\n$ unzip -q milvus_docs_2.4.x_en.zip -d milvus_docs\n","from glob import glob\n\ntext_lines = []\n\nfor file_path in glob(\"milvus_docs/en/faq/*.md\", recursive=True):\n    with open(file_path, \"r\") as file:\n        file_text = file.read()\n\n    text_lines += file_text.split(\"# \")\n","! ollama pull mxbai-embed-large\n","! ollama pull llama3.2\n","import ollama\n\n\ndef emb_text(text):\n    response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=text)\n    return response[\"embedding\"]\n","test_embedding = emb_text(\"This is a test\")\nembedding_dim = len(test_embedding)\nprint(embedding_dim)\nprint(test_embedding[:10])\n","from pymilvus import MilvusClient\n\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\ncollection_name = \"my_rag_collection\"\n","if milvus_client.has_collection(collection_name):\n    milvus_client.drop_collection(collection_name)\n","milvus_client.create_collection(\n    collection_name=collection_name,\n    dimension=embedding_dim,\n    metric_type=\"IP\",  # Inner product distance\n    consistency_level=\"Strong\",  # Strong consistency level\n)\n","from tqdm import tqdm\n\ndata = []\n\nfor i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n\nmilvus_client.insert(collection_name=collection_name, data=data)\n","question = \"How is data stored in milvus?\"\n","search_res = milvus_client.search(\n    collection_name=collection_name,\n    data=[\n        emb_text(question)\n    ],  # Use the `emb_text` function to convert the question to an embedding vector\n    limit=3,  # Return top 3 results\n    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n    output_fields=[\"text\"],  # Return the text field\n)\n","import json\n\nretrieved_lines_with_distances = [\n    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n]\nprint(json.dumps(retrieved_lines_with_distances, indent=4))\n","context = \"\\n\".join(\n    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n)\n","SYSTEM_PROMPT = \"\"\"\nHuman: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n\"\"\"\nUSER_PROMPT = f\"\"\"\nUse the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n<context>\n{context}\n</context>\n<question>\n{question}\n</question>\n\"\"\"\n","from ollama import chat\nfrom ollama import ChatResponse\n\nresponse: ChatResponse = chat(\n    model=\"llama3.2\",\n    messages=[\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_PROMPT},\n    ],\n)\nprint(response[\"message\"][\"content\"])\n"],"headingContent":"Build RAG with Milvus and Ollama","anchorList":[{"label":"Build RAG with Milvus and Ollama","href":"Build-RAG-with-Milvus-and-Ollama","type":1,"isActive":false},{"label":"Preparation","href":"Preparation","type":2,"isActive":false},{"label":"Load data into Milvus","href":"Load-data-into-Milvus","type":2,"isActive":false},{"label":"Build RAG","href":"Build-RAG","type":2,"isActive":false}]}